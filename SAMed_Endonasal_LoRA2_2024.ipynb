{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/tutorial_notebooks/blob/main/SAMed_Endonasal_LoRA2_2024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1P2M4gZbKZWt"
      },
      "source": [
        "# Customized Segment Anything Model for Medical Image Segmentation\n",
        "### [[Paper](https://arxiv.org/pdf/2304.13785.pdf)] [[Github](https://github.com/hitachinsk/SAMed)]\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Id3D1PuuLQMm"
      },
      "source": [
        "# Setup environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmmYvx7FLUif",
        "outputId": "316402d2-f3d2-4755-b286-74533192300c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/151.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m143.4/151.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.8/151.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'opencv-python' candidate (version 4.5.4.58 at https://files.pythonhosted.org/packages/ea/8c/e01428f31e473f765355c65c24f2dbd62a6a093a3248a9fa97bc65eeeb22/opencv_python-4.5.4.58-cp310-cp310-manylinux2014_x86_64.whl (from https://pypi.org/simple/opencv-python/) (requires-python:>=3.6))\n",
            "Reason for being yanked: deprecated, use  4.5.4.60\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.3/60.3 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.7/52.7 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m869.5/869.5 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for MedPy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ml-collections (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q gdown==4.6.0 einops==0.6.1 icecream==2.1.3 MedPy==0.4.0 monai==1.1.0 opencv_python==4.5.4.58 SimpleITK==2.2.1 tensorboardX==2.6 ml-collections==0.1.1 onnx==1.13.1 onnxruntime==1.14.1 tensorboardX torchmetrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-tSMFkgPhyc"
      },
      "source": [
        "# Download codes, pretrained weights and test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RyB2eYACPtEX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c6a4bcc-c3a5-436c-91bb-7ab07ef667ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'samed_codes'...\n",
            "remote: Enumerating objects: 225, done.\u001b[K\n",
            "remote: Counting objects: 100% (58/58), done.\u001b[K\n",
            "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
            "remote: Total 225 (delta 42), reused 29 (delta 29), pack-reused 167 (from 1)\u001b[K\n",
            "Receiving objects: 100% (225/225), 636.92 KiB | 2.31 MiB/s, done.\n",
            "Resolving deltas: 100% (103/103), done.\n"
          ]
        }
      ],
      "source": [
        "# prepare codes\n",
        "import os\n",
        "CODE_DIR = 'samed_codes'\n",
        "os.makedirs(f'./{CODE_DIR}')\n",
        "!git clone https://github.com/hitachinsk/SAMed.git $CODE_DIR\n",
        "os.chdir(f'./{CODE_DIR}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1g_Hb28K0wsfKyUGdgNnPNfrmSAAXYl5L\n",
        "!gdown 1P0Bm-05l-rfeghbrT1B62v5eN-3A-uOr\n",
        "!gdown 1_oCdoEEu3mNhRfFxeWyRerOKt8OEUvcg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBR-shCzwRnR",
        "outputId": "04edfc97-872c-4821-865a-7322da566725"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1g_Hb28K0wsfKyUGdgNnPNfrmSAAXYl5L\n",
            "To: /content/samed_codes/Endonasal_Slices_Voxel.zip\n",
            "100% 138M/138M [00:04<00:00, 32.9MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1P0Bm-05l-rfeghbrT1B62v5eN-3A-uOr\n",
            "To: /content/samed_codes/epoch_159.pth\n",
            "100% 19.7M/19.7M [00:01<00:00, 18.6MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1_oCdoEEu3mNhRfFxeWyRerOKt8OEUvcg\n",
            "To: /content/samed_codes/sam_vit_b_01ec64.pth\n",
            "100% 375M/375M [00:08<00:00, 42.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q Endonasal_Slices_Voxel.zip"
      ],
      "metadata": {
        "id": "7gRXdOOlxVHZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnQmJASbCqUT"
      },
      "source": [
        "Dataloader:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8w27C6DKCvOK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "8ab1f16c-5f81-4190-ab52-171cb3f81eb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Train Sample: 798 Test Sample: 342\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "%cd /content/samed_codes\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from glob import glob\n",
        "import imageio.v2 as iio\n",
        "import matplotlib.pyplot as plt\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "from scipy.ndimage import zoom\n",
        "from einops import repeat\n",
        "from scipy import ndimage\n",
        "import random\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "def normalise_intensity(image, ROI_thres=0.1):\n",
        "    pixel_thres = np.percentile(image, ROI_thres)\n",
        "    ROI = np.where(image > pixel_thres, image, 0) # If image value is greater than pixel threshold, return image value, otherwise return 0\n",
        "    mean = np.mean(ROI)\n",
        "    std = np.std(ROI)\n",
        "    ROI_norm = (ROI - mean) / (std + 1e-8) # Normalise ROI\n",
        "    return ROI_norm\n",
        "\n",
        "def random_rot_flip(image, label):\n",
        "    k = np.random.randint(0, 4)\n",
        "    image = np.rot90(image, k)\n",
        "    label = np.rot90(label, k)\n",
        "    axis = np.random.randint(0, 2)\n",
        "    image = np.flip(image, axis=axis).copy()\n",
        "    label = np.flip(label, axis=axis).copy()\n",
        "    return image, label\n",
        "\n",
        "\n",
        "def random_rotate(image, label):\n",
        "    angle = np.random.randint(-20, 20)\n",
        "    image = ndimage.rotate(image, angle, order=0, reshape=False)\n",
        "    label = ndimage.rotate(label, angle, order=0, reshape=False)\n",
        "    return image, label\n",
        "\n",
        "class EndonasalDataset(Dataset):\n",
        "    def __init__(self, root='endonasal_train', low_res=None, isTrain=False):\n",
        "        self.img_path_all = glob(root + '/mri_t1c/*.png')  # Update the path and pattern\n",
        "        self.mask_path_all = glob(root + '/mri_masks/*.png')  # Update the path and pattern\n",
        "        self.isTrain = isTrain\n",
        "        self.isTrain = isTrain\n",
        "        self.low_res = low_res\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.img_path_all)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image = iio.imread(self.img_path_all[index])\n",
        "        image = normalise_intensity(image)\n",
        "        image = zoom(image, (512/image.shape[0], 512/image.shape[1]), order=0)\n",
        "        label = iio.imread(self.mask_path_all[index])\n",
        "        label = zoom(label, (512/label.shape[0], 512/label.shape[1]), order=0)\n",
        "        if self.isTrain:\n",
        "            if random.random() > 0.5:\n",
        "                image, label = random_rot_flip(image, label)\n",
        "            elif random.random() > 0.5:\n",
        "                image, label = random_rotate(image, label)\n",
        "\n",
        "        image = repeat(np.expand_dims(image, axis=0), 'c h w -> (repeat c) h w', repeat=3)\n",
        "        sample = {'image': image, 'label': label}\n",
        "        if self.low_res:\n",
        "            low_res_label = zoom(label, (self.low_res/label.shape[0], self.low_res/label.shape[1]), order=0)\n",
        "            sample = {'image': image, 'label': label, 'low_res_label': low_res_label}\n",
        "\n",
        "        return sample\n",
        "\n",
        "train_dataset = EndonasalDataset(root='Endonasal_Slices_Voxel/Train', low_res=128, isTrain=True)\n",
        "test_dataset = EndonasalDataset(root='Endonasal_Slices_Voxel/Test', low_res=128)\n",
        "print('Train Sample:', len(train_dataset), 'Test Sample:', len(test_dataset))\n",
        "sample = train_dataset[10]\n",
        "input, label, low_res_label = np.array(sample['image']), sample['label'], sample['low_res_label']\n",
        "plt.subplot(1,4,1), plt.axis('OFF'), plt.title('in:{}'.format(input.shape)), plt.imshow(input.transpose(1,2,0))\n",
        "plt.subplot(1,4,2), plt.axis('OFF'), plt.title('in:{}'.format(input[0].shape)), plt.imshow(input[0], cmap='gray')\n",
        "plt.subplot(1,4,3), plt.axis('OFF'), plt.title('lab:{}'.format(label.shape)), plt.imshow(label, cmap='gray');\n",
        "plt.subplot(1,4,4), plt.axis('OFF'), plt.title('low:{}'.format(low_res_label.shape)), plt.imshow(low_res_label, cmap='gray');"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from segment_anything import build_sam, SamPredictor\n",
        "from segment_anything import sam_model_registry\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from torch.nn.parameter import Parameter\n",
        "from segment_anything.modeling import Sam\n",
        "from safetensors import safe_open\n",
        "from safetensors.torch import save_file\n",
        "\n",
        "from icecream import ic\n",
        "\n",
        "class _LoRA_qkv_v0_v2(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            qkv: nn.Module,\n",
        "            linear_a_q: nn.Module,\n",
        "            linear_b_q: nn.Module,\n",
        "            linear_a_v: nn.Module,\n",
        "            linear_b_v: nn.Module,\n",
        "            conv_se_q: nn.Module,\n",
        "            conv_se_v: nn.Module,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.qkv = qkv\n",
        "        self.linear_a_q = linear_a_q\n",
        "        self.linear_b_q = linear_b_q\n",
        "        self.linear_a_v = linear_a_v\n",
        "        self.linear_b_v = linear_b_v\n",
        "        self.conv_se_q = conv_se_q\n",
        "        self.conv_se_v = conv_se_v\n",
        "\n",
        "        self.dim = qkv.in_features\n",
        "        self.w_identity = torch.eye(qkv.in_features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        qkv = self.qkv(x)\n",
        "        a_q_out = self.linear_a_q(x)\n",
        "        a_v_out = self.linear_a_v(x)\n",
        "        a_q_out_temp = self.conv_se_q(a_q_out.permute(0,3,1,2)).permute(0,2,3,1)\n",
        "        a_v_out_temp = self.conv_se_v(a_v_out.permute(0,3,1,2)).permute(0,2,3,1)\n",
        "\n",
        "        new_q = self.linear_b_q(torch.mul(a_q_out, torch.sigmoid(a_q_out_temp)))#SE = Squeeze and Excitation\n",
        "        new_v = self.linear_b_v(torch.mul(a_v_out, torch.sigmoid(a_v_out_temp)))\n",
        "\n",
        "        qkv[:, :, :, : self.dim] += new_q\n",
        "        qkv[:, :, :, -self.dim:] += new_v\n",
        "        return qkv\n",
        "\n",
        "class LoRA_Sam_v0_v2(nn.Module):\n",
        "\n",
        "    def __init__(self, sam_model: Sam, r: int, lora_layer=None):\n",
        "        super(LoRA_Sam_v0_v2, self).__init__()\n",
        "\n",
        "        assert r > 0\n",
        "        if lora_layer:\n",
        "            self.lora_layer = lora_layer\n",
        "        else:\n",
        "            self.lora_layer = list(\n",
        "                range(len(sam_model.image_encoder.blocks)))  # Only apply lora to the image encoder by default\n",
        "        # create for storage, then we can init them or load weights\n",
        "        self.w_As = []  # These are linear layers\n",
        "        self.w_Bs = []\n",
        "\n",
        "        # lets freeze first\n",
        "        for param in sam_model.image_encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Here, we do the surgery\n",
        "        for t_layer_i, blk in enumerate(sam_model.image_encoder.blocks):\n",
        "            # If we only want few lora layer instead of all\n",
        "            if t_layer_i not in self.lora_layer:\n",
        "                continue\n",
        "            w_qkv_linear = blk.attn.qkv\n",
        "            self.dim = w_qkv_linear.in_features\n",
        "            w_a_linear_q = nn.Linear(self.dim, r, bias=False)\n",
        "            w_b_linear_q = nn.Linear(r, self.dim, bias=False)\n",
        "            w_a_linear_v = nn.Linear(self.dim, r, bias=False)\n",
        "            w_b_linear_v = nn.Linear(r, self.dim, bias=False)\n",
        "\n",
        "            conv_se_q = nn.Conv2d(r, r, kernel_size=1,\n",
        "                                    stride=1, padding=0, bias=False)\n",
        "            conv_se_v = nn.Conv2d(r, r, kernel_size=1,\n",
        "                                    stride=1, padding=0, bias=False)\n",
        "            self.w_As.append(w_a_linear_q)\n",
        "            self.w_Bs.append(w_b_linear_q)\n",
        "            self.w_As.append(w_a_linear_v)\n",
        "            self.w_Bs.append(w_b_linear_v)\n",
        "            self.w_As.append(conv_se_q)\n",
        "            self.w_As.append(conv_se_v)\n",
        "            blk.attn.qkv = _LoRA_qkv_v0_v2(\n",
        "                w_qkv_linear,\n",
        "                w_a_linear_q,\n",
        "                w_b_linear_q,\n",
        "                w_a_linear_v,\n",
        "                w_b_linear_v,\n",
        "                conv_se_q,\n",
        "                conv_se_v,\n",
        "            )\n",
        "        self.reset_parameters()\n",
        "        self.sam = sam_model\n",
        "\n",
        "    def save_lora_parameters(self, filename: str) -> None:\n",
        "        r\"\"\"Only safetensors is supported now.\n",
        "\n",
        "        pip install safetensor if you do not have one installed yet.\n",
        "\n",
        "        save both lora and fc parameters.\n",
        "        \"\"\"\n",
        "\n",
        "        assert filename.endswith(\".pt\") or filename.endswith('.pth')\n",
        "\n",
        "        num_layer = len(self.w_As)  # actually, it is half\n",
        "        a_tensors = {f\"w_a_{i:03d}\": self.w_As[i].weight for i in range(num_layer)}\n",
        "        b_tensors = {f\"w_b_{i:03d}\": self.w_Bs[i].weight for i in range(num_layer)}\n",
        "        prompt_encoder_tensors = {}\n",
        "        mask_decoder_tensors = {}\n",
        "\n",
        "        # save prompt encoder, only `state_dict`, the `named_parameter` is not permitted\n",
        "        if isinstance(self.sam, torch.nn.DataParallel) or isinstance(self.sam, torch.nn.parallel.DistributedDataParallel):\n",
        "            state_dict = self.sam.module.state_dict()\n",
        "        else:\n",
        "            state_dict = self.sam.state_dict()\n",
        "        for key, value in state_dict.items():\n",
        "            if 'prompt_encoder' in key:\n",
        "                prompt_encoder_tensors[key] = value\n",
        "            if 'mask_decoder' in key:\n",
        "                mask_decoder_tensors[key] = value\n",
        "\n",
        "        merged_dict = {**a_tensors, **b_tensors, **prompt_encoder_tensors, **mask_decoder_tensors}\n",
        "        torch.save(merged_dict, filename)\n",
        "\n",
        "    def load_lora_parameters(self, filename: str) -> None:\n",
        "        r\"\"\"Only safetensors is supported now.\n",
        "\n",
        "        pip install safetensor if you do not have one installed yet.\\\n",
        "\n",
        "        load both lora and fc parameters.\n",
        "        \"\"\"\n",
        "\n",
        "        assert filename.endswith(\".pt\") or filename.endswith('.pth')\n",
        "\n",
        "        state_dict = torch.load(filename)\n",
        "\n",
        "        for i, w_A_linear in enumerate(self.w_As):\n",
        "            saved_key = f\"w_a_{i:03d}\"\n",
        "            # print('mobarak:', saved_key)\n",
        "            saved_tensor = state_dict[saved_key]\n",
        "            w_A_linear.weight = Parameter(saved_tensor)\n",
        "\n",
        "        for i, w_B_linear in enumerate(self.w_Bs):\n",
        "            saved_key = f\"w_b_{i:03d}\"\n",
        "            saved_tensor = state_dict[saved_key]\n",
        "            w_B_linear.weight = Parameter(saved_tensor)\n",
        "\n",
        "        sam_dict = self.sam.state_dict()\n",
        "        sam_keys = sam_dict.keys()\n",
        "\n",
        "        # load prompt encoder\n",
        "        prompt_encoder_keys = [k for k in sam_keys if 'prompt_encoder' in k]\n",
        "        prompt_encoder_values = [state_dict[k] for k in prompt_encoder_keys]\n",
        "        prompt_encoder_new_state_dict = {k: v for k, v in zip(prompt_encoder_keys, prompt_encoder_values)}\n",
        "        sam_dict.update(prompt_encoder_new_state_dict)\n",
        "\n",
        "        # load mask decoder\n",
        "        mask_decoder_keys = [k for k in sam_keys if 'mask_decoder' in k]\n",
        "        mask_decoder_values = [state_dict[k] for k in mask_decoder_keys]\n",
        "        mask_decoder_new_state_dict = {k: v for k, v in zip(mask_decoder_keys, mask_decoder_values)}\n",
        "        sam_dict.update(mask_decoder_new_state_dict)\n",
        "        self.sam.load_state_dict(sam_dict)\n",
        "\n",
        "    def reset_parameters(self) -> None:\n",
        "        for w_A in self.w_As:\n",
        "            nn.init.kaiming_uniform_(w_A.weight, a=math.sqrt(5))\n",
        "        for w_B in self.w_Bs:\n",
        "            nn.init.zeros_(w_B.weight)\n",
        "\n",
        "    def forward(self, batched_input, multimask_output, image_size):\n",
        "        return self.sam(batched_input, multimask_output, image_size)"
      ],
      "metadata": {
        "id": "EgNWBG2vjr9i"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Network Training Parameter:"
      ],
      "metadata": {
        "id": "bQLGnu6awRcz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8j2gsPPfB45E"
      },
      "source": [
        "SAM Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXw9Wa1dDFET",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cffb50d-049e-4a0b-9aed-dd3192d2237a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/samed_codes/segment_anything/build_sam.py:131: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(f)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on: cuda train sample size: 798 test sample size: 342 batch: 10\n"
          ]
        }
      ],
      "source": [
        "%cd /content/samed_codes\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "import numpy as np\n",
        "import argparse\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.backends.cudnn as cudnn\n",
        "from importlib import import_module\n",
        "from segment_anything import sam_model_registry\n",
        "from datasets.dataset_synapse import Synapse_dataset\n",
        "from icecream import ic\n",
        "from medpy import metric\n",
        "from scipy.ndimage import zoom\n",
        "import torch.nn as nn\n",
        "import SimpleITK as sitk\n",
        "import torch.nn.functional as F\n",
        "import imageio\n",
        "from einops import repeat\n",
        "\n",
        "from torch.nn.modules.loss import CrossEntropyLoss\n",
        "from utils import DiceLoss\n",
        "import torch.optim as optim\n",
        "from collections import Counter\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def adjust_learning_rate(optimizer, iter_num, args):\n",
        "    if args.warmup and iter_num < args.warmup_period:\n",
        "        lr_ = args.base_lr * ((iter_num + 1) / args.warmup_period)\n",
        "    else:\n",
        "        if args.warmup:\n",
        "            shift_iter = iter_num - args.warmup_period\n",
        "            assert shift_iter >= 0, f'Shift iter is {shift_iter}, smaller than zero'\n",
        "        else:\n",
        "            shift_iter = iter_num\n",
        "        lr_ = args.base_lr * (1.0 - shift_iter / args.max_iterations) ** 0.9\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr_\n",
        "    return lr_\n",
        "\n",
        "def calculate_confusion_matrix_from_arrays(prediction, ground_truth, nr_labels):\n",
        "    replace_indices = np.vstack((\n",
        "        ground_truth.flatten(),\n",
        "        prediction.flatten())\n",
        "    ).T\n",
        "    confusion_matrix, _ = np.histogramdd(\n",
        "        replace_indices,\n",
        "        bins=(nr_labels, nr_labels),\n",
        "        range=[(0, nr_labels), (0, nr_labels)]\n",
        "    )\n",
        "    confusion_matrix = confusion_matrix.astype(np.uint32)\n",
        "    return confusion_matrix\n",
        "\n",
        "def calculate_dice(confusion_matrix):\n",
        "    dices = []\n",
        "    for index in range(confusion_matrix.shape[0]):\n",
        "        true_positives = confusion_matrix[index, index]\n",
        "        false_positives = confusion_matrix[:, index].sum() - true_positives\n",
        "        false_negatives = confusion_matrix[index, :].sum() - true_positives\n",
        "        denom = 2 * true_positives + false_positives + false_negatives\n",
        "        if denom == 0:\n",
        "            dice = 0\n",
        "        else:\n",
        "            dice = 2 * float(true_positives) / denom\n",
        "        dices.append(dice)\n",
        "    return dices\n",
        "\n",
        "def inference_per_epoch(model, testloader, ce_loss, dice_loss, multimask_output=True, args=None):\n",
        "    model.eval()\n",
        "    # fig, axs = plt.subplots(len(testloader), 3, figsize=(1*3, len(testloader)*1), subplot_kw=dict(xticks=[],yticks=[]))\n",
        "    loss_per_epoch, dice_per_epoch = [], []\n",
        "    num_classes = args.num_classes + 1\n",
        "    confusion_matrix = np.zeros((num_classes, num_classes), dtype=np.uint32)\n",
        "    class_wise_dice = []\n",
        "    with torch.no_grad():\n",
        "        for i_batch, sampled_batch in enumerate(testloader):\n",
        "            image_batch, label_batch, low_res_label_batch = sampled_batch['image'],sampled_batch['label'], sampled_batch['low_res_label']\n",
        "            image_batch, label_batch, low_res_label_batch = image_batch.to(device, dtype=torch.float32), label_batch.to(device, dtype=torch.long), low_res_label_batch.to(device, dtype=torch.long)\n",
        "            outputs = model(image_batch, multimask_output, args.img_size)\n",
        "            logits = outputs['masks']\n",
        "            prob = F.softmax(logits, dim=1)\n",
        "            pred_seg = torch.argmax(prob, dim=1)\n",
        "            confusion_matrix += calculate_confusion_matrix_from_arrays(pred_seg.cpu(), label_batch.cpu(), num_classes)\n",
        "            loss, loss_ce, loss_dice = calc_loss(logits, label_batch, ce_loss, dice_loss, args)\n",
        "            loss_per_epoch.append(loss.item())\n",
        "            dice_per_epoch.append(1-loss_dice.item())\n",
        "            low_res_logits = outputs['low_res_logits']\n",
        "            loss_dice = dice_loss(low_res_logits, low_res_label_batch, softmax=True)\n",
        "            img_num = 0\n",
        "            metric_list = []\n",
        "            pred_seg, label_batch = pred_seg.cpu().detach().numpy(), label_batch.cpu().detach().numpy()\n",
        "\n",
        "        confusion_matrix = confusion_matrix[1:, 1:]  # exclude background\n",
        "        dices_per_class = {'dice_cls:{}'.format(cls + 1): dice\n",
        "                    for cls, dice in enumerate(calculate_dice(confusion_matrix))}\n",
        "\n",
        "    return np.mean(loss_per_epoch), np.mean(dice_per_epoch), dices_per_class\n",
        "def seed_everything(seed=42):\n",
        "    cudnn.benchmark = False\n",
        "    cudnn.deterministic = True\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "\n",
        "def calc_loss(output, label_batch, ce_loss, dice_loss, args):\n",
        "    loss_ce = ce_loss(output, label_batch[:].long())\n",
        "    loss_dice = dice_loss(output, label_batch, softmax=True)\n",
        "    loss = (1 - args.dice_weight) * loss_ce + args.dice_weight * loss_dice\n",
        "    return loss, loss_ce, loss_dice\n",
        "\n",
        "\n",
        "def training_per_epoch(model, trainloader, optimizer, iter_num, ce_loss, dice_loss, multimask_output=True, args=None):\n",
        "    model.train()\n",
        "    loss_all = []\n",
        "\n",
        "    for i_batch, sampled_batch in enumerate(trainloader):\n",
        "        image_batch, label_batch, low_res_label_batch = sampled_batch['image'],sampled_batch['label'], sampled_batch['low_res_label']\n",
        "        image_batch, label_batch, low_res_label_batch = image_batch.to(device, dtype=torch.float32), label_batch.to(device, dtype=torch.long), low_res_label_batch.to(device, dtype=torch.long)\n",
        "        batch_dict = {'image_batch':label_batch, 'label_batch':label_batch, 'low_res_label_batch':low_res_label_batch}\n",
        "        outputs = model(image_batch, multimask_output, args.img_size)\n",
        "        output = outputs[args.output_key]\n",
        "        loss_label_batch = batch_dict[args.batch_key]\n",
        "        loss, loss_ce, loss_dice = calc_loss(output, loss_label_batch, ce_loss, dice_loss, args)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        # Update learning rate and increment iteration count\n",
        "        lr_current = adjust_learning_rate(optimizer, iter_num, args)\n",
        "        iter_num += 1\n",
        "\n",
        "        loss_all.append(loss.item())\n",
        "\n",
        "\n",
        "    return np.mean(loss_all), iter_num, lr_current\n",
        "\n",
        "\n",
        "def test_per_epoch(model, testloader, ce_loss, dice_loss, multimask_output=True, args=None):\n",
        "    model.eval()\n",
        "    loss_per_epoch, dice_per_epoch = [], []\n",
        "    with torch.no_grad():\n",
        "        for i_batch, sampled_batch in enumerate(testloader):\n",
        "            image_batch, label_batch, low_res_label_batch = sampled_batch['image'],sampled_batch['label'], sampled_batch['low_res_label']\n",
        "            image_batch, label_batch, low_res_label_batch = image_batch.to(device, dtype=torch.float32), label_batch.to(device, dtype=torch.long), low_res_label_batch.to(device, dtype=torch.long)\n",
        "            batch_dict = {'image_batch':label_batch, 'label_batch':label_batch, 'low_res_label_batch':low_res_label_batch}\n",
        "            outputs = model(image_batch, multimask_output, args.img_size)\n",
        "            output = outputs[args.output_key]\n",
        "            loss_label_batch = batch_dict[args.batch_key]\n",
        "            loss, loss_ce, loss_dice = calc_loss(output, loss_label_batch, ce_loss, dice_loss, args)\n",
        "            loss_per_epoch.append(loss.item())\n",
        "            dice_per_epoch.append(1-loss_dice.item())\n",
        "    return np.mean(loss_per_epoch), np.mean(dice_per_epoch)\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    # Add new arguments\n",
        "    parser.add_argument('--batch_key', type=str, default='low_res_label_batch', help='Key for accessing label batch')\n",
        "    parser.add_argument('--output_key', type=str, default='low_res_logits', help='Key for accessing model outputs')\n",
        "\n",
        "    parser.add_argument('--dice_weight', type=float, default=0.8, help='Weight for dice loss in the loss calculation')\n",
        "    parser.add_argument('--weights', type=int, nargs='+', default=None,\n",
        "                    help='List of weights for each class. Provide space-separated values.')\n",
        "\n",
        "    parser.add_argument('--config', type=str, default=None, help='The config file provided by the trained model')\n",
        "    parser.add_argument('--volume_path', type=str, default='testset/test_vol_h5/')\n",
        "    parser.add_argument('--data_path', type=str, default='Endonasal_Slices_Voxel')\n",
        "    parser.add_argument('--dataset', type=str, default='Synapse', help='Experiment name')\n",
        "    parser.add_argument('--num_classes', type=int, default=2)\n",
        "    parser.add_argument('--list_dir', type=str, default='./lists/lists_Synapse/', help='list_dir')\n",
        "    parser.add_argument('--output_dir', type=str, default='results')\n",
        "    parser.add_argument('--output_file', type=str, default='Endo_best.pt')\n",
        "    parser.add_argument('--img_size', type=int, default=512, help='Input image size of the network')\n",
        "    parser.add_argument('--input_size', type=int, default=224, help='The input size for training SAM model')\n",
        "    parser.add_argument('--seed', type=int,\n",
        "                        default=1234, help='random seed')\n",
        "    parser.add_argument('--is_savenii', action='store_true', help='Whether to save results during inference')\n",
        "    parser.add_argument('--deterministic', type=int, default=1, help='whether use deterministic training')\n",
        "    parser.add_argument('--ckpt', type=str, default='checkpoints/sam_vit_b_01ec64.pth',\n",
        "                        help='Pretrained checkpoint')\n",
        "    parser.add_argument('--lora_ckpt', type=str, default='checkpoints/epoch_159.pth', help='The checkpoint from LoRA')\n",
        "    parser.add_argument('--vit_name', type=str, default='vit_b', help='Select one vit model')\n",
        "    parser.add_argument('--rank', type=int, default=6, help='Rank for LoRA adaptation')\n",
        "    parser.add_argument('--module', type=str, default='sam_lora_image_encoder')\n",
        "\n",
        "    parser.add_argument('--base_lr', type=float, default=0.0005, help='segmentation network learning rate')\n",
        "    parser.add_argument('--batch_size', type=int, default=10, help='batch_size per gpu')\n",
        "    parser.add_argument('--warmup', type=bool, default=True, help='If activated, warp up the learning from a lower lr to the base_lr')\n",
        "    parser.add_argument('--warmup_period', type=int, default=250, help='Warp up iterations, only valid whrn warmup is activated')\n",
        "    parser.add_argument('--AdamW', type=bool, default=True, help='If activated, use AdamW to finetune SAM model')\n",
        "    parser.add_argument('--max_epochs', type=int, default=10, help='maximum epoch number to train')\n",
        "    parser.add_argument('--max_iterations', type=int, default=30000, help='maximum epoch number to train')\n",
        "\n",
        "    if 'ipykernel' in sys.modules:\n",
        "        args = parser.parse_args([])\n",
        "    else:\n",
        "        args = parser.parse_args()\n",
        "\n",
        "    args.output_dir = 'results'\n",
        "    args.ckpt = 'sam_vit_b_01ec64.pth'\n",
        "    args.lora_ckpt = 'results/' + args.output_file\n",
        "    os.makedirs(args.output_dir, exist_ok = True)\n",
        "\n",
        "    sam, img_embedding_size = sam_model_registry[args.vit_name](image_size=args.img_size,\n",
        "                                                                    num_classes=args.num_classes,\n",
        "                                                                    checkpoint=args.ckpt, pixel_mean=[0, 0, 0],\n",
        "                                                                    pixel_std=[1, 1, 1])\n",
        "\n",
        "    # pkg = import_module(args.module)\n",
        "    net = LoRA_Sam_v0_v2(sam, args.rank).cuda()\n",
        "    # net.load_lora_parameters(args.lora_ckpt)\n",
        "    multimask_output = True if args.num_classes > 1 else False\n",
        "    train_dataset = EndonasalDataset(root=(args.data_path+'/Train'), low_res=128, isTrain=True)\n",
        "    test_dataset = EndonasalDataset(root=(args.data_path+'/Test'), low_res=128)\n",
        "    trainloader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=2)\n",
        "    testloader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=2)\n",
        "    print('Training on:', device, 'train sample size:', len(train_dataset), 'test sample size:', len(test_dataset), 'batch:', args.batch_size)\n",
        "\n",
        "    ce_loss = CrossEntropyLoss()\n",
        "    dice_loss = DiceLoss(args.num_classes + 1)\n",
        "    b_lr = args.base_lr / args.warmup_period\n",
        "    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, net.parameters()), lr=b_lr, betas=(0.9, 0.999), weight_decay=0.1)\n",
        "    iter_num = 0\n",
        "\n",
        "    best_epoch, best_loss = 0.0, np.inf\n",
        "    for epoch in range(args.max_epochs):\n",
        "        loss_training, iter_num, lr_current = training_per_epoch(net, trainloader, optimizer, iter_num, ce_loss, dice_loss, multimask_output=multimask_output, args=args)\n",
        "        loss_testing, dice = test_per_epoch(net, testloader, ce_loss, dice_loss,multimask_output=True, args=args)\n",
        "\n",
        "        if loss_testing < best_loss:\n",
        "            best_loss = loss_testing\n",
        "            best_epoch = epoch\n",
        "            net.save_lora_parameters(os.path.join(args.output_dir, args.output_file))\n",
        "\n",
        "        print('--- Epoch {}/{}: Training loss = {:.4f}, Testing: [loss = {:.4f}, dice = {:.4f}], Best loss = {:.4f}, Best epoch = {}, lr = {:.6f}'.\\\n",
        "    format(epoch, args.max_epochs, loss_training, loss_testing, dice, best_loss, best_epoch, lr_current))\n",
        "\n",
        "    assert args.lora_ckpt is not None\n",
        "    net.load_lora_parameters(args.lora_ckpt)\n",
        "    testloader = DataLoader(test_dataset, batch_size=20, shuffle=False, num_workers=2)\n",
        "    test_loss, overall_dice, dices_per_class = inference_per_epoch(net, testloader, ce_loss, dice_loss, multimask_output=True, args=args)\n",
        "    dices_per_class_list = np.array(list(dices_per_class.values()))\n",
        "    print('Class Wise Dice:', dices_per_class)\n",
        "    print('Overall Dice:', np.mean(dices_per_class_list))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    seed_everything()\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    main()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dupTnYetNuNe"
      },
      "source": [
        "Inference: My Dice Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJEAXtUcLSVV"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def calculate_confusion_matrix_from_arrays(prediction, ground_truth, nr_labels):\n",
        "    replace_indices = np.vstack((\n",
        "        ground_truth.flatten(),\n",
        "        prediction.flatten())\n",
        "    ).T\n",
        "    confusion_matrix, _ = np.histogramdd(\n",
        "        replace_indices,\n",
        "        bins=(nr_labels, nr_labels),\n",
        "        range=[(0, nr_labels), (0, nr_labels)]\n",
        "    )\n",
        "    confusion_matrix = confusion_matrix.astype(np.uint32)\n",
        "    return confusion_matrix\n",
        "\n",
        "def calculate_dice(confusion_matrix):\n",
        "    dices = []\n",
        "    for index in range(confusion_matrix.shape[0]):\n",
        "        true_positives = confusion_matrix[index, index]\n",
        "        false_positives = confusion_matrix[:, index].sum() - true_positives\n",
        "        false_negatives = confusion_matrix[index, :].sum() - true_positives\n",
        "        denom = 2 * true_positives + false_positives + false_negatives\n",
        "        if denom == 0:\n",
        "            dice = 0\n",
        "        else:\n",
        "            dice = 2 * float(true_positives) / denom\n",
        "        dices.append(dice)\n",
        "    return dices\n",
        "\n",
        "def test_per_epoch(model, testloader, ce_loss, dice_loss, multimask_output=True, args=None):\n",
        "    model.eval()\n",
        "    fig, axs = plt.subplots(len(testloader), 3, figsize=(1*3, len(testloader)*1), subplot_kw=dict(xticks=[],yticks=[]))\n",
        "    loss_per_epoch, dice_per_epoch = [], []\n",
        "    num_classes = args.num_classes + 1\n",
        "    confusion_matrix = np.zeros((num_classes, num_classes), dtype=np.uint32)\n",
        "    class_wise_dice = []\n",
        "    with torch.no_grad():\n",
        "        for i_batch, sampled_batch in enumerate(testloader):\n",
        "            image_batch, label_batch, low_res_label_batch = sampled_batch['image'],sampled_batch['label'], sampled_batch['low_res_label']\n",
        "            image_batch, label_batch, low_res_label_batch = image_batch.to(device, dtype=torch.float32), label_batch.to(device, dtype=torch.long), low_res_label_batch.to(device, dtype=torch.long)\n",
        "            outputs = model(image_batch, multimask_output, args.img_size)\n",
        "            logits = outputs['masks']\n",
        "            prob = F.softmax(logits, dim=1)\n",
        "            pred_seg = torch.argmax(prob, dim=1)\n",
        "            confusion_matrix += calculate_confusion_matrix_from_arrays(pred_seg.cpu(), label_batch.cpu(), num_classes)\n",
        "            loss, loss_ce, loss_dice = calc_loss(outputs, low_res_label_batch, ce_loss, dice_loss)\n",
        "            loss_per_epoch.append(loss.item())\n",
        "            dice_per_epoch.append(1-loss_dice.item())\n",
        "            low_res_logits = outputs['low_res_logits']\n",
        "            loss_dice = dice_loss(low_res_logits, low_res_label_batch, softmax=True)\n",
        "            img_num = 0\n",
        "            axs[i_batch, 0].imshow(image_batch[img_num, 0].cpu().numpy(), cmap='gray')\n",
        "            axs[i_batch, 1].imshow(label_batch[img_num].cpu().numpy(), cmap='gray')\n",
        "            axs[i_batch, 2].imshow(pred_seg[img_num].cpu().numpy(), cmap='gray')\n",
        "            metric_list = []\n",
        "            pred_seg, label_batch = pred_seg.cpu().detach().numpy(), label_batch.cpu().detach().numpy()\n",
        "\n",
        "        confusion_matrix = confusion_matrix[1:, 1:]  # exclude background\n",
        "        dices_per_class = {'dice_cls:{}'.format(cls + 1): dice\n",
        "                    for cls, dice in enumerate(calculate_dice(confusion_matrix))}\n",
        "\n",
        "    return np.mean(loss_per_epoch), np.mean(dice_per_epoch), dices_per_class\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--config', type=str, default=None, help='The config file provided by the trained model')\n",
        "    parser.add_argument('--volume_path', type=str, default='testset/test_vol_h5/')\n",
        "    parser.add_argument('--dataset', type=str, default='Synapse', help='Experiment name')\n",
        "    parser.add_argument('--num_classes', type=int, default=2)\n",
        "    parser.add_argument('--list_dir', type=str, default='./lists/lists_Synapse/', help='list_dir')\n",
        "    parser.add_argument('--output_dir', type=str, default='results')\n",
        "    parser.add_argument('--img_size', type=int, default=512, help='Input image size of the network')\n",
        "    parser.add_argument('--input_size', type=int, default=224, help='The input size for training SAM model')\n",
        "    parser.add_argument('--seed', type=int,\n",
        "                        default=1234, help='random seed')\n",
        "    parser.add_argument('--is_savenii', action='store_true', help='Whether to save results during inference')\n",
        "    parser.add_argument('--deterministic', type=int, default=1, help='whether use deterministic training')\n",
        "    parser.add_argument('--ckpt', type=str, default='checkpoints/sam_vit_b_01ec64.pth',\n",
        "                        help='Pretrained checkpoint')\n",
        "    parser.add_argument('--lora_ckpt', type=str, default='checkpoints/epoch_159.pth', help='The checkpoint from LoRA')\n",
        "    parser.add_argument('--vit_name', type=str, default='vit_b', help='Select one vit model')\n",
        "    parser.add_argument('--rank', type=int, default=4, help='Rank for LoRA adaptation')\n",
        "    parser.add_argument('--module', type=str, default='sam_lora_image_encoder')\n",
        "\n",
        "    parser.add_argument('--base_lr', type=float, default=0.005, help='segmentation network learning rate')\n",
        "    parser.add_argument('--batch_size', type=int, default=12, help='batch_size per gpu')\n",
        "    parser.add_argument('--warmup', type=bool, default=True, help='If activated, warp up the learning from a lower lr to the base_lr')\n",
        "    parser.add_argument('--warmup_period', type=int, default=250, help='Warp up iterations, only valid whrn warmup is activated')\n",
        "    parser.add_argument('--AdamW', type=bool, default=True, help='If activated, use AdamW to finetune SAM model')\n",
        "    parser.add_argument('--max_epochs', type=int, default=10, help='maximum epoch number to train')\n",
        "    parser.add_argument('--max_iterations', type=int, default=30000, help='maximum epoch number to train')\n",
        "\n",
        "\n",
        "    if 'ipykernel' in sys.modules:\n",
        "        args = parser.parse_args([])\n",
        "    else:\n",
        "        args = parser.parse_args()\n",
        "\n",
        "    args.ckpt = 'sam_vit_b_01ec64.pth'\n",
        "    args.lora_ckpt = 'results/model_best.pt'\n",
        "    sam, img_embedding_size = sam_model_registry[args.vit_name](image_size=args.img_size,\n",
        "                                                                    num_classes=args.num_classes,\n",
        "                                                                    checkpoint=args.ckpt, pixel_mean=[0, 0, 0],\n",
        "                                                                    pixel_std=[1, 1, 1])\n",
        "\n",
        "    net = LoRA_Sam_v0_v2(sam, args.rank).cuda()\n",
        "    ce_loss = CrossEntropyLoss()\n",
        "    dice_loss = DiceLoss(args.num_classes + 1)\n",
        "\n",
        "    assert args.lora_ckpt is not None\n",
        "    net.load_lora_parameters(args.lora_ckpt)\n",
        "    testloader = DataLoader(test_dataset, batch_size=20, shuffle=False, num_workers=2)\n",
        "    test_loss, overall_dice, dices_per_class = test_per_epoch(net, testloader, ce_loss, dice_loss, multimask_output=True, args=args)\n",
        "    dices_per_class_list = np.array(list(dices_per_class.values()))\n",
        "    print('Class Wise Dice:', dices_per_class)\n",
        "    print('Overall Dice:', np.mean(dices_per_class_list))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference MRI to MRI<br>\n",
        "download test mri"
      ],
      "metadata": {
        "id": "OeaaFJN0iUvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "url = 'https://drive.google.com/uc?id=1zcvnBscFVI2v5ieAlGGnp0zpX8WmIrD4'\n",
        "gdown.download(url,'endonasal_mri_patients.zip',quiet=True)\n",
        "!unzip -q endonasal_mri_patients\n",
        "!rm -rf /content/endonasal_mri_patients/.DS_Store\n",
        "!rm -rf /content/endonasal_mri_patients/**/.DS_Store"
      ],
      "metadata": {
        "id": "u55aRX3KjbPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MRI to MRI"
      ],
      "metadata": {
        "id": "3mLQJNoU0rId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nibabel as nib\n",
        "import cv2\n",
        "import numpy as np\n",
        "from scipy.ndimage import zoom\n",
        "from einops import repeat\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def read_mri(mri_path):\n",
        "    img_meta = nib.load(mri_path)\n",
        "    array = img_meta.get_fdata()\n",
        "    return np.rot90(array)\n",
        "\n",
        "def normalise_intensity(image, ROI_thres=0.1):\n",
        "    pixel_thres = np.percentile(image, ROI_thres)\n",
        "    ROI = np.where(image > pixel_thres, image, 0) # If image value is greater than pixel threshold, return image value, otherwise return 0\n",
        "    mean = np.mean(ROI)\n",
        "    std = np.std(ROI)\n",
        "    ROI_norm = (ROI - mean) / (std + 1e-8) # Normalise ROI\n",
        "    return ROI_norm\n",
        "\n",
        "class EndonasalDataset_MRI(Dataset):\n",
        "    def __init__(self, root='endonasal_mri_patients', patient=None, low_res=None, isTrain=False):\n",
        "\n",
        "        mri_path = 'endonasal_mri_patients/mri0{}/mri0{}_t1c.nii.gz'.format(patient, patient)\n",
        "        self.mask_path = 'endonasal_mri_patients/mri0{}/mri0{}_mask.nii.gz'.format(patient, patient)\n",
        "        mri_array = read_mri(mri_path)\n",
        "        self.image_all = []\n",
        "        for z in range(mri_array.shape[2]):\n",
        "            normalized_slice = cv2.normalize(mri_array[:, :, z], None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
        "            self.image_all.append(normalise_intensity(normalized_slice))\n",
        "\n",
        "        self.mask_all = read_mri(self.mask_path)\n",
        "        self.isTrain = isTrain\n",
        "        self.low_res = low_res\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.image_all)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image = self.image_all[index]\n",
        "        image = zoom(image, (512/image.shape[0], 512/image.shape[1]), order=0)\n",
        "        label = self.mask_all[:,:,index]\n",
        "        label = zoom(label, (512/label.shape[0], 512/label.shape[1]), order=0)\n",
        "        if self.isTrain:\n",
        "            if random.random() > 0.5:\n",
        "                image, label = random_rot_flip(image, label)\n",
        "            elif random.random() > 0.5:\n",
        "                image, label = random_rotate(image, label)\n",
        "\n",
        "        image = repeat(np.expand_dims(image, axis=0), 'c h w -> (repeat c) h w', repeat=3)\n",
        "        sample = {'image': image, 'label': label}\n",
        "        if self.low_res:\n",
        "            low_res_label = zoom(label, (self.low_res/label.shape[0], self.low_res/label.shape[1]), order=0)\n",
        "            sample = {'image': image, 'label': label, 'low_res_label': low_res_label, 'maskpath': self.mask_path}\n",
        "\n",
        "        return sample\n",
        "\n",
        "\n",
        "def calculate_confusion_matrix_from_arrays(prediction, ground_truth, nr_labels):\n",
        "    replace_indices = np.vstack((\n",
        "        ground_truth.flatten(),\n",
        "        prediction.flatten())\n",
        "    ).T\n",
        "    confusion_matrix, _ = np.histogramdd(\n",
        "        replace_indices,\n",
        "        bins=(nr_labels, nr_labels),\n",
        "        range=[(0, nr_labels), (0, nr_labels)]\n",
        "    )\n",
        "    confusion_matrix = confusion_matrix.astype(np.uint32)\n",
        "    return confusion_matrix\n",
        "\n",
        "def calculate_dice(confusion_matrix):\n",
        "    dices = []\n",
        "    for index in range(confusion_matrix.shape[0]):\n",
        "        true_positives = confusion_matrix[index, index]\n",
        "        false_positives = confusion_matrix[:, index].sum() - true_positives\n",
        "        false_negatives = confusion_matrix[index, :].sum() - true_positives\n",
        "        denom = 2 * true_positives + false_positives + false_negatives\n",
        "        if denom == 0:\n",
        "            dice = 0\n",
        "        else:\n",
        "            dice = 2 * float(true_positives) / denom\n",
        "        dices.append(dice)\n",
        "    return dices\n",
        "\n",
        "def pred_to_mri(pred_seg_all, mask_path):\n",
        "    os.makedirs('predicted_mri', mode = 0o777, exist_ok = True)\n",
        "    img_meta = nib.load(mask_path)\n",
        "    pred_seg_all = np.rot90(np.array(pred_seg_all).transpose(1,2,0), k=-1)\n",
        "    img_nifti = nib.Nifti1Image(pred_seg_all, img_meta.affine, header=img_meta.header)\n",
        "    nib.save(img_nifti,'predicted_mri/'+os.path.basename(mask_path))\n",
        "\n",
        "def test_per_epoch(model, testloader, ce_loss, dice_loss, multimask_output=True, args=None):\n",
        "    model.eval()\n",
        "    loss_per_epoch, dice_per_epoch = [], []\n",
        "    num_classes = args.num_classes + 1\n",
        "    confusion_matrix = np.zeros((num_classes, num_classes), dtype=np.uint32)\n",
        "    class_wise_dice = []\n",
        "    pred_seg_all = []\n",
        "    with torch.no_grad():\n",
        "        for i_batch, sampled_batch in enumerate(testloader):\n",
        "            image_batch, label_batch, low_res_label_batch = sampled_batch['image'],sampled_batch['label'], sampled_batch['low_res_label']\n",
        "            image_batch, label_batch, low_res_label_batch = image_batch.to(device, dtype=torch.float32), label_batch.to(device, dtype=torch.long), low_res_label_batch.to(device, dtype=torch.long)\n",
        "            outputs = model(image_batch, multimask_output, args.img_size)\n",
        "            logits = outputs['masks']\n",
        "            prob = F.softmax(logits, dim=1)\n",
        "            pred_seg = torch.argmax(prob, dim=1)\n",
        "            pred_seg_all.extend(pred_seg.detach().cpu().numpy())\n",
        "            confusion_matrix += calculate_confusion_matrix_from_arrays(pred_seg.cpu(), label_batch.cpu(), num_classes)\n",
        "            loss, loss_ce, loss_dice = calc_loss(outputs, low_res_label_batch, ce_loss, dice_loss)\n",
        "            loss_per_epoch.append(loss.item())\n",
        "            dice_per_epoch.append(1-loss_dice.item())\n",
        "            low_res_logits = outputs['low_res_logits']\n",
        "            loss_dice = dice_loss(low_res_logits, low_res_label_batch, softmax=True)\n",
        "            metric_list = []\n",
        "            pred_seg, label_batch = pred_seg.cpu().detach().numpy(), label_batch.cpu().detach().numpy()\n",
        "\n",
        "        pred_to_mri(np.array(pred_seg_all), sampled_batch['maskpath'][0])\n",
        "        confusion_matrix = confusion_matrix[1:, 1:]  # exclude background\n",
        "        dices_per_class = {'dice_cls:{}'.format(cls + 1): round(dice, 4)\n",
        "                    for cls, dice in enumerate(calculate_dice(confusion_matrix))}\n",
        "\n",
        "    return np.mean(loss_per_epoch), np.mean(dice_per_epoch), dices_per_class\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--config', type=str, default=None, help='The config file provided by the trained model')\n",
        "    parser.add_argument('--volume_path', type=str, default='testset/test_vol_h5/')\n",
        "    parser.add_argument('--dataset', type=str, default='Synapse', help='Experiment name')\n",
        "    parser.add_argument('--num_classes', type=int, default=2)\n",
        "    parser.add_argument('--list_dir', type=str, default='./lists/lists_Synapse/', help='list_dir')\n",
        "    parser.add_argument('--output_dir', type=str, default='results')\n",
        "    parser.add_argument('--img_size', type=int, default=512, help='Input image size of the network')\n",
        "    parser.add_argument('--input_size', type=int, default=224, help='The input size for training SAM model')\n",
        "    parser.add_argument('--seed', type=int,\n",
        "                        default=1234, help='random seed')\n",
        "    parser.add_argument('--is_savenii', action='store_true', help='Whether to save results during inference')\n",
        "    parser.add_argument('--deterministic', type=int, default=1, help='whether use deterministic training')\n",
        "    parser.add_argument('--ckpt', type=str, default='checkpoints/sam_vit_b_01ec64.pth',\n",
        "                        help='Pretrained checkpoint')\n",
        "    parser.add_argument('--lora_ckpt', type=str, default='checkpoints/epoch_159.pth', help='The checkpoint from LoRA')\n",
        "    parser.add_argument('--vit_name', type=str, default='vit_b', help='Select one vit model')\n",
        "    parser.add_argument('--rank', type=int, default=4, help='Rank for LoRA adaptation')\n",
        "    parser.add_argument('--module', type=str, default='sam_lora_image_encoder')\n",
        "\n",
        "    parser.add_argument('--base_lr', type=float, default=0.005, help='segmentation network learning rate')\n",
        "    parser.add_argument('--batch_size', type=int, default=12, help='batch_size per gpu')\n",
        "    parser.add_argument('--warmup', type=bool, default=True, help='If activated, warp up the learning from a lower lr to the base_lr')\n",
        "    parser.add_argument('--warmup_period', type=int, default=250, help='Warp up iterations, only valid whrn warmup is activated')\n",
        "    parser.add_argument('--AdamW', type=bool, default=True, help='If activated, use AdamW to finetune SAM model')\n",
        "    parser.add_argument('--max_epochs', type=int, default=10, help='maximum epoch number to train')\n",
        "    parser.add_argument('--max_iterations', type=int, default=30000, help='maximum epoch number to train')\n",
        "\n",
        "\n",
        "    if 'ipykernel' in sys.modules:\n",
        "        args = parser.parse_args([])\n",
        "    else:\n",
        "        args = parser.parse_args()\n",
        "\n",
        "    args.ckpt = 'sam_vit_b_01ec64.pth'\n",
        "    args.lora_ckpt = 'results/model_best.pt'\n",
        "    sam, img_embedding_size = sam_model_registry[args.vit_name](image_size=args.img_size,\n",
        "                                                                    num_classes=args.num_classes,\n",
        "                                                                    checkpoint=args.ckpt, pixel_mean=[0, 0, 0],\n",
        "                                                                    pixel_std=[1, 1, 1])\n",
        "\n",
        "    net = LoRA_Sam_v0_v2(sam, args.rank).cuda()\n",
        "    ce_loss = CrossEntropyLoss()\n",
        "    dice_loss = DiceLoss(args.num_classes + 1)\n",
        "\n",
        "    assert args.lora_ckpt is not None\n",
        "    net.load_lora_parameters(args.lora_ckpt)\n",
        "\n",
        "    patients = ['154', '169', '170']\n",
        "    mean_overall = []\n",
        "    tumor_overall = []\n",
        "    carotid_overall = []\n",
        "    for patient in patients:\n",
        "        test_dataset = EndonasalDataset_MRI(root='endonasal_mri_patients', patient=patient, low_res=128)\n",
        "        testloader = DataLoader(test_dataset, batch_size=20, shuffle=False, num_workers=2)\n",
        "        test_loss, overall_dice, dices_per_class = test_per_epoch(net, testloader, ce_loss, dice_loss, multimask_output=True, args=args)\n",
        "        dices_per_class_list = np.array(list(dices_per_class.values()))\n",
        "        overall = round(np.mean(dices_per_class_list),4)\n",
        "        mean_overall.append(overall)\n",
        "        tumor_overall.append(dices_per_class['dice_cls:1'])\n",
        "        carotid_overall.append(dices_per_class['dice_cls:2'])\n",
        "        print('Patient:', patient, ',Class Wise:', dices_per_class, ',Overall :', overall)\n",
        "\n",
        "    print('Overall Model Performance [Mean Overall]:', round(np.mean(mean_overall),4), '[cls-1:{}]'.format(round(np.mean(tumor_overall),4)),\\\n",
        "            '[cls-2:{}]'.format(round(np.mean(carotid_overall),4)))\n"
      ],
      "metadata": {
        "id": "IJKKAr8UiTry",
        "outputId": "1bbecab0-dff9-4478-d4f4-6c5916cb1ca9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Patient: 154 ,Class Wise: {'dice_cls:1': 0.9737, 'dice_cls:2': 0.7039} ,Overall : 0.8388\n",
            "Patient: 169 ,Class Wise: {'dice_cls:1': 0.991, 'dice_cls:2': 0.9621} ,Overall : 0.9766\n",
            "Patient: 170 ,Class Wise: {'dice_cls:1': 0.9894, 'dice_cls:2': 0.8928} ,Overall : 0.9411\n",
            "Overall Model Performance [Mean Overall]: 0.9188 [cls-1:0.9847] [cls-2:0.8529]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Post Processing to remove outliers (small seg)"
      ],
      "metadata": {
        "id": "_dBB8gMP8yHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage import morphology\n",
        "import nibabel as nib\n",
        "mask_meta = nib.load('/content/samed_codes/endonasal_mri_patients/mri0169/mri0169_mask.nii.gz')\n",
        "mask = nib.load('/content/samed_codes/predicted_mri/mri0169_mask.nii.gz').get_fdata()\n",
        "\n",
        "binary_mask = morphology.remove_small_objects(mask>0, 50)\n",
        "mask[binary_mask==0] = 0\n",
        "img_nifti = nib.Nifti1Image(mask, mask_meta.affine, header=mask_meta.header)\n",
        "nib.save(img_nifti,'mri0169_mask_post_processed.nii.gz')"
      ],
      "metadata": {
        "id": "c8y3CsOW1O6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Er8Wnp5M85Zd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}