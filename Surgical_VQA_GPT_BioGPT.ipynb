{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "daaddbe39a9d48b39e8bd21767c49488": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a5c53149f32d48b18437c59beab4bf79",
              "IPY_MODEL_11e09a1170c14c31a7f9297ac4a04f9a",
              "IPY_MODEL_3385d92b209e4638b1c126cf3eba5def"
            ],
            "layout": "IPY_MODEL_d4409187ed76424bbe035bec7abf2402"
          }
        },
        "a5c53149f32d48b18437c59beab4bf79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f996f16e3f14b979f2a68335b468196",
            "placeholder": "​",
            "style": "IPY_MODEL_45fd6be400bc4c08a8686a2f384227b9",
            "value": "Downloading (…)&quot;pytorch_model.bin&quot;;: 100%"
          }
        },
        "11e09a1170c14c31a7f9297ac4a04f9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21f2cde5e5fe439189ca9d717ea9cda8",
            "max": 1560781537,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d03bb1e3fda945fca91d32bb0941d4cc",
            "value": 1560781537
          }
        },
        "3385d92b209e4638b1c126cf3eba5def": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_075e5a31742b4f3ab9fcc46a8a3ddcea",
            "placeholder": "​",
            "style": "IPY_MODEL_9eeed93d37ec41dcb32ce18cb9c613f8",
            "value": " 1.56G/1.56G [00:34&lt;00:00, 43.5MB/s]"
          }
        },
        "d4409187ed76424bbe035bec7abf2402": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f996f16e3f14b979f2a68335b468196": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45fd6be400bc4c08a8686a2f384227b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "21f2cde5e5fe439189ca9d717ea9cda8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d03bb1e3fda945fca91d32bb0941d4cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "075e5a31742b4f3ab9fcc46a8a3ddcea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9eeed93d37ec41dcb32ce18cb9c613f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef5cf6978ffa4480804cc676f860f6a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_71f8c62331824acc9845dce9906c4391",
              "IPY_MODEL_30bf1ecc0ab04dc4a4f0b7dc25233faf",
              "IPY_MODEL_620dfc78080c4896b5dc79ae971e9ebd"
            ],
            "layout": "IPY_MODEL_d658a694f2eb457e8841f01307e4f790"
          }
        },
        "71f8c62331824acc9845dce9906c4391": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23c76a95e0ef48db9c124a8c53339663",
            "placeholder": "​",
            "style": "IPY_MODEL_4d31d3897c224e95b4428f8ef2effc45",
            "value": "100%"
          }
        },
        "30bf1ecc0ab04dc4a4f0b7dc25233faf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_320a0bb8f9c341cc922f297d3fedd5fb",
            "max": 46830571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4946aa447f524c7abf33405471083259",
            "value": 46830571
          }
        },
        "620dfc78080c4896b5dc79ae971e9ebd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4c8878d808c4a98b918f32e5124f518",
            "placeholder": "​",
            "style": "IPY_MODEL_d712a203406f4507a7a1cc3ff6413baf",
            "value": " 44.7M/44.7M [00:00&lt;00:00, 88.7MB/s]"
          }
        },
        "d658a694f2eb457e8841f01307e4f790": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23c76a95e0ef48db9c124a8c53339663": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d31d3897c224e95b4428f8ef2effc45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "320a0bb8f9c341cc922f297d3fedd5fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4946aa447f524c7abf33405471083259": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f4c8878d808c4a98b918f32e5124f518": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d712a203406f4507a7a1cc3ff6413baf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ad1d7a6e32ef42ffa4a4885045530460": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cfd09395071c4615bca50a7945ca2909",
              "IPY_MODEL_24950234f99342f5935dedfd981f200f",
              "IPY_MODEL_dca45872852e49838a52cdafd796cbe5"
            ],
            "layout": "IPY_MODEL_7690f8e096c34e0bbd5af5b971b4d74a"
          }
        },
        "cfd09395071c4615bca50a7945ca2909": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f63ec2e1292342d09c47dc64be251c7e",
            "placeholder": "​",
            "style": "IPY_MODEL_a13f9f781fb944d8a304518d7668bba9",
            "value": "100%"
          }
        },
        "24950234f99342f5935dedfd981f200f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1cc55d1888064f399e9ea765f2e5ae13",
            "max": 46830571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3498f3d96e7e480495115b9bc27c5045",
            "value": 46830571
          }
        },
        "dca45872852e49838a52cdafd796cbe5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea12db800c664382a8e4ea35d530efb6",
            "placeholder": "​",
            "style": "IPY_MODEL_c69b111dbb584e68baf41bfb7111ddcc",
            "value": " 44.7M/44.7M [00:01&lt;00:00, 73.5MB/s]"
          }
        },
        "7690f8e096c34e0bbd5af5b971b4d74a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f63ec2e1292342d09c47dc64be251c7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a13f9f781fb944d8a304518d7668bba9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1cc55d1888064f399e9ea765f2e5ae13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3498f3d96e7e480495115b9bc27c5045": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ea12db800c664382a8e4ea35d530efb6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c69b111dbb584e68baf41bfb7111ddcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/tutorial_notebooks/blob/main/Surgical_VQA_GPT_BioGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VQA Surgery (Naive version)\n",
        "\n",
        "sentence-transformers by UKPLab (To classify text) <br>\n",
        "page: https://www.libhunt.com/r/sentence-transformers<br>\n",
        "github: https://github.com/UKPLab/sentence-transformers"
      ],
      "metadata": {
        "id": "jZ790EGOPO5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "url = 'https://www.frontiersin.org/files/MyHome%20Article%20Library/446547/446547_Thumb_400.jpg'\n",
        "img = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "labels = [ 'grasping', 'retraction', 'tissue manipulation', 'tool manipulation', \n",
        "          'cutting', 'cauterization', 'suction', 'looping', 'suturing', 'clipping', 'staple', 'ultrasound sensing']\n",
        "\n",
        "preprocess = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )])\n",
        "\n",
        "\n",
        "num_classes = 12\n",
        "img_preprocessed = preprocess(img)\n",
        "batch_img_tensor = torch.unsqueeze(img_preprocessed, 0)\n",
        "\n",
        "model = models.resnet50(pretrained=True)\n",
        "model.fc = torch.nn.Linear(model.fc.in_features, num_classes)\n",
        "model.eval();\n",
        "new_fc = torch.nn.Sequential(*list(model.fc.children())[:-1])\n",
        "model.fc = new_fc\n",
        "img_features = model(batch_img_tensor)\n",
        "print(img_features.shape)\n",
        "\n",
        "class Surgical_VQA(nn.Module):\n",
        "    def __init__(self, num_classes=12):\n",
        "        super(Surgical_VQA, self).__init__()\n",
        "        #self.num_classes = num_classes\n",
        "\n",
        "        # text processing\n",
        "        self.text_feature_extractor = SentenceTransformer('bert-base-nli-mean-tokens').cuda()\n",
        "        # image processing\n",
        "        self.img_feature_extractor = models.resnet50(pretrained=True)\n",
        "        new_fc = nn.Sequential(*list(self.img_feature_extractor.fc.children())[:-1])\n",
        "        self.img_feature_extractor.fc = new_fc\n",
        "\n",
        "        #classifier\n",
        "        self.classifier = nn.Linear(2816, num_classes)\n",
        "\n",
        "    def forward(self, img, text):\n",
        "        img_feature = self.img_feature_extractor(img)\n",
        "        text_feature = self.text_feature_extractor.encode([text])[0]\n",
        "        img_text_features = torch.cat((img_feature, torch.tensor(text_feature).unsqueeze(0).cuda()), dim=1)\n",
        "        out = self.classifier(img_text_features)\n",
        "        return out\n",
        "\n",
        "text = \"What is the state of bipolar_forceps?\"\n",
        "SVQA = Surgical_VQA(num_classes=12).cuda()\n",
        "output = SVQA(batch_img_tensor.cuda(), text)\n",
        "answer = output.argmax(dim=1)\n",
        "print('Question: {} \\nAnswer: {}'.format(text, labels[answer.item()]))"
      ],
      "metadata": {
        "id": "iZTAuDsANuzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#VQA: VisualBERT + ResNet (Early Fusion)"
      ],
      "metadata": {
        "id": "zn7vfq74YgK7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install transformers"
      ],
      "metadata": {
        "id": "sx_btuvAjZcb"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "from torchvision import transforms\n",
        "from transformers import VisualBertModel, VisualBertConfig, BertTokenizerFast\n",
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "class VisualBERT_VQA(nn.Module):\n",
        "    def __init__(self, num_class=2):\n",
        "        super(VisualBERT_VQA, self).__init__()\n",
        "        self.visualbert = VisualBertModel.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n",
        "        self.cls = nn.Linear(768, num_class)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        last_hidden_state = self.visualbert(**inputs).last_hidden_state #[1, 56, 768]\n",
        "\n",
        "        # Get the index of the last text token\n",
        "        index_to_gather = inputs['attention_mask'].sum(1) - 2  # as in original code 5\n",
        "        index_to_gather = (\n",
        "            index_to_gather.unsqueeze(-1).unsqueeze(-1).expand(index_to_gather.size(0), 1, last_hidden_state.size(-1))\n",
        "        ) # [b c hw]=[1, 1, 768]\n",
        "\n",
        "        pooled_output = torch.gather(last_hidden_state, 1, index_to_gather) # [1, 1, 768]\n",
        "        logits = self.cls(pooled_output).squeeze(1)\n",
        "        return logits\n",
        "\n",
        "\n",
        "url = 'https://www.frontiersin.org/files/MyHome%20Article%20Library/446547/446547_Thumb_400.jpg'\n",
        "img = Image.open(requests.get(url, stream=True).raw)\n",
        "preprocess = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )])\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "labels = [ 'grasping', 'retraction', 'tissue manipulation', 'tool manipulation', \n",
        "          'cutting', 'cauterization', 'suction', 'looping', 'suturing', 'clipping', 'staple', 'ultrasound sensing']\n",
        "num_classes = 12\n",
        "\n",
        "#visual feature\n",
        "img_preprocessed = preprocess(img)\n",
        "batch_img_tensor = torch.stack([img_preprocessed, img_preprocessed])\n",
        "model_visual_feat = models.resnet50(pretrained=True)\n",
        "model_visual_feat.avgpool = nn.Identity()\n",
        "model_visual_feat.fc = nn.Identity()\n",
        "model_visual_feat.eval()\n",
        "visual_embeds = model_visual_feat(batch_img_tensor).view(-1, 49, 2048)\n",
        "visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long)\n",
        "visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float)\n",
        "\n",
        "#text feature\n",
        "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
        "question = \"What is the state of bipolar_forceps?\"\n",
        "questions = list([question, question])\n",
        "inputs = tokenizer(questions, return_tensors=\"pt\", padding=\"max_length\",max_length=20,)\n",
        "\n",
        "\n",
        "#update diction \n",
        "inputs.update(\n",
        "    {\n",
        "        \"visual_embeds\": visual_embeds,\n",
        "        \"visual_token_type_ids\": visual_token_type_ids,\n",
        "        \"visual_attention_mask\": visual_attention_mask,\n",
        "    }\n",
        ")\n",
        "\n",
        "print('visual_embeds', inputs['visual_embeds'].shape, 'Text:', inputs['input_ids'].shape)\n",
        "model = VisualBERT_VQA(num_class=18)\n",
        "model.eval()\n",
        "logits = model(inputs)\n",
        "pred_vqa = logits.argmax(-1)\n",
        "print('Logits:',logits, 'Prediction:', pred_vqa)  \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpo2NQZPYmd1",
        "outputId": "72dba108-a720-478c-fa36-a4b753bdef09"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "visual_embeds torch.Size([2, 49, 2048]) Text: torch.Size([2, 20])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at uclanlp/visualbert-vqa-coco-pre were not used when initializing VisualBertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing VisualBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing VisualBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits: tensor([[ 0.1263,  0.3838, -0.0967, -0.2127,  0.0030,  0.4668,  0.0651,  0.0341,\n",
            "          0.1571,  0.0366, -0.2244, -0.2846,  0.4410, -0.0932,  0.1033, -0.1001,\n",
            "          0.4315, -0.0655],\n",
            "        [ 0.1263,  0.3838, -0.0967, -0.2127,  0.0030,  0.4668,  0.0651,  0.0341,\n",
            "          0.1571,  0.0366, -0.2244, -0.2846,  0.4410, -0.0932,  0.1033, -0.1001,\n",
            "          0.4315, -0.0655]], grad_fn=<SqueezeBackward1>) Prediction: tensor([5, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#VQA: ChatGPT + ResNet (Early Fusion)"
      ],
      "metadata": {
        "id": "_YkrY6w9v9TL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from transformers import VisualBertModel, BertTokenizer, VisualBertConfig, GPT2Model, GPT2Tokenizer, GPT2Config\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class GPT2_VQA(nn.Module):\n",
        "    def __init__(self, num_class=2):\n",
        "        super(GPT2_VQA, self).__init__()\n",
        "        self.gpt2 = GPT2Model.from_pretrained('gpt2')\n",
        "        self.config = GPT2Config.from_pretrained(\"gpt2\")\n",
        "        self.classifier = nn.Linear(59 * 768, num_class)\n",
        "\n",
        "        self.config_bert = VisualBertConfig.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n",
        "        self.config_bert.visual_embedding_dim = 2048 #most right dim of the visual features\n",
        "        self.config_bert.hidden_size = self.config.hidden_size\n",
        "        self.config_bert.vocab_size = self.config.vocab_size \n",
        "        self.config_bert.pad_token_id = self.config.pad_token_id \n",
        "\n",
        "        self.visualbert = VisualBertModel(config=self.config_bert)\n",
        "        self.embeddings = self.visualbert.embeddings\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        hidden_states = self.embeddings(\n",
        "            input_ids=inputs['input_ids'],\n",
        "            token_type_ids=inputs['token_type_ids'],\n",
        "            position_ids=None,\n",
        "            inputs_embeds=None,\n",
        "            visual_embeds=inputs['visual_embeds'],\n",
        "            visual_token_type_ids=inputs['visual_token_type_ids'],\n",
        "            image_text_alignment=None,\n",
        "        )\n",
        "\n",
        "        hidden_states = self.gpt2.drop(hidden_states)\n",
        "        input_shape = inputs['input_ids'].size()\n",
        "        visual_input_shape = inputs['visual_embeds'].size()[:-1]\n",
        "        combined_attention_mask = torch.cat((inputs['attention_mask'], inputs['visual_attention_mask']), dim=-1)\n",
        "        extended_attention_mask: torch.Tensor = self.gpt2.get_extended_attention_mask(\n",
        "            combined_attention_mask, (input_shape[0], input_shape + visual_input_shape)\n",
        "        )\n",
        "        output_attentions = self.config.output_attentions\n",
        "        head_mask = self.gpt2.get_head_mask(None, self.config.n_layer)\n",
        "        past_key_values = tuple([None] * len(self.gpt2.h))\n",
        "        for i, (block, layer_past) in enumerate(zip(self.gpt2.h, past_key_values)):\n",
        "            outputs = block(\n",
        "                    hidden_states,\n",
        "                    layer_past=layer_past,\n",
        "                    attention_mask=extended_attention_mask,\n",
        "                    head_mask=head_mask[i],\n",
        "                    encoder_hidden_states=None,\n",
        "                    encoder_attention_mask=None,\n",
        "                    use_cache=None,\n",
        "                    output_attentions=output_attentions,\n",
        "                )\n",
        "            \n",
        "            hidden_states = outputs[0]\n",
        "\n",
        "        hidden_states = self.gpt2.ln_f(hidden_states) #[2, 59, 768]\n",
        "        x = torch.flatten(hidden_states, 1) \n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "num_classes = 18\n",
        "\n",
        "url = 'https://www.frontiersin.org/files/MyHome%20Article%20Library/446547/446547_Thumb_400.jpg'\n",
        "img = Image.open(requests.get(url, stream=True).raw)\n",
        "preprocess = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )])\n",
        "\n",
        "\n",
        "#visual feature\n",
        "img_preprocessed = preprocess(img)\n",
        "batch_img_tensor = torch.stack([img_preprocessed, img_preprocessed])\n",
        "model_visual_feat = models.resnet50(pretrained=True)\n",
        "model_visual_feat.avgpool = nn.Identity()\n",
        "model_visual_feat.fc = nn.Identity()\n",
        "model_visual_feat.eval()\n",
        "visual_embeds = model_visual_feat(batch_img_tensor).view(-1, 49, 2048)\n",
        "visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long)\n",
        "visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float)\n",
        "\n",
        "#text feature\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "question = \"What is the state of bipolar_forceps?\"\n",
        "questions = list([question, question])\n",
        "inputs = tokenizer(questions, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "# inputs = tokenizer(questions, return_tensors=\"pt\", padding=\"max_length\",max_length=20,)#[2 20]\n",
        "token_type_ids = torch.zeros(inputs['input_ids'].shape, dtype=torch.long) # zeros because text id types is ones\n",
        "\n",
        "inputs.update(\n",
        "    {\n",
        "        \"token_type_ids\": token_type_ids,\n",
        "        \"visual_embeds\": visual_embeds, #[2, 49, 2048]\n",
        "        \"visual_token_type_ids\": visual_token_type_ids,\n",
        "        \"visual_attention_mask\": visual_attention_mask,\n",
        "    }\n",
        ")\n",
        "\n",
        "print(inputs['input_ids'].shape, inputs['token_type_ids'].shape, inputs['attention_mask'].shape, \n",
        "      inputs['visual_embeds'].shape, inputs['visual_token_type_ids'].shape, inputs['visual_attention_mask'].shape)\n",
        "\n",
        "model = GPT2_VQA(num_class=18)\n",
        "model.eval()\n",
        "logits = model(inputs)\n",
        "answer = logits.argmax(dim=1)\n",
        "print(logits.shape, answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yiP8okl0jTIc",
        "outputId": "70072718-4a71-4232-c8fd-d53a5db51733"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 10]) torch.Size([2, 10]) torch.Size([2, 10]) torch.Size([2, 49, 2048]) torch.Size([2, 49]) torch.Size([2, 49])\n",
            "torch.Size([2, 18]) tensor([1, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#VQA: ChatGPT + ResNet (Late Fusion)"
      ],
      "metadata": {
        "id": "2bNrxVprohLu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install transformers"
      ],
      "metadata": {
        "id": "996Wfz5RBgEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "from torchvision import transforms\n",
        "from transformers import BertTokenizer, GPT2Model, GPT2Tokenizer\n",
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "url = 'https://www.frontiersin.org/files/MyHome%20Article%20Library/446547/446547_Thumb_400.jpg'\n",
        "img = Image.open(requests.get(url, stream=True).raw)\n",
        "preprocess = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )])\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "labels = [ 'grasping', 'retraction', 'tissue manipulation', 'tool manipulation', \n",
        "          'cutting', 'cauterization', 'suction', 'looping', 'suturing', 'clipping', 'staple', 'ultrasound sensing']\n",
        "num_classes = 12\n",
        "model = models.resnet50(pretrained=True)\n",
        "model.fc = torch.nn.Linear(model.fc.in_features, num_classes)\n",
        "model.eval();\n",
        "\n",
        "class GPT2RS18Classification(nn.Module):\n",
        "    def __init__(self, num_class = 12):\n",
        "        super(GPT2RS18Classification, self).__init__()\n",
        "\n",
        "        # text processing\n",
        "        self.text_feature_extractor = GPT2Model.from_pretrained('gpt2')\n",
        " \n",
        "        # image processing\n",
        "        self.img_feature_extractor = models.resnet18(pretrained=True)\n",
        "        new_fc = nn.Sequential(*list(self.img_feature_extractor.fc.children())[:-1])\n",
        "        self.img_feature_extractor.fc = new_fc\n",
        "\n",
        "        #intermediate_layers\n",
        "        self.intermediate_layer = nn.Linear(1280, 512)  #(512+768)\n",
        "        self.LayerNorm = nn.BatchNorm1d(512)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "        # classifier\n",
        "        self.classifier = nn.Linear(512, num_class)\n",
        "\n",
        "    def forward(self, input, img):\n",
        "        \n",
        "        # image encoder features\n",
        "        img_feature = self.img_feature_extractor(img)\n",
        "        \n",
        "        # question tokenizer features\n",
        "        input['input_ids'] = input['input_ids'].to(device)\n",
        "        input['attention_mask'] = input['attention_mask'].to(device)\n",
        "\n",
        "        # GPT text encoder\n",
        "        text_feature = self.text_feature_extractor(**input) # [2, 10, 768]\n",
        "        print(text_feature.last_hidden_state.shape)\n",
        "        text_feature = text_feature.last_hidden_state.swapaxes(1,2)\n",
        "        text_feature = F.adaptive_avg_pool1d(text_feature,1)\n",
        "        text_feature = text_feature.swapaxes(1,2).squeeze(1)        \n",
        "        \n",
        "        # late visual-text fusion\n",
        "        img_text_features = torch.cat((img_feature, text_feature), dim=1)\n",
        "\n",
        "        # intermediate layers\n",
        "        out =self.intermediate_layer(img_text_features)\n",
        "        out = self.LayerNorm(out)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        # classifier\n",
        "        out = self.classifier(out)\n",
        "        print(out.size())\n",
        "        return out\n",
        "\n",
        "\n",
        "# questions = \"What is the state of bipolar_forceps?\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "question = \"What is the state of bipolar_forceps?\"\n",
        "questions = list([question, question])\n",
        "inputs = tokenizer(questions, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "img_preprocessed = preprocess(img)\n",
        "batch_img_tensor = torch.stack([img_preprocessed, img_preprocessed])\n",
        "new_fc = torch.nn.Sequential(*list(model.fc.children())[:-1])\n",
        "model.fc = new_fc\n",
        "img_features = model(batch_img_tensor)\n",
        "\n",
        "SVQA = GPT2RS18Classification(num_class=12).cuda()\n",
        "output = SVQA(inputs, batch_img_tensor.cuda())\n",
        "answer = output.argmax(dim=1)\n",
        "print('Question: {} \\nAnswer: {}'.format(questions[0], labels[answer[0].item()]))"
      ],
      "metadata": {
        "id": "fgB69AIFXhuX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260,
          "referenced_widgets": [
            "ad1d7a6e32ef42ffa4a4885045530460",
            "cfd09395071c4615bca50a7945ca2909",
            "24950234f99342f5935dedfd981f200f",
            "dca45872852e49838a52cdafd796cbe5",
            "7690f8e096c34e0bbd5af5b971b4d74a",
            "f63ec2e1292342d09c47dc64be251c7e",
            "a13f9f781fb944d8a304518d7668bba9",
            "1cc55d1888064f399e9ea765f2e5ae13",
            "3498f3d96e7e480495115b9bc27c5045",
            "ea12db800c664382a8e4ea35d530efb6",
            "c69b111dbb584e68baf41bfb7111ddcc"
          ]
        },
        "outputId": "6e5c08ee-a8a4-42ee-9363-82bd3d48eeb8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/44.7M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ad1d7a6e32ef42ffa4a4885045530460"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 10, 768])\n",
            "torch.Size([2, 12])\n",
            "Question: What is the state of bipolar_forceps? \n",
            "Answer: tool manipulation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#VQA: BioGPT + ResNet(Late Fusion)\n",
        "src: https://huggingface.co/microsoft/biogpt<br>\n",
        "git: https://github.com/microsoft/BioGPT<br>\n",
        "paper: https://arxiv.org/abs/2210.10341<br>\n"
      ],
      "metadata": {
        "id": "bTsb1uZJ0xO4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install transformers sacremoses"
      ],
      "metadata": {
        "id": "3iYUCf1JDNRN",
        "outputId": "bac96429-eddc-4767-d4c0-9b9a80629d14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/880.6 KB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/880.6 KB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m870.4/880.6 KB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 KB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "from torchvision import transforms\n",
        "from transformers import BioGptTokenizer, BioGptForCausalLM\n",
        "from transformers import BertTokenizer, GPT2Tokenizer\n",
        "from PIL import Image\n",
        "import requests\n",
        "url = 'https://www.frontiersin.org/files/MyHome%20Article%20Library/446547/446547_Thumb_400.jpg'\n",
        "img = Image.open(requests.get(url, stream=True).raw)\n",
        "preprocess = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )])\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "labels = [ 'grasping', 'retraction', 'tissue manipulation', 'tool manipulation', \n",
        "          'cutting', 'cauterization', 'suction', 'looping', 'suturing', 'clipping', 'staple', 'ultrasound sensing']\n",
        "num_classes = 12\n",
        "model = models.resnet50(pretrained=True)\n",
        "model.fc = torch.nn.Linear(model.fc.in_features, num_classes)\n",
        "model.eval();\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class GPT2RS18Classification(nn.Module):\n",
        "    def __init__(self, num_class = 12):\n",
        "        super(GPT2RS18Classification, self).__init__()\n",
        "\n",
        "        # text processing\n",
        "        self.text_feature_extractor = BioGptForCausalLM.from_pretrained(\"microsoft/biogpt\")\n",
        " \n",
        "        # image processing\n",
        "        self.img_feature_extractor = models.resnet18(pretrained=True)\n",
        "        new_fc = nn.Sequential(*list(self.img_feature_extractor.fc.children())[:-1])\n",
        "        self.img_feature_extractor.fc = new_fc\n",
        "\n",
        "        #intermediate_layers\n",
        "        self.intermediate_layer = nn.Linear(42896, 512)  #(512+768)\n",
        "        self.LayerNorm = nn.BatchNorm1d(512)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "        # classifier\n",
        "        self.classifier = nn.Linear(512, num_class)\n",
        "\n",
        "    def forward(self, input, img):\n",
        "        \n",
        "        # image encoder features\n",
        "        img_feature = self.img_feature_extractor(img)\n",
        "        \n",
        "        # question tokenizer features\n",
        "        input['input_ids'] = input['input_ids'].to(device)\n",
        "        input['attention_mask'] = input['attention_mask'].to(device)\n",
        "\n",
        "        # GPT text encoder\n",
        "        text_feature = self.text_feature_extractor(**input)\n",
        "        text_feature = text_feature[0].swapaxes(1,2)\n",
        "        #mobarak: [1, 12, 42384], text feature is too big compare to img. We may pool it to 512 the equal size of img\n",
        "        #F.adaptive_avg_pool2d(output[0],[1, 512])\n",
        "        text_feature = F.adaptive_avg_pool1d(text_feature,1) \n",
        "        text_feature = text_feature.swapaxes(1,2).squeeze()\n",
        "\n",
        "        # late visual-text fusion\n",
        "        #mobarak: advanced level fusion can be used instead of naive concat (e.g., multihead attention fusion)\n",
        "        img_text_features = torch.cat((img_feature, text_feature), dim=1)\n",
        "\n",
        "        # intermediate layers\n",
        "        out =self.intermediate_layer(img_text_features)\n",
        "        #mobarak: we may add one more intermidiate layer if the features size is bigger\n",
        "        out = self.LayerNorm(out)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        # classifier\n",
        "        out = self.classifier(out)\n",
        "        print(out.size())\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "tokenizer = BioGptTokenizer.from_pretrained(\"microsoft/biogpt\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "question = \"What is the state of bipolar_forceps?\"\n",
        "questions = []\n",
        "questions.append(question)\n",
        "questions.append(question)\n",
        "inputs = tokenizer(questions, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "img_preprocessed = preprocess(img)\n",
        "batch_img_tensor = torch.stack([img_preprocessed, img_preprocessed])\n",
        "new_fc = torch.nn.Sequential(*list(model.fc.children())[:-1])\n",
        "model.fc = new_fc\n",
        "img_features = model(batch_img_tensor)\n",
        "\n",
        "SVQA = GPT2RS18Classification(num_class=12).cuda()\n",
        "output = SVQA(inputs, batch_img_tensor.cuda())\n",
        "answer = output.argmax(dim=1)\n",
        "print('Question: {} \\nAnswer: {}'.format(questions[0], labels[answer[0].item()]))"
      ],
      "metadata": {
        "id": "vxFEDhkNBbc9",
        "outputId": "56e357a7-9366-4f90-f24c-5b576d87c609",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208,
          "referenced_widgets": [
            "daaddbe39a9d48b39e8bd21767c49488",
            "a5c53149f32d48b18437c59beab4bf79",
            "11e09a1170c14c31a7f9297ac4a04f9a",
            "3385d92b209e4638b1c126cf3eba5def",
            "d4409187ed76424bbe035bec7abf2402",
            "0f996f16e3f14b979f2a68335b468196",
            "45fd6be400bc4c08a8686a2f384227b9",
            "21f2cde5e5fe439189ca9d717ea9cda8",
            "d03bb1e3fda945fca91d32bb0941d4cc",
            "075e5a31742b4f3ab9fcc46a8a3ddcea",
            "9eeed93d37ec41dcb32ce18cb9c613f8",
            "ef5cf6978ffa4480804cc676f860f6a4",
            "71f8c62331824acc9845dce9906c4391",
            "30bf1ecc0ab04dc4a4f0b7dc25233faf",
            "620dfc78080c4896b5dc79ae971e9ebd",
            "d658a694f2eb457e8841f01307e4f790",
            "23c76a95e0ef48db9c124a8c53339663",
            "4d31d3897c224e95b4428f8ef2effc45",
            "320a0bb8f9c341cc922f297d3fedd5fb",
            "4946aa447f524c7abf33405471083259",
            "f4c8878d808c4a98b918f32e5124f518",
            "d712a203406f4507a7a1cc3ff6413baf"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/1.56G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "daaddbe39a9d48b39e8bd21767c49488"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/44.7M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ef5cf6978ffa4480804cc676f860f6a4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 12])\n",
            "Question: What is the state of bipolar_forceps? \n",
            "Answer: cauterization\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5Gl4-N5iKmdj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}