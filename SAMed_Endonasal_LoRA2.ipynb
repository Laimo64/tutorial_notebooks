{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/tutorial_notebooks/blob/main/SAMed_Endonasal_LoRA2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1P2M4gZbKZWt"
      },
      "source": [
        "# Customized Segment Anything Model for Medical Image Segmentation\n",
        "### [[Paper](https://arxiv.org/pdf/2304.13785.pdf)] [[Github](https://github.com/hitachinsk/SAMed)]\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Id3D1PuuLQMm"
      },
      "source": [
        "# Setup environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmmYvx7FLUif",
        "outputId": "4f7c8885-9736-46e4-e47a-4877185d0572"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.8/151.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.3/60.3 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.7/52.7 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m841.5/841.5 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m753.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for MedPy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ml-collections (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q gdown==4.6.0 einops==0.6.1 icecream==2.1.3 MedPy==0.4.0 monai==1.1.0 opencv_python==4.5.4.58 SimpleITK==2.2.1 tensorboardX==2.6 ml-collections==0.1.1 onnx==1.13.1 onnxruntime==1.14.1 tensorboardX torchmetrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-tSMFkgPhyc"
      },
      "source": [
        "# Download codes, pretrained weights and test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RyB2eYACPtEX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "446287c1-0e0e-48c5-d4da-43169c4fb0e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'samed_codes'...\n",
            "remote: Enumerating objects: 225, done.\u001b[K\n",
            "remote: Counting objects: 100% (102/102), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 225 (delta 86), reused 72 (delta 72), pack-reused 123\u001b[K\n",
            "Receiving objects: 100% (225/225), 635.01 KiB | 17.64 MiB/s, done.\n",
            "Resolving deltas: 100% (105/105), done.\n"
          ]
        }
      ],
      "source": [
        "# prepare codes\n",
        "import os\n",
        "CODE_DIR = 'samed_codes'\n",
        "os.makedirs(f'./{CODE_DIR}')\n",
        "!git clone https://github.com/hitachinsk/SAMed.git $CODE_DIR\n",
        "os.chdir(f'./{CODE_DIR}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !gdown https://drive.google.com/uc?id=1bxPjOUm6CqKsQ8wdpUgXN0djF_hoO1Km\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Function to download file into Colab\n",
        "def download_file_from_google_drive(file_id, destination):\n",
        "    downloaded = drive.CreateFile({'id': file_id})\n",
        "    downloaded.GetContentFile(destination)\n",
        "\n",
        "# Example usage\n",
        "download_file_from_google_drive('1HbERnBvsZXb3jLoTq-zh0A8pj5__L-5s', 'Endonasal_Slices_Voxel.zip')"
      ],
      "metadata": {
        "id": "5XeWl16zq0XO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1fAoOVHvAxPh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b04f0192-cd46-4ad5-f9ca-7582188f5c9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd /content/samed_codes\n",
        "#dataset\n",
        "import zipfile\n",
        "with zipfile.ZipFile('Endonasal_Slices_Voxel.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall()\n",
        "#weights\n",
        "\n",
        "# !gdown https://drive.google.com/uc?id=1P0Bm-05l-rfeghbrT1B62v5eN-3A-uOr #'epoch_159.pth'\n",
        "# !gdown https://drive.google.com/uc?id=1_oCdoEEu3mNhRfFxeWyRerOKt8OEUvcg #'sam_vit_b_01ec64.pth'\n",
        "\n",
        "download_file_from_google_drive('1_oCdoEEu3mNhRfFxeWyRerOKt8OEUvcg', 'sam_vit_b_01ec64.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnQmJASbCqUT"
      },
      "source": [
        "Dataloader:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8w27C6DKCvOK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "4ba8496e-a6ad-4dee-ea7c-f1f5b8e26872"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Sample: 798 Test Sample: 342\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "%cd /content/samed_codes\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from glob import glob\n",
        "import imageio.v2 as iio\n",
        "import matplotlib.pyplot as plt\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "from scipy.ndimage import zoom\n",
        "from einops import repeat\n",
        "from scipy import ndimage\n",
        "import random\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "def normalise_intensity(image, ROI_thres=0.1):\n",
        "    pixel_thres = np.percentile(image, ROI_thres)\n",
        "    ROI = np.where(image > pixel_thres, image, 0) # If image value is greater than pixel threshold, return image value, otherwise return 0\n",
        "    mean = np.mean(ROI)\n",
        "    std = np.std(ROI)\n",
        "    ROI_norm = (ROI - mean) / (std + 1e-8) # Normalise ROI\n",
        "    return ROI_norm\n",
        "\n",
        "def random_rot_flip(image, label):\n",
        "    k = np.random.randint(0, 4)\n",
        "    image = np.rot90(image, k)\n",
        "    label = np.rot90(label, k)\n",
        "    axis = np.random.randint(0, 2)\n",
        "    image = np.flip(image, axis=axis).copy()\n",
        "    label = np.flip(label, axis=axis).copy()\n",
        "    return image, label\n",
        "\n",
        "\n",
        "def random_rotate(image, label):\n",
        "    angle = np.random.randint(-20, 20)\n",
        "    image = ndimage.rotate(image, angle, order=0, reshape=False)\n",
        "    label = ndimage.rotate(label, angle, order=0, reshape=False)\n",
        "    return image, label\n",
        "\n",
        "def map_labels(label):\n",
        "    label_map = {0: 0, 85: 1, 128:0, 170: 2, 255: 3}\n",
        "    mapped_label = label.copy()\n",
        "    for k, v in label_map.items():\n",
        "        mapped_label[label == k] = v\n",
        "    return mapped_label\n",
        "\n",
        "class EndonasalDataset(Dataset):\n",
        "    def __init__(self, root='endonasal_train', low_res=None, isTrain=False):\n",
        "        self.img_path_all = glob(root + '/mri_t1c/*.png')  # Update the path and pattern\n",
        "        self.mask_path_all = glob(root + '/mri_masks/*.png')  # Update the path and pattern\n",
        "        self.isTrain = isTrain\n",
        "        self.isTrain = isTrain\n",
        "        self.low_res = low_res\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.img_path_all)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image = iio.imread(self.img_path_all[index])\n",
        "        image = normalise_intensity(image)\n",
        "        image = zoom(image, (512/image.shape[0], 512/image.shape[1]), order=0)\n",
        "        label = iio.imread(self.mask_path_all[index])\n",
        "        label = zoom(label, (512/label.shape[0], 512/label.shape[1]), order=0)\n",
        "        if self.isTrain:\n",
        "            if random.random() > 0.5:\n",
        "                image, label = random_rot_flip(image, label)\n",
        "            elif random.random() > 0.5:\n",
        "                image, label = random_rotate(image, label)\n",
        "\n",
        "        image = repeat(np.expand_dims(image, axis=0), 'c h w -> (repeat c) h w', repeat=3)\n",
        "        sample = {'image': image, 'label': label}\n",
        "        if self.low_res:\n",
        "            low_res_label = zoom(label, (self.low_res/label.shape[0], self.low_res/label.shape[1]), order=0)\n",
        "            sample = {'image': image, 'label': label, 'low_res_label': low_res_label}\n",
        "\n",
        "        return sample\n",
        "\n",
        "train_dataset = EndonasalDataset(root='Endonasal_Slices_Voxel/Train', low_res=128, isTrain=True)\n",
        "test_dataset = EndonasalDataset(root='Endonasal_Slices_Voxel/Test', low_res=128)\n",
        "print('Train Sample:', len(train_dataset), 'Test Sample:', len(test_dataset))\n",
        "sample = train_dataset[7]\n",
        "input, label, low_res_label = np.array(sample['image']), sample['label'], sample['low_res_label']\n",
        "plt.subplot(1,4,1), plt.axis('OFF'), plt.title('in:{}'.format(input.shape)), plt.imshow(input.transpose(1,2,0))\n",
        "plt.subplot(1,4,2), plt.axis('OFF'), plt.title('in:{}'.format(input[0].shape)), plt.imshow(input[0], cmap='gray')\n",
        "plt.subplot(1,4,3), plt.axis('OFF'), plt.title('lab:{}'.format(label.shape)), plt.imshow(label, cmap='gray');\n",
        "plt.subplot(1,4,4), plt.axis('OFF'), plt.title('low:{}'.format(low_res_label.shape)), plt.imshow(low_res_label, cmap='gray');"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from segment_anything import build_sam, SamPredictor\n",
        "from segment_anything import sam_model_registry\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from torch.nn.parameter import Parameter\n",
        "from segment_anything.modeling import Sam\n",
        "from safetensors import safe_open\n",
        "from safetensors.torch import save_file\n",
        "\n",
        "from icecream import ic\n",
        "\n",
        "class _LoRA_qkv_v0_v2(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            qkv: nn.Module,\n",
        "            linear_a_q: nn.Module,\n",
        "            linear_b_q: nn.Module,\n",
        "            linear_a_v: nn.Module,\n",
        "            linear_b_v: nn.Module,\n",
        "            conv_se_q: nn.Module,\n",
        "            conv_se_v: nn.Module,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.qkv = qkv\n",
        "        self.linear_a_q = linear_a_q\n",
        "        self.linear_b_q = linear_b_q\n",
        "        self.linear_a_v = linear_a_v\n",
        "        self.linear_b_v = linear_b_v\n",
        "        self.conv_se_q = conv_se_q\n",
        "        self.conv_se_v = conv_se_v\n",
        "\n",
        "        self.dim = qkv.in_features\n",
        "        self.w_identity = torch.eye(qkv.in_features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        qkv = self.qkv(x)\n",
        "        a_q_out = self.linear_a_q(x)\n",
        "        a_v_out = self.linear_a_v(x)\n",
        "        a_q_out_temp = self.conv_se_q(a_q_out.permute(0,3,1,2)).permute(0,2,3,1)\n",
        "        a_v_out_temp = self.conv_se_v(a_v_out.permute(0,3,1,2)).permute(0,2,3,1)\n",
        "\n",
        "        new_q = self.linear_b_q(torch.mul(a_q_out, torch.sigmoid(a_q_out_temp)))#SE = Squeeze and Excitation\n",
        "        new_v = self.linear_b_v(torch.mul(a_v_out, torch.sigmoid(a_v_out_temp)))\n",
        "\n",
        "        qkv[:, :, :, : self.dim] += new_q\n",
        "        qkv[:, :, :, -self.dim:] += new_v\n",
        "        return qkv\n",
        "\n",
        "class LoRA_Sam_v0_v2(nn.Module):\n",
        "\n",
        "    def __init__(self, sam_model: Sam, r: int, lora_layer=None):\n",
        "        super(LoRA_Sam_v0_v2, self).__init__()\n",
        "\n",
        "        assert r > 0\n",
        "        if lora_layer:\n",
        "            self.lora_layer = lora_layer\n",
        "        else:\n",
        "            self.lora_layer = list(\n",
        "                range(len(sam_model.image_encoder.blocks)))  # Only apply lora to the image encoder by default\n",
        "        # create for storage, then we can init them or load weights\n",
        "        self.w_As = []  # These are linear layers\n",
        "        self.w_Bs = []\n",
        "\n",
        "        # lets freeze first\n",
        "        for param in sam_model.image_encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Here, we do the surgery\n",
        "        for t_layer_i, blk in enumerate(sam_model.image_encoder.blocks):\n",
        "            # If we only want few lora layer instead of all\n",
        "            if t_layer_i not in self.lora_layer:\n",
        "                continue\n",
        "            w_qkv_linear = blk.attn.qkv\n",
        "            self.dim = w_qkv_linear.in_features\n",
        "            w_a_linear_q = nn.Linear(self.dim, r, bias=False)\n",
        "            w_b_linear_q = nn.Linear(r, self.dim, bias=False)\n",
        "            w_a_linear_v = nn.Linear(self.dim, r, bias=False)\n",
        "            w_b_linear_v = nn.Linear(r, self.dim, bias=False)\n",
        "\n",
        "            conv_se_q = nn.Conv2d(r, r, kernel_size=1,\n",
        "                                    stride=1, padding=0, bias=False)\n",
        "            conv_se_v = nn.Conv2d(r, r, kernel_size=1,\n",
        "                                    stride=1, padding=0, bias=False)\n",
        "            self.w_As.append(w_a_linear_q)\n",
        "            self.w_Bs.append(w_b_linear_q)\n",
        "            self.w_As.append(w_a_linear_v)\n",
        "            self.w_Bs.append(w_b_linear_v)\n",
        "            self.w_As.append(conv_se_q)\n",
        "            self.w_As.append(conv_se_v)\n",
        "            blk.attn.qkv = _LoRA_qkv_v0_v2(\n",
        "                w_qkv_linear,\n",
        "                w_a_linear_q,\n",
        "                w_b_linear_q,\n",
        "                w_a_linear_v,\n",
        "                w_b_linear_v,\n",
        "                conv_se_q,\n",
        "                conv_se_v,\n",
        "            )\n",
        "        self.reset_parameters()\n",
        "        self.sam = sam_model\n",
        "\n",
        "    def save_lora_parameters(self, filename: str) -> None:\n",
        "        r\"\"\"Only safetensors is supported now.\n",
        "\n",
        "        pip install safetensor if you do not have one installed yet.\n",
        "\n",
        "        save both lora and fc parameters.\n",
        "        \"\"\"\n",
        "\n",
        "        assert filename.endswith(\".pt\") or filename.endswith('.pth')\n",
        "\n",
        "        num_layer = len(self.w_As)  # actually, it is half\n",
        "        a_tensors = {f\"w_a_{i:03d}\": self.w_As[i].weight for i in range(num_layer)}\n",
        "        b_tensors = {f\"w_b_{i:03d}\": self.w_Bs[i].weight for i in range(num_layer)}\n",
        "        prompt_encoder_tensors = {}\n",
        "        mask_decoder_tensors = {}\n",
        "\n",
        "        # save prompt encoder, only `state_dict`, the `named_parameter` is not permitted\n",
        "        if isinstance(self.sam, torch.nn.DataParallel) or isinstance(self.sam, torch.nn.parallel.DistributedDataParallel):\n",
        "            state_dict = self.sam.module.state_dict()\n",
        "        else:\n",
        "            state_dict = self.sam.state_dict()\n",
        "        for key, value in state_dict.items():\n",
        "            if 'prompt_encoder' in key:\n",
        "                prompt_encoder_tensors[key] = value\n",
        "            if 'mask_decoder' in key:\n",
        "                mask_decoder_tensors[key] = value\n",
        "\n",
        "        merged_dict = {**a_tensors, **b_tensors, **prompt_encoder_tensors, **mask_decoder_tensors}\n",
        "        torch.save(merged_dict, filename)\n",
        "\n",
        "    def load_lora_parameters(self, filename: str) -> None:\n",
        "        r\"\"\"Only safetensors is supported now.\n",
        "\n",
        "        pip install safetensor if you do not have one installed yet.\\\n",
        "\n",
        "        load both lora and fc parameters.\n",
        "        \"\"\"\n",
        "\n",
        "        assert filename.endswith(\".pt\") or filename.endswith('.pth')\n",
        "\n",
        "        state_dict = torch.load(filename)\n",
        "\n",
        "        for i, w_A_linear in enumerate(self.w_As):\n",
        "            saved_key = f\"w_a_{i:03d}\"\n",
        "            # print('mobarak:', saved_key)\n",
        "            saved_tensor = state_dict[saved_key]\n",
        "            w_A_linear.weight = Parameter(saved_tensor)\n",
        "\n",
        "        for i, w_B_linear in enumerate(self.w_Bs):\n",
        "            saved_key = f\"w_b_{i:03d}\"\n",
        "            saved_tensor = state_dict[saved_key]\n",
        "            w_B_linear.weight = Parameter(saved_tensor)\n",
        "\n",
        "        sam_dict = self.sam.state_dict()\n",
        "        sam_keys = sam_dict.keys()\n",
        "\n",
        "        # load prompt encoder\n",
        "        prompt_encoder_keys = [k for k in sam_keys if 'prompt_encoder' in k]\n",
        "        prompt_encoder_values = [state_dict[k] for k in prompt_encoder_keys]\n",
        "        prompt_encoder_new_state_dict = {k: v for k, v in zip(prompt_encoder_keys, prompt_encoder_values)}\n",
        "        sam_dict.update(prompt_encoder_new_state_dict)\n",
        "\n",
        "        # load mask decoder\n",
        "        mask_decoder_keys = [k for k in sam_keys if 'mask_decoder' in k]\n",
        "        mask_decoder_values = [state_dict[k] for k in mask_decoder_keys]\n",
        "        mask_decoder_new_state_dict = {k: v for k, v in zip(mask_decoder_keys, mask_decoder_values)}\n",
        "        sam_dict.update(mask_decoder_new_state_dict)\n",
        "        self.sam.load_state_dict(sam_dict)\n",
        "\n",
        "    def reset_parameters(self) -> None:\n",
        "        for w_A in self.w_As:\n",
        "            nn.init.kaiming_uniform_(w_A.weight, a=math.sqrt(5))\n",
        "        for w_B in self.w_Bs:\n",
        "            nn.init.zeros_(w_B.weight)\n",
        "\n",
        "    def forward(self, batched_input, multimask_output, image_size):\n",
        "        return self.sam(batched_input, multimask_output, image_size)"
      ],
      "metadata": {
        "id": "EgNWBG2vjr9i"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8j2gsPPfB45E"
      },
      "source": [
        "SAM Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXw9Wa1dDFET",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b780203-ab1d-4ab5-afeb-ec67ac9a49aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Training on: cuda train sample size: 798 test sample size: 342 batch: 10\n"
          ]
        }
      ],
      "source": [
        "%cd /content/samed_codes\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "import numpy as np\n",
        "import argparse\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.backends.cudnn as cudnn\n",
        "from importlib import import_module\n",
        "from segment_anything import sam_model_registry\n",
        "from datasets.dataset_synapse import Synapse_dataset\n",
        "from icecream import ic\n",
        "from medpy import metric\n",
        "from scipy.ndimage import zoom\n",
        "import torch.nn as nn\n",
        "import SimpleITK as sitk\n",
        "import torch.nn.functional as F\n",
        "import imageio\n",
        "from einops import repeat\n",
        "\n",
        "from torch.nn.modules.loss import CrossEntropyLoss\n",
        "from utils import DiceLoss\n",
        "import torch.optim as optim\n",
        "from collections import Counter\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def adjust_learning_rate(optimizer, iter_num, args):\n",
        "    if args.warmup and iter_num < args.warmup_period:\n",
        "        lr_ = args.base_lr * ((iter_num + 1) / args.warmup_period)\n",
        "    else:\n",
        "        if args.warmup:\n",
        "            shift_iter = iter_num - args.warmup_period\n",
        "            assert shift_iter >= 0, f'Shift iter is {shift_iter}, smaller than zero'\n",
        "        else:\n",
        "            shift_iter = iter_num\n",
        "        lr_ = args.base_lr * (1.0 - shift_iter / args.max_iterations) ** 0.9\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr_\n",
        "    return lr_\n",
        "\n",
        "def calculate_confusion_matrix_from_arrays(prediction, ground_truth, nr_labels):\n",
        "    replace_indices = np.vstack((\n",
        "        ground_truth.flatten(),\n",
        "        prediction.flatten())\n",
        "    ).T\n",
        "    confusion_matrix, _ = np.histogramdd(\n",
        "        replace_indices,\n",
        "        bins=(nr_labels, nr_labels),\n",
        "        range=[(0, nr_labels), (0, nr_labels)]\n",
        "    )\n",
        "    confusion_matrix = confusion_matrix.astype(np.uint32)\n",
        "    return confusion_matrix\n",
        "\n",
        "def calculate_dice(confusion_matrix):\n",
        "    dices = []\n",
        "    for index in range(confusion_matrix.shape[0]):\n",
        "        true_positives = confusion_matrix[index, index]\n",
        "        false_positives = confusion_matrix[:, index].sum() - true_positives\n",
        "        false_negatives = confusion_matrix[index, :].sum() - true_positives\n",
        "        denom = 2 * true_positives + false_positives + false_negatives\n",
        "        if denom == 0:\n",
        "            dice = 0\n",
        "        else:\n",
        "            dice = 2 * float(true_positives) / denom\n",
        "        dices.append(dice)\n",
        "    return dices\n",
        "\n",
        "def inference_per_epoch(model, testloader, ce_loss, dice_loss, multimask_output=True, args=None):\n",
        "    model.eval()\n",
        "    # fig, axs = plt.subplots(len(testloader), 3, figsize=(1*3, len(testloader)*1), subplot_kw=dict(xticks=[],yticks=[]))\n",
        "    loss_per_epoch, dice_per_epoch = [], []\n",
        "    num_classes = args.num_classes + 1\n",
        "    confusion_matrix = np.zeros((num_classes, num_classes), dtype=np.uint32)\n",
        "    class_wise_dice = []\n",
        "    with torch.no_grad():\n",
        "        for i_batch, sampled_batch in enumerate(testloader):\n",
        "            image_batch, label_batch, low_res_label_batch = sampled_batch['image'],sampled_batch['label'], sampled_batch['low_res_label']\n",
        "            image_batch, label_batch, low_res_label_batch = image_batch.to(device, dtype=torch.float32), label_batch.to(device, dtype=torch.long), low_res_label_batch.to(device, dtype=torch.long)\n",
        "            outputs = model(image_batch, multimask_output, args.img_size)\n",
        "            logits = outputs['masks']\n",
        "            prob = F.softmax(logits, dim=1)\n",
        "            pred_seg = torch.argmax(prob, dim=1)\n",
        "            confusion_matrix += calculate_confusion_matrix_from_arrays(pred_seg.cpu(), label_batch.cpu(), num_classes)\n",
        "            loss, loss_ce, loss_dice = calc_loss(logits, label_batch, ce_loss, dice_loss, args)\n",
        "            loss_per_epoch.append(loss.item())\n",
        "            dice_per_epoch.append(1-loss_dice.item())\n",
        "            low_res_logits = outputs['low_res_logits']\n",
        "            loss_dice = dice_loss(low_res_logits, low_res_label_batch, softmax=True)\n",
        "            img_num = 0\n",
        "            metric_list = []\n",
        "            pred_seg, label_batch = pred_seg.cpu().detach().numpy(), label_batch.cpu().detach().numpy()\n",
        "\n",
        "        confusion_matrix = confusion_matrix[1:, 1:]  # exclude background\n",
        "        dices_per_class = {'dice_cls:{}'.format(cls + 1): dice\n",
        "                    for cls, dice in enumerate(calculate_dice(confusion_matrix))}\n",
        "\n",
        "    return np.mean(loss_per_epoch), np.mean(dice_per_epoch), dices_per_class\n",
        "def seed_everything(seed=42):\n",
        "    cudnn.benchmark = False\n",
        "    cudnn.deterministic = True\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "\n",
        "def calc_loss(output, label_batch, ce_loss, dice_loss, args):\n",
        "    loss_ce = ce_loss(output, label_batch[:].long())\n",
        "    loss_dice = dice_loss(output, label_batch, softmax=True)\n",
        "    loss = (1 - args.dice_weight) * loss_ce + args.dice_weight * loss_dice\n",
        "    return loss, loss_ce, loss_dice\n",
        "\n",
        "\n",
        "def training_per_epoch(model, trainloader, optimizer, iter_num, ce_loss, dice_loss, multimask_output=True, args=None):\n",
        "    model.train()\n",
        "    loss_all = []\n",
        "\n",
        "    for i_batch, sampled_batch in enumerate(trainloader):\n",
        "        image_batch, label_batch, low_res_label_batch = sampled_batch['image'],sampled_batch['label'], sampled_batch['low_res_label']\n",
        "        image_batch, label_batch, low_res_label_batch = image_batch.to(device, dtype=torch.float32), label_batch.to(device, dtype=torch.long), low_res_label_batch.to(device, dtype=torch.long)\n",
        "        batch_dict = {'image_batch':label_batch, 'label_batch':label_batch, 'low_res_label_batch':low_res_label_batch}\n",
        "        outputs = model(image_batch, multimask_output, args.img_size)\n",
        "        output = outputs[args.output_key]\n",
        "        loss_label_batch = batch_dict[args.batch_key]\n",
        "        loss, loss_ce, loss_dice = calc_loss(output, loss_label_batch, ce_loss, dice_loss, args)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        # Update learning rate and increment iteration count\n",
        "        lr_current = adjust_learning_rate(optimizer, iter_num, args)\n",
        "        iter_num += 1\n",
        "\n",
        "        loss_all.append(loss.item())\n",
        "\n",
        "\n",
        "    return np.mean(loss_all), iter_num, lr_current\n",
        "\n",
        "\n",
        "def test_per_epoch(model, testloader, ce_loss, dice_loss, multimask_output=True, args=None):\n",
        "    model.eval()\n",
        "    loss_per_epoch, dice_per_epoch = [], []\n",
        "    with torch.no_grad():\n",
        "        for i_batch, sampled_batch in enumerate(testloader):\n",
        "            image_batch, label_batch, low_res_label_batch = sampled_batch['image'],sampled_batch['label'], sampled_batch['low_res_label']\n",
        "            image_batch, label_batch, low_res_label_batch = image_batch.to(device, dtype=torch.float32), label_batch.to(device, dtype=torch.long), low_res_label_batch.to(device, dtype=torch.long)\n",
        "            batch_dict = {'image_batch':label_batch, 'label_batch':label_batch, 'low_res_label_batch':low_res_label_batch}\n",
        "            outputs = model(image_batch, multimask_output, args.img_size)\n",
        "            output = outputs[args.output_key]\n",
        "            loss_label_batch = batch_dict[args.batch_key]\n",
        "            loss, loss_ce, loss_dice = calc_loss(output, loss_label_batch, ce_loss, dice_loss, args)\n",
        "            loss_per_epoch.append(loss.item())\n",
        "            dice_per_epoch.append(1-loss_dice.item())\n",
        "    return np.mean(loss_per_epoch), np.mean(dice_per_epoch)\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    # Add new arguments\n",
        "    parser.add_argument('--batch_key', type=str, default='low_res_label_batch', help='Key for accessing label batch')\n",
        "    parser.add_argument('--output_key', type=str, default='low_res_logits', help='Key for accessing model outputs')\n",
        "\n",
        "    parser.add_argument('--dice_weight', type=float, default=0.8, help='Weight for dice loss in the loss calculation')\n",
        "    parser.add_argument('--weights', type=int, nargs='+', default=None,\n",
        "                    help='List of weights for each class. Provide space-separated values.')\n",
        "\n",
        "    parser.add_argument('--config', type=str, default=None, help='The config file provided by the trained model')\n",
        "    parser.add_argument('--volume_path', type=str, default='testset/test_vol_h5/')\n",
        "    parser.add_argument('--data_path', type=str, default='Endonasal_Slices_Voxel')\n",
        "    parser.add_argument('--dataset', type=str, default='Synapse', help='Experiment name')\n",
        "    parser.add_argument('--num_classes', type=int, default=2)\n",
        "    parser.add_argument('--list_dir', type=str, default='./lists/lists_Synapse/', help='list_dir')\n",
        "    parser.add_argument('--output_dir', type=str, default='results')\n",
        "    parser.add_argument('--output_file', type=str, default='Endo_best.pt')\n",
        "    parser.add_argument('--img_size', type=int, default=512, help='Input image size of the network')\n",
        "    parser.add_argument('--input_size', type=int, default=224, help='The input size for training SAM model')\n",
        "    parser.add_argument('--seed', type=int,\n",
        "                        default=1234, help='random seed')\n",
        "    parser.add_argument('--is_savenii', action='store_true', help='Whether to save results during inference')\n",
        "    parser.add_argument('--deterministic', type=int, default=1, help='whether use deterministic training')\n",
        "    parser.add_argument('--ckpt', type=str, default='checkpoints/sam_vit_b_01ec64.pth',\n",
        "                        help='Pretrained checkpoint')\n",
        "    parser.add_argument('--lora_ckpt', type=str, default='checkpoints/epoch_159.pth', help='The checkpoint from LoRA')\n",
        "    parser.add_argument('--vit_name', type=str, default='vit_b', help='Select one vit model')\n",
        "    parser.add_argument('--rank', type=int, default=6, help='Rank for LoRA adaptation')\n",
        "    parser.add_argument('--module', type=str, default='sam_lora_image_encoder')\n",
        "\n",
        "    parser.add_argument('--base_lr', type=float, default=0.0005, help='segmentation network learning rate')\n",
        "    parser.add_argument('--batch_size', type=int, default=10, help='batch_size per gpu')\n",
        "    parser.add_argument('--warmup', type=bool, default=True, help='If activated, warp up the learning from a lower lr to the base_lr')\n",
        "    parser.add_argument('--warmup_period', type=int, default=250, help='Warp up iterations, only valid whrn warmup is activated')\n",
        "    parser.add_argument('--AdamW', type=bool, default=True, help='If activated, use AdamW to finetune SAM model')\n",
        "    parser.add_argument('--max_epochs', type=int, default=1, help='maximum epoch number to train')\n",
        "    parser.add_argument('--max_iterations', type=int, default=30000, help='maximum epoch number to train')\n",
        "\n",
        "    if 'ipykernel' in sys.modules:\n",
        "        args = parser.parse_args([])\n",
        "    else:\n",
        "        args = parser.parse_args()\n",
        "\n",
        "    args.output_dir = 'results'\n",
        "    args.ckpt = 'sam_vit_b_01ec64.pth'\n",
        "    args.lora_ckpt = 'results/' + args.output_file\n",
        "    os.makedirs(args.output_dir, exist_ok = True)\n",
        "\n",
        "    sam, img_embedding_size = sam_model_registry[args.vit_name](image_size=args.img_size,\n",
        "                                                                    num_classes=args.num_classes,\n",
        "                                                                    checkpoint=args.ckpt, pixel_mean=[0, 0, 0],\n",
        "                                                                    pixel_std=[1, 1, 1])\n",
        "\n",
        "    # pkg = import_module(args.module)\n",
        "    net = LoRA_Sam_v0_v2(sam, args.rank).cuda()\n",
        "    # net.load_lora_parameters(args.lora_ckpt)\n",
        "    multimask_output = True if args.num_classes > 1 else False\n",
        "    train_dataset = EndonasalDataset(root=(args.data_path+'/Train'), low_res=128, isTrain=True)\n",
        "    test_dataset = EndonasalDataset(root=(args.data_path+'/Test'), low_res=128)\n",
        "    trainloader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=2)\n",
        "    testloader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=2)\n",
        "    print('Training on:', device, 'train sample size:', len(train_dataset), 'test sample size:', len(test_dataset), 'batch:', args.batch_size)\n",
        "\n",
        "    ce_loss = CrossEntropyLoss()\n",
        "    dice_loss = DiceLoss(args.num_classes + 1)\n",
        "    b_lr = args.base_lr / args.warmup_period\n",
        "    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, net.parameters()), lr=b_lr, betas=(0.9, 0.999), weight_decay=0.1)\n",
        "    iter_num = 0\n",
        "\n",
        "    best_epoch, best_loss = 0.0, np.inf\n",
        "    for epoch in range(args.max_epochs):\n",
        "        loss_training, iter_num, lr_current = training_per_epoch(net, trainloader, optimizer, iter_num, ce_loss, dice_loss, multimask_output=multimask_output, args=args)\n",
        "        loss_testing, dice = test_per_epoch(net, testloader, ce_loss, dice_loss,multimask_output=True, args=args)\n",
        "\n",
        "        if loss_testing < best_loss:\n",
        "            best_loss = loss_testing\n",
        "            best_epoch = epoch\n",
        "            net.save_lora_parameters(os.path.join(args.output_dir, args.output_file))\n",
        "\n",
        "        print('--- Epoch {}/{}: Training loss = {:.4f}, Testing: [loss = {:.4f}, dice = {:.4f}], Best loss = {:.4f}, Best epoch = {}, lr = {:.6f}'.\\\n",
        "    format(epoch, args.max_epochs, loss_training, loss_testing, dice, best_loss, best_epoch, lr_current))\n",
        "\n",
        "    assert args.lora_ckpt is not None\n",
        "    net.load_lora_parameters(args.lora_ckpt)\n",
        "    testloader = DataLoader(test_dataset, batch_size=20, shuffle=False, num_workers=2)\n",
        "    test_loss, overall_dice, dices_per_class = inference_per_epoch(net, testloader, ce_loss, dice_loss, multimask_output=True, args=args)\n",
        "    dices_per_class_list = np.array(list(dices_per_class.values()))\n",
        "    print('Class Wise Dice:', dices_per_class)\n",
        "    print('Overall Dice:', np.mean(dices_per_class_list))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    seed_everything()\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    main()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dupTnYetNuNe"
      },
      "source": [
        "Inference: My Dice Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJEAXtUcLSVV"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def calculate_confusion_matrix_from_arrays(prediction, ground_truth, nr_labels):\n",
        "    replace_indices = np.vstack((\n",
        "        ground_truth.flatten(),\n",
        "        prediction.flatten())\n",
        "    ).T\n",
        "    confusion_matrix, _ = np.histogramdd(\n",
        "        replace_indices,\n",
        "        bins=(nr_labels, nr_labels),\n",
        "        range=[(0, nr_labels), (0, nr_labels)]\n",
        "    )\n",
        "    confusion_matrix = confusion_matrix.astype(np.uint32)\n",
        "    return confusion_matrix\n",
        "\n",
        "def calculate_dice(confusion_matrix):\n",
        "    dices = []\n",
        "    for index in range(confusion_matrix.shape[0]):\n",
        "        true_positives = confusion_matrix[index, index]\n",
        "        false_positives = confusion_matrix[:, index].sum() - true_positives\n",
        "        false_negatives = confusion_matrix[index, :].sum() - true_positives\n",
        "        denom = 2 * true_positives + false_positives + false_negatives\n",
        "        if denom == 0:\n",
        "            dice = 0\n",
        "        else:\n",
        "            dice = 2 * float(true_positives) / denom\n",
        "        dices.append(dice)\n",
        "    return dices\n",
        "\n",
        "def test_per_epoch(model, testloader, ce_loss, dice_loss, multimask_output=True, args=None):\n",
        "    model.eval()\n",
        "    fig, axs = plt.subplots(len(testloader), 3, figsize=(1*3, len(testloader)*1), subplot_kw=dict(xticks=[],yticks=[]))\n",
        "    loss_per_epoch, dice_per_epoch = [], []\n",
        "    num_classes = args.num_classes + 1\n",
        "    confusion_matrix = np.zeros((num_classes, num_classes), dtype=np.uint32)\n",
        "    class_wise_dice = []\n",
        "    with torch.no_grad():\n",
        "        for i_batch, sampled_batch in enumerate(testloader):\n",
        "            image_batch, label_batch, low_res_label_batch = sampled_batch['image'],sampled_batch['label'], sampled_batch['low_res_label']\n",
        "            image_batch, label_batch, low_res_label_batch = image_batch.to(device, dtype=torch.float32), label_batch.to(device, dtype=torch.long), low_res_label_batch.to(device, dtype=torch.long)\n",
        "            outputs = model(image_batch, multimask_output, args.img_size)\n",
        "            logits = outputs['masks']\n",
        "            prob = F.softmax(logits, dim=1)\n",
        "            pred_seg = torch.argmax(prob, dim=1)\n",
        "            confusion_matrix += calculate_confusion_matrix_from_arrays(pred_seg.cpu(), label_batch.cpu(), num_classes)\n",
        "            loss, loss_ce, loss_dice = calc_loss(outputs, low_res_label_batch, ce_loss, dice_loss)\n",
        "            loss_per_epoch.append(loss.item())\n",
        "            dice_per_epoch.append(1-loss_dice.item())\n",
        "            low_res_logits = outputs['low_res_logits']\n",
        "            loss_dice = dice_loss(low_res_logits, low_res_label_batch, softmax=True)\n",
        "            img_num = 0\n",
        "            axs[i_batch, 0].imshow(image_batch[img_num, 0].cpu().numpy(), cmap='gray')\n",
        "            axs[i_batch, 1].imshow(label_batch[img_num].cpu().numpy(), cmap='gray')\n",
        "            axs[i_batch, 2].imshow(pred_seg[img_num].cpu().numpy(), cmap='gray')\n",
        "            metric_list = []\n",
        "            pred_seg, label_batch = pred_seg.cpu().detach().numpy(), label_batch.cpu().detach().numpy()\n",
        "\n",
        "        confusion_matrix = confusion_matrix[1:, 1:]  # exclude background\n",
        "        dices_per_class = {'dice_cls:{}'.format(cls + 1): dice\n",
        "                    for cls, dice in enumerate(calculate_dice(confusion_matrix))}\n",
        "\n",
        "    return np.mean(loss_per_epoch), np.mean(dice_per_epoch), dices_per_class\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--config', type=str, default=None, help='The config file provided by the trained model')\n",
        "    parser.add_argument('--volume_path', type=str, default='testset/test_vol_h5/')\n",
        "    parser.add_argument('--dataset', type=str, default='Synapse', help='Experiment name')\n",
        "    parser.add_argument('--num_classes', type=int, default=2)\n",
        "    parser.add_argument('--list_dir', type=str, default='./lists/lists_Synapse/', help='list_dir')\n",
        "    parser.add_argument('--output_dir', type=str, default='results')\n",
        "    parser.add_argument('--img_size', type=int, default=512, help='Input image size of the network')\n",
        "    parser.add_argument('--input_size', type=int, default=224, help='The input size for training SAM model')\n",
        "    parser.add_argument('--seed', type=int,\n",
        "                        default=1234, help='random seed')\n",
        "    parser.add_argument('--is_savenii', action='store_true', help='Whether to save results during inference')\n",
        "    parser.add_argument('--deterministic', type=int, default=1, help='whether use deterministic training')\n",
        "    parser.add_argument('--ckpt', type=str, default='checkpoints/sam_vit_b_01ec64.pth',\n",
        "                        help='Pretrained checkpoint')\n",
        "    parser.add_argument('--lora_ckpt', type=str, default='checkpoints/epoch_159.pth', help='The checkpoint from LoRA')\n",
        "    parser.add_argument('--vit_name', type=str, default='vit_b', help='Select one vit model')\n",
        "    parser.add_argument('--rank', type=int, default=4, help='Rank for LoRA adaptation')\n",
        "    parser.add_argument('--module', type=str, default='sam_lora_image_encoder')\n",
        "\n",
        "    parser.add_argument('--base_lr', type=float, default=0.005, help='segmentation network learning rate')\n",
        "    parser.add_argument('--batch_size', type=int, default=12, help='batch_size per gpu')\n",
        "    parser.add_argument('--warmup', type=bool, default=True, help='If activated, warp up the learning from a lower lr to the base_lr')\n",
        "    parser.add_argument('--warmup_period', type=int, default=250, help='Warp up iterations, only valid whrn warmup is activated')\n",
        "    parser.add_argument('--AdamW', type=bool, default=True, help='If activated, use AdamW to finetune SAM model')\n",
        "    parser.add_argument('--max_epochs', type=int, default=10, help='maximum epoch number to train')\n",
        "    parser.add_argument('--max_iterations', type=int, default=30000, help='maximum epoch number to train')\n",
        "\n",
        "\n",
        "    if 'ipykernel' in sys.modules:\n",
        "        args = parser.parse_args([])\n",
        "    else:\n",
        "        args = parser.parse_args()\n",
        "\n",
        "    args.ckpt = 'sam_vit_b_01ec64.pth'\n",
        "    args.lora_ckpt = 'results/model_best.pt'\n",
        "    sam, img_embedding_size = sam_model_registry[args.vit_name](image_size=args.img_size,\n",
        "                                                                    num_classes=args.num_classes,\n",
        "                                                                    checkpoint=args.ckpt, pixel_mean=[0, 0, 0],\n",
        "                                                                    pixel_std=[1, 1, 1])\n",
        "\n",
        "    net = LoRA_Sam_v0_v2(sam, args.rank).cuda()\n",
        "    ce_loss = CrossEntropyLoss()\n",
        "    dice_loss = DiceLoss(args.num_classes + 1)\n",
        "\n",
        "    assert args.lora_ckpt is not None\n",
        "    net.load_lora_parameters(args.lora_ckpt)\n",
        "    testloader = DataLoader(test_dataset, batch_size=20, shuffle=False, num_workers=2)\n",
        "    test_loss, overall_dice, dices_per_class = test_per_epoch(net, testloader, ce_loss, dice_loss, multimask_output=True, args=args)\n",
        "    dices_per_class_list = np.array(list(dices_per_class.values()))\n",
        "    print('Class Wise Dice:', dices_per_class)\n",
        "    print('Overall Dice:', np.mean(dices_per_class_list))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference MRI to MRI<br>\n",
        "download test mri"
      ],
      "metadata": {
        "id": "OeaaFJN0iUvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "url = 'https://drive.google.com/uc?id=1zcvnBscFVI2v5ieAlGGnp0zpX8WmIrD4'\n",
        "gdown.download(url,'endonasal_mri_patients.zip',quiet=True)\n",
        "!unzip -q endonasal_mri_patients\n",
        "!rm -rf /content/endonasal_mri_patients/.DS_Store\n",
        "!rm -rf /content/endonasal_mri_patients/**/.DS_Store"
      ],
      "metadata": {
        "id": "u55aRX3KjbPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MRI to MRI"
      ],
      "metadata": {
        "id": "3mLQJNoU0rId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nibabel as nib\n",
        "import cv2\n",
        "import numpy as np\n",
        "from scipy.ndimage import zoom\n",
        "from einops import repeat\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def read_mri(mri_path):\n",
        "    img_meta = nib.load(mri_path)\n",
        "    array = img_meta.get_fdata()\n",
        "    return np.rot90(array)\n",
        "\n",
        "def normalise_intensity(image, ROI_thres=0.1):\n",
        "    pixel_thres = np.percentile(image, ROI_thres)\n",
        "    ROI = np.where(image > pixel_thres, image, 0) # If image value is greater than pixel threshold, return image value, otherwise return 0\n",
        "    mean = np.mean(ROI)\n",
        "    std = np.std(ROI)\n",
        "    ROI_norm = (ROI - mean) / (std + 1e-8) # Normalise ROI\n",
        "    return ROI_norm\n",
        "\n",
        "class EndonasalDataset_MRI(Dataset):\n",
        "    def __init__(self, root='endonasal_mri_patients', patient=None, low_res=None, isTrain=False):\n",
        "\n",
        "        mri_path = 'endonasal_mri_patients/mri0{}/mri0{}_t1c.nii.gz'.format(patient, patient)\n",
        "        self.mask_path = 'endonasal_mri_patients/mri0{}/mri0{}_mask.nii.gz'.format(patient, patient)\n",
        "        mri_array = read_mri(mri_path)\n",
        "        self.image_all = []\n",
        "        for z in range(mri_array.shape[2]):\n",
        "            normalized_slice = cv2.normalize(mri_array[:, :, z], None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
        "            self.image_all.append(normalise_intensity(normalized_slice))\n",
        "\n",
        "        self.mask_all = read_mri(self.mask_path)\n",
        "        self.isTrain = isTrain\n",
        "        self.low_res = low_res\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.image_all)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image = self.image_all[index]\n",
        "        image = zoom(image, (512/image.shape[0], 512/image.shape[1]), order=0)\n",
        "        label = self.mask_all[:,:,index]\n",
        "        label = zoom(label, (512/label.shape[0], 512/label.shape[1]), order=0)\n",
        "        if self.isTrain:\n",
        "            if random.random() > 0.5:\n",
        "                image, label = random_rot_flip(image, label)\n",
        "            elif random.random() > 0.5:\n",
        "                image, label = random_rotate(image, label)\n",
        "\n",
        "        image = repeat(np.expand_dims(image, axis=0), 'c h w -> (repeat c) h w', repeat=3)\n",
        "        sample = {'image': image, 'label': label}\n",
        "        if self.low_res:\n",
        "            low_res_label = zoom(label, (self.low_res/label.shape[0], self.low_res/label.shape[1]), order=0)\n",
        "            sample = {'image': image, 'label': label, 'low_res_label': low_res_label, 'maskpath': self.mask_path}\n",
        "\n",
        "        return sample\n",
        "\n",
        "\n",
        "def calculate_confusion_matrix_from_arrays(prediction, ground_truth, nr_labels):\n",
        "    replace_indices = np.vstack((\n",
        "        ground_truth.flatten(),\n",
        "        prediction.flatten())\n",
        "    ).T\n",
        "    confusion_matrix, _ = np.histogramdd(\n",
        "        replace_indices,\n",
        "        bins=(nr_labels, nr_labels),\n",
        "        range=[(0, nr_labels), (0, nr_labels)]\n",
        "    )\n",
        "    confusion_matrix = confusion_matrix.astype(np.uint32)\n",
        "    return confusion_matrix\n",
        "\n",
        "def calculate_dice(confusion_matrix):\n",
        "    dices = []\n",
        "    for index in range(confusion_matrix.shape[0]):\n",
        "        true_positives = confusion_matrix[index, index]\n",
        "        false_positives = confusion_matrix[:, index].sum() - true_positives\n",
        "        false_negatives = confusion_matrix[index, :].sum() - true_positives\n",
        "        denom = 2 * true_positives + false_positives + false_negatives\n",
        "        if denom == 0:\n",
        "            dice = 0\n",
        "        else:\n",
        "            dice = 2 * float(true_positives) / denom\n",
        "        dices.append(dice)\n",
        "    return dices\n",
        "\n",
        "def pred_to_mri(pred_seg_all, mask_path):\n",
        "    os.makedirs('predicted_mri', mode = 0o777, exist_ok = True)\n",
        "    img_meta = nib.load(mask_path)\n",
        "    pred_seg_all = np.rot90(np.array(pred_seg_all).transpose(1,2,0), k=-1)\n",
        "    img_nifti = nib.Nifti1Image(pred_seg_all, img_meta.affine, header=img_meta.header)\n",
        "    nib.save(img_nifti,'predicted_mri/'+os.path.basename(mask_path))\n",
        "\n",
        "def test_per_epoch(model, testloader, ce_loss, dice_loss, multimask_output=True, args=None):\n",
        "    model.eval()\n",
        "    loss_per_epoch, dice_per_epoch = [], []\n",
        "    num_classes = args.num_classes + 1\n",
        "    confusion_matrix = np.zeros((num_classes, num_classes), dtype=np.uint32)\n",
        "    class_wise_dice = []\n",
        "    pred_seg_all = []\n",
        "    with torch.no_grad():\n",
        "        for i_batch, sampled_batch in enumerate(testloader):\n",
        "            image_batch, label_batch, low_res_label_batch = sampled_batch['image'],sampled_batch['label'], sampled_batch['low_res_label']\n",
        "            image_batch, label_batch, low_res_label_batch = image_batch.to(device, dtype=torch.float32), label_batch.to(device, dtype=torch.long), low_res_label_batch.to(device, dtype=torch.long)\n",
        "            outputs = model(image_batch, multimask_output, args.img_size)\n",
        "            logits = outputs['masks']\n",
        "            prob = F.softmax(logits, dim=1)\n",
        "            pred_seg = torch.argmax(prob, dim=1)\n",
        "            pred_seg_all.extend(pred_seg.detach().cpu().numpy())\n",
        "            confusion_matrix += calculate_confusion_matrix_from_arrays(pred_seg.cpu(), label_batch.cpu(), num_classes)\n",
        "            loss, loss_ce, loss_dice = calc_loss(outputs, low_res_label_batch, ce_loss, dice_loss)\n",
        "            loss_per_epoch.append(loss.item())\n",
        "            dice_per_epoch.append(1-loss_dice.item())\n",
        "            low_res_logits = outputs['low_res_logits']\n",
        "            loss_dice = dice_loss(low_res_logits, low_res_label_batch, softmax=True)\n",
        "            metric_list = []\n",
        "            pred_seg, label_batch = pred_seg.cpu().detach().numpy(), label_batch.cpu().detach().numpy()\n",
        "\n",
        "        pred_to_mri(np.array(pred_seg_all), sampled_batch['maskpath'][0])\n",
        "        confusion_matrix = confusion_matrix[1:, 1:]  # exclude background\n",
        "        dices_per_class = {'dice_cls:{}'.format(cls + 1): round(dice, 4)\n",
        "                    for cls, dice in enumerate(calculate_dice(confusion_matrix))}\n",
        "\n",
        "    return np.mean(loss_per_epoch), np.mean(dice_per_epoch), dices_per_class\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--config', type=str, default=None, help='The config file provided by the trained model')\n",
        "    parser.add_argument('--volume_path', type=str, default='testset/test_vol_h5/')\n",
        "    parser.add_argument('--dataset', type=str, default='Synapse', help='Experiment name')\n",
        "    parser.add_argument('--num_classes', type=int, default=2)\n",
        "    parser.add_argument('--list_dir', type=str, default='./lists/lists_Synapse/', help='list_dir')\n",
        "    parser.add_argument('--output_dir', type=str, default='results')\n",
        "    parser.add_argument('--img_size', type=int, default=512, help='Input image size of the network')\n",
        "    parser.add_argument('--input_size', type=int, default=224, help='The input size for training SAM model')\n",
        "    parser.add_argument('--seed', type=int,\n",
        "                        default=1234, help='random seed')\n",
        "    parser.add_argument('--is_savenii', action='store_true', help='Whether to save results during inference')\n",
        "    parser.add_argument('--deterministic', type=int, default=1, help='whether use deterministic training')\n",
        "    parser.add_argument('--ckpt', type=str, default='checkpoints/sam_vit_b_01ec64.pth',\n",
        "                        help='Pretrained checkpoint')\n",
        "    parser.add_argument('--lora_ckpt', type=str, default='checkpoints/epoch_159.pth', help='The checkpoint from LoRA')\n",
        "    parser.add_argument('--vit_name', type=str, default='vit_b', help='Select one vit model')\n",
        "    parser.add_argument('--rank', type=int, default=4, help='Rank for LoRA adaptation')\n",
        "    parser.add_argument('--module', type=str, default='sam_lora_image_encoder')\n",
        "\n",
        "    parser.add_argument('--base_lr', type=float, default=0.005, help='segmentation network learning rate')\n",
        "    parser.add_argument('--batch_size', type=int, default=12, help='batch_size per gpu')\n",
        "    parser.add_argument('--warmup', type=bool, default=True, help='If activated, warp up the learning from a lower lr to the base_lr')\n",
        "    parser.add_argument('--warmup_period', type=int, default=250, help='Warp up iterations, only valid whrn warmup is activated')\n",
        "    parser.add_argument('--AdamW', type=bool, default=True, help='If activated, use AdamW to finetune SAM model')\n",
        "    parser.add_argument('--max_epochs', type=int, default=10, help='maximum epoch number to train')\n",
        "    parser.add_argument('--max_iterations', type=int, default=30000, help='maximum epoch number to train')\n",
        "\n",
        "\n",
        "    if 'ipykernel' in sys.modules:\n",
        "        args = parser.parse_args([])\n",
        "    else:\n",
        "        args = parser.parse_args()\n",
        "\n",
        "    args.ckpt = 'sam_vit_b_01ec64.pth'\n",
        "    args.lora_ckpt = 'results/model_best.pt'\n",
        "    sam, img_embedding_size = sam_model_registry[args.vit_name](image_size=args.img_size,\n",
        "                                                                    num_classes=args.num_classes,\n",
        "                                                                    checkpoint=args.ckpt, pixel_mean=[0, 0, 0],\n",
        "                                                                    pixel_std=[1, 1, 1])\n",
        "\n",
        "    net = LoRA_Sam_v0_v2(sam, args.rank).cuda()\n",
        "    ce_loss = CrossEntropyLoss()\n",
        "    dice_loss = DiceLoss(args.num_classes + 1)\n",
        "\n",
        "    assert args.lora_ckpt is not None\n",
        "    net.load_lora_parameters(args.lora_ckpt)\n",
        "\n",
        "    patients = ['154', '169', '170']\n",
        "    mean_overall = []\n",
        "    tumor_overall = []\n",
        "    carotid_overall = []\n",
        "    for patient in patients:\n",
        "        test_dataset = EndonasalDataset_MRI(root='endonasal_mri_patients', patient=patient, low_res=128)\n",
        "        testloader = DataLoader(test_dataset, batch_size=20, shuffle=False, num_workers=2)\n",
        "        test_loss, overall_dice, dices_per_class = test_per_epoch(net, testloader, ce_loss, dice_loss, multimask_output=True, args=args)\n",
        "        dices_per_class_list = np.array(list(dices_per_class.values()))\n",
        "        overall = round(np.mean(dices_per_class_list),4)\n",
        "        mean_overall.append(overall)\n",
        "        tumor_overall.append(dices_per_class['dice_cls:1'])\n",
        "        carotid_overall.append(dices_per_class['dice_cls:2'])\n",
        "        print('Patient:', patient, ',Class Wise:', dices_per_class, ',Overall :', overall)\n",
        "\n",
        "    print('Overall Model Performance [Mean Overall]:', round(np.mean(mean_overall),4), '[cls-1:{}]'.format(round(np.mean(tumor_overall),4)),\\\n",
        "            '[cls-2:{}]'.format(round(np.mean(carotid_overall),4)))\n"
      ],
      "metadata": {
        "id": "IJKKAr8UiTry",
        "outputId": "1bbecab0-dff9-4478-d4f4-6c5916cb1ca9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Patient: 154 ,Class Wise: {'dice_cls:1': 0.9737, 'dice_cls:2': 0.7039} ,Overall : 0.8388\n",
            "Patient: 169 ,Class Wise: {'dice_cls:1': 0.991, 'dice_cls:2': 0.9621} ,Overall : 0.9766\n",
            "Patient: 170 ,Class Wise: {'dice_cls:1': 0.9894, 'dice_cls:2': 0.8928} ,Overall : 0.9411\n",
            "Overall Model Performance [Mean Overall]: 0.9188 [cls-1:0.9847] [cls-2:0.8529]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Post Processing to remove outliers (small seg)"
      ],
      "metadata": {
        "id": "_dBB8gMP8yHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage import morphology\n",
        "import nibabel as nib\n",
        "mask_meta = nib.load('/content/samed_codes/endonasal_mri_patients/mri0169/mri0169_mask.nii.gz')\n",
        "mask = nib.load('/content/samed_codes/predicted_mri/mri0169_mask.nii.gz').get_fdata()\n",
        "\n",
        "binary_mask = morphology.remove_small_objects(mask>0, 50)\n",
        "mask[binary_mask==0] = 0\n",
        "img_nifti = nib.Nifti1Image(mask, mask_meta.affine, header=mask_meta.header)\n",
        "nib.save(img_nifti,'mri0169_mask_post_processed.nii.gz')"
      ],
      "metadata": {
        "id": "c8y3CsOW1O6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Er8Wnp5M85Zd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}