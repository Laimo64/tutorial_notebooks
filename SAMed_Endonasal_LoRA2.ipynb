{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/tutorial_notebooks/blob/main/SAMed_Endonasal_LoRA2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1P2M4gZbKZWt"
      },
      "source": [
        "# Customized Segment Anything Model for Medical Image Segmentation\n",
        "### [[Paper](https://arxiv.org/pdf/2304.13785.pdf)] [[Github](https://github.com/hitachinsk/SAMed)]\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Id3D1PuuLQMm"
      },
      "source": [
        "# Setup environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmmYvx7FLUif",
        "outputId": "98f27639-4886-43bf-f6c3-aa7897572fc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.8/151.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.3/60.3 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.7/52.7 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m841.5/841.5 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for MedPy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ml-collections (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q gdown==4.6.0 einops==0.6.1 icecream==2.1.3 MedPy==0.4.0 monai==1.1.0 opencv_python==4.5.4.58 SimpleITK==2.2.1 tensorboardX==2.6 ml-collections==0.1.1 onnx==1.13.1 onnxruntime==1.14.1 tensorboardX torchmetrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-tSMFkgPhyc"
      },
      "source": [
        "# Download codes, pretrained weights and test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RyB2eYACPtEX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc6d4fbc-aae2-4851-eedd-d725e89a6b55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'samed_codes'...\n",
            "remote: Enumerating objects: 225, done.\u001b[K\n",
            "remote: Counting objects: 100% (102/102), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 225 (delta 86), reused 72 (delta 72), pack-reused 123\u001b[K\n",
            "Receiving objects: 100% (225/225), 635.01 KiB | 24.42 MiB/s, done.\n",
            "Resolving deltas: 100% (105/105), done.\n"
          ]
        }
      ],
      "source": [
        "# prepare codes\n",
        "import os\n",
        "CODE_DIR = 'samed_codes'\n",
        "os.makedirs(f'./{CODE_DIR}')\n",
        "!git clone https://github.com/hitachinsk/SAMed.git $CODE_DIR\n",
        "os.chdir(f'./{CODE_DIR}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !gdown https://drive.google.com/uc?id=1bxPjOUm6CqKsQ8wdpUgXN0djF_hoO1Km\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Function to download file into Colab\n",
        "def download_file_from_google_drive(file_id, destination):\n",
        "    downloaded = drive.CreateFile({'id': file_id})\n",
        "    downloaded.GetContentFile(destination)\n",
        "\n",
        "# Example usage\n",
        "download_file_from_google_drive('1HbERnBvsZXb3jLoTq-zh0A8pj5__L-5s', 'Endonasal_Slices_Voxel.zip')"
      ],
      "metadata": {
        "id": "5XeWl16zq0XO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9fe584d-8f26-4738-d9f6-d948b929851b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:pydrive is deprecated and no longer maintained. We recommend that you migrate your projects to pydrive2, the maintained fork of pydrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1fAoOVHvAxPh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c710f352-4cff-4b1e-8299-6a63e81ee206"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd /content/samed_codes\n",
        "#dataset\n",
        "import zipfile\n",
        "with zipfile.ZipFile('Endonasal_Slices_Voxel.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall()\n",
        "#weights\n",
        "\n",
        "# !gdown https://drive.google.com/uc?id=1P0Bm-05l-rfeghbrT1B62v5eN-3A-uOr #'epoch_159.pth'\n",
        "# !gdown https://drive.google.com/uc?id=1_oCdoEEu3mNhRfFxeWyRerOKt8OEUvcg #'sam_vit_b_01ec64.pth'\n",
        "\n",
        "download_file_from_google_drive('1_oCdoEEu3mNhRfFxeWyRerOKt8OEUvcg', 'sam_vit_b_01ec64.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnQmJASbCqUT"
      },
      "source": [
        "Dataloader:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8w27C6DKCvOK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "05c3bdf7-53d8-41ba-8f49-06916be386dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Sample: 798 Test Sample: 342\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "%cd /content/samed_codes\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from glob import glob\n",
        "import imageio.v2 as iio\n",
        "import matplotlib.pyplot as plt\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "from scipy.ndimage import zoom\n",
        "from einops import repeat\n",
        "from scipy import ndimage\n",
        "import random\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "def normalise_intensity(image, ROI_thres=0.1):\n",
        "    pixel_thres = np.percentile(image, ROI_thres)\n",
        "    ROI = np.where(image > pixel_thres, image, 0) # If image value is greater than pixel threshold, return image value, otherwise return 0\n",
        "    mean = np.mean(ROI)\n",
        "    std = np.std(ROI)\n",
        "    ROI_norm = (ROI - mean) / (std + 1e-8) # Normalise ROI\n",
        "    return ROI_norm\n",
        "\n",
        "def random_rot_flip(image, label):\n",
        "    k = np.random.randint(0, 4)\n",
        "    image = np.rot90(image, k)\n",
        "    label = np.rot90(label, k)\n",
        "    axis = np.random.randint(0, 2)\n",
        "    image = np.flip(image, axis=axis).copy()\n",
        "    label = np.flip(label, axis=axis).copy()\n",
        "    return image, label\n",
        "\n",
        "\n",
        "def random_rotate(image, label):\n",
        "    angle = np.random.randint(-20, 20)\n",
        "    image = ndimage.rotate(image, angle, order=0, reshape=False)\n",
        "    label = ndimage.rotate(label, angle, order=0, reshape=False)\n",
        "    return image, label\n",
        "\n",
        "def map_labels(label):\n",
        "    label_map = {0: 0, 85: 1, 128:0, 170: 2, 255: 3}\n",
        "    mapped_label = label.copy()\n",
        "    for k, v in label_map.items():\n",
        "        mapped_label[label == k] = v\n",
        "    return mapped_label\n",
        "\n",
        "class EndonasalDataset(Dataset):\n",
        "    def __init__(self, root='endonasal_train', low_res=None, isTrain=False):\n",
        "        self.img_path_all = glob(root + '/mri_t1c/*.png')  # Update the path and pattern\n",
        "        self.mask_path_all = glob(root + '/mri_masks/*.png')  # Update the path and pattern\n",
        "        self.isTrain = isTrain\n",
        "        self.isTrain = isTrain\n",
        "        self.low_res = low_res\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.img_path_all)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image = iio.imread(self.img_path_all[index])\n",
        "        image = normalise_intensity(image)\n",
        "        image = zoom(image, (512/image.shape[0], 512/image.shape[1]), order=0)\n",
        "        label = iio.imread(self.mask_path_all[index])\n",
        "        label = zoom(label, (512/label.shape[0], 512/label.shape[1]), order=0)\n",
        "        if self.isTrain:\n",
        "            if random.random() > 0.5:\n",
        "                image, label = random_rot_flip(image, label)\n",
        "            elif random.random() > 0.5:\n",
        "                image, label = random_rotate(image, label)\n",
        "\n",
        "        image = repeat(np.expand_dims(image, axis=0), 'c h w -> (repeat c) h w', repeat=3)\n",
        "        sample = {'image': image, 'label': label}\n",
        "        if self.low_res:\n",
        "            low_res_label = zoom(label, (self.low_res/label.shape[0], self.low_res/label.shape[1]), order=0)\n",
        "            sample = {'image': image, 'label': label, 'low_res_label': low_res_label}\n",
        "\n",
        "        return sample\n",
        "\n",
        "train_dataset = EndonasalDataset(root='Endonasal_Slices_Voxel/Train', low_res=128, isTrain=True)\n",
        "test_dataset = EndonasalDataset(root='Endonasal_Slices_Voxel/Test', low_res=128)\n",
        "print('Train Sample:', len(train_dataset), 'Test Sample:', len(test_dataset))\n",
        "sample = train_dataset[7]\n",
        "input, label, low_res_label = np.array(sample['image']), sample['label'], sample['low_res_label']\n",
        "plt.subplot(1,4,1), plt.axis('OFF'), plt.title('in:{}'.format(input.shape)), plt.imshow(input.transpose(1,2,0))\n",
        "plt.subplot(1,4,2), plt.axis('OFF'), plt.title('in:{}'.format(input[0].shape)), plt.imshow(input[0], cmap='gray')\n",
        "plt.subplot(1,4,3), plt.axis('OFF'), plt.title('lab:{}'.format(label.shape)), plt.imshow(label, cmap='gray');\n",
        "plt.subplot(1,4,4), plt.axis('OFF'), plt.title('low:{}'.format(low_res_label.shape)), plt.imshow(low_res_label, cmap='gray');"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from segment_anything import build_sam, SamPredictor\n",
        "from segment_anything import sam_model_registry\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from torch.nn.parameter import Parameter\n",
        "from segment_anything.modeling import Sam\n",
        "from safetensors import safe_open\n",
        "from safetensors.torch import save_file\n",
        "\n",
        "from icecream import ic\n",
        "\n",
        "class _LoRA_qkv_v0_v2(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            qkv: nn.Module,\n",
        "            linear_a_q: nn.Module,\n",
        "            linear_b_q: nn.Module,\n",
        "            linear_a_v: nn.Module,\n",
        "            linear_b_v: nn.Module,\n",
        "            conv_se_q: nn.Module,\n",
        "            conv_se_v: nn.Module,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.qkv = qkv\n",
        "        self.linear_a_q = linear_a_q\n",
        "        self.linear_b_q = linear_b_q\n",
        "        self.linear_a_v = linear_a_v\n",
        "        self.linear_b_v = linear_b_v\n",
        "        self.conv_se_q = conv_se_q\n",
        "        self.conv_se_v = conv_se_v\n",
        "\n",
        "        self.dim = qkv.in_features\n",
        "        self.w_identity = torch.eye(qkv.in_features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        qkv = self.qkv(x)\n",
        "        a_q_out = self.linear_a_q(x)\n",
        "        a_v_out = self.linear_a_v(x)\n",
        "        a_q_out_temp = self.conv_se_q(a_q_out.permute(0,3,1,2)).permute(0,2,3,1)\n",
        "        a_v_out_temp = self.conv_se_v(a_v_out.permute(0,3,1,2)).permute(0,2,3,1)\n",
        "\n",
        "        new_q = self.linear_b_q(torch.mul(a_q_out, torch.sigmoid(a_q_out_temp)))#SE = Squeeze and Excitation\n",
        "        new_v = self.linear_b_v(torch.mul(a_v_out, torch.sigmoid(a_v_out_temp)))\n",
        "\n",
        "        qkv[:, :, :, : self.dim] += new_q\n",
        "        qkv[:, :, :, -self.dim:] += new_v\n",
        "        return qkv\n",
        "\n",
        "class LoRA_Sam_v0_v2(nn.Module):\n",
        "\n",
        "    def __init__(self, sam_model: Sam, r: int, lora_layer=None):\n",
        "        super(LoRA_Sam_v0_v2, self).__init__()\n",
        "\n",
        "        assert r > 0\n",
        "        if lora_layer:\n",
        "            self.lora_layer = lora_layer\n",
        "        else:\n",
        "            self.lora_layer = list(\n",
        "                range(len(sam_model.image_encoder.blocks)))  # Only apply lora to the image encoder by default\n",
        "        # create for storage, then we can init them or load weights\n",
        "        self.w_As = []  # These are linear layers\n",
        "        self.w_Bs = []\n",
        "\n",
        "        # lets freeze first\n",
        "        for param in sam_model.image_encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Here, we do the surgery\n",
        "        for t_layer_i, blk in enumerate(sam_model.image_encoder.blocks):\n",
        "            # If we only want few lora layer instead of all\n",
        "            if t_layer_i not in self.lora_layer:\n",
        "                continue\n",
        "            w_qkv_linear = blk.attn.qkv\n",
        "            self.dim = w_qkv_linear.in_features\n",
        "            w_a_linear_q = nn.Linear(self.dim, r, bias=False)\n",
        "            w_b_linear_q = nn.Linear(r, self.dim, bias=False)\n",
        "            w_a_linear_v = nn.Linear(self.dim, r, bias=False)\n",
        "            w_b_linear_v = nn.Linear(r, self.dim, bias=False)\n",
        "\n",
        "            conv_se_q = nn.Conv2d(r, r, kernel_size=1,\n",
        "                                    stride=1, padding=0, bias=False)\n",
        "            conv_se_v = nn.Conv2d(r, r, kernel_size=1,\n",
        "                                    stride=1, padding=0, bias=False)\n",
        "            self.w_As.append(w_a_linear_q)\n",
        "            self.w_Bs.append(w_b_linear_q)\n",
        "            self.w_As.append(w_a_linear_v)\n",
        "            self.w_Bs.append(w_b_linear_v)\n",
        "            self.w_As.append(conv_se_q)\n",
        "            self.w_As.append(conv_se_v)\n",
        "            blk.attn.qkv = _LoRA_qkv_v0_v2(\n",
        "                w_qkv_linear,\n",
        "                w_a_linear_q,\n",
        "                w_b_linear_q,\n",
        "                w_a_linear_v,\n",
        "                w_b_linear_v,\n",
        "                conv_se_q,\n",
        "                conv_se_v,\n",
        "            )\n",
        "        self.reset_parameters()\n",
        "        self.sam = sam_model\n",
        "\n",
        "    def save_lora_parameters(self, filename: str) -> None:\n",
        "        r\"\"\"Only safetensors is supported now.\n",
        "\n",
        "        pip install safetensor if you do not have one installed yet.\n",
        "\n",
        "        save both lora and fc parameters.\n",
        "        \"\"\"\n",
        "\n",
        "        assert filename.endswith(\".pt\") or filename.endswith('.pth')\n",
        "\n",
        "        num_layer = len(self.w_As)  # actually, it is half\n",
        "        a_tensors = {f\"w_a_{i:03d}\": self.w_As[i].weight for i in range(num_layer)}\n",
        "        b_tensors = {f\"w_b_{i:03d}\": self.w_Bs[i].weight for i in range(num_layer)}\n",
        "        prompt_encoder_tensors = {}\n",
        "        mask_decoder_tensors = {}\n",
        "\n",
        "        # save prompt encoder, only `state_dict`, the `named_parameter` is not permitted\n",
        "        if isinstance(self.sam, torch.nn.DataParallel) or isinstance(self.sam, torch.nn.parallel.DistributedDataParallel):\n",
        "            state_dict = self.sam.module.state_dict()\n",
        "        else:\n",
        "            state_dict = self.sam.state_dict()\n",
        "        for key, value in state_dict.items():\n",
        "            if 'prompt_encoder' in key:\n",
        "                prompt_encoder_tensors[key] = value\n",
        "            if 'mask_decoder' in key:\n",
        "                mask_decoder_tensors[key] = value\n",
        "\n",
        "        merged_dict = {**a_tensors, **b_tensors, **prompt_encoder_tensors, **mask_decoder_tensors}\n",
        "        torch.save(merged_dict, filename)\n",
        "\n",
        "    def load_lora_parameters(self, filename: str) -> None:\n",
        "        r\"\"\"Only safetensors is supported now.\n",
        "\n",
        "        pip install safetensor if you do not have one installed yet.\\\n",
        "\n",
        "        load both lora and fc parameters.\n",
        "        \"\"\"\n",
        "\n",
        "        assert filename.endswith(\".pt\") or filename.endswith('.pth')\n",
        "\n",
        "        state_dict = torch.load(filename)\n",
        "\n",
        "        for i, w_A_linear in enumerate(self.w_As):\n",
        "            saved_key = f\"w_a_{i:03d}\"\n",
        "            # print('mobarak:', saved_key)\n",
        "            saved_tensor = state_dict[saved_key]\n",
        "            w_A_linear.weight = Parameter(saved_tensor)\n",
        "\n",
        "        for i, w_B_linear in enumerate(self.w_Bs):\n",
        "            saved_key = f\"w_b_{i:03d}\"\n",
        "            saved_tensor = state_dict[saved_key]\n",
        "            w_B_linear.weight = Parameter(saved_tensor)\n",
        "\n",
        "        sam_dict = self.sam.state_dict()\n",
        "        sam_keys = sam_dict.keys()\n",
        "\n",
        "        # load prompt encoder\n",
        "        prompt_encoder_keys = [k for k in sam_keys if 'prompt_encoder' in k]\n",
        "        prompt_encoder_values = [state_dict[k] for k in prompt_encoder_keys]\n",
        "        prompt_encoder_new_state_dict = {k: v for k, v in zip(prompt_encoder_keys, prompt_encoder_values)}\n",
        "        sam_dict.update(prompt_encoder_new_state_dict)\n",
        "\n",
        "        # load mask decoder\n",
        "        mask_decoder_keys = [k for k in sam_keys if 'mask_decoder' in k]\n",
        "        mask_decoder_values = [state_dict[k] for k in mask_decoder_keys]\n",
        "        mask_decoder_new_state_dict = {k: v for k, v in zip(mask_decoder_keys, mask_decoder_values)}\n",
        "        sam_dict.update(mask_decoder_new_state_dict)\n",
        "        self.sam.load_state_dict(sam_dict)\n",
        "\n",
        "    def reset_parameters(self) -> None:\n",
        "        for w_A in self.w_As:\n",
        "            nn.init.kaiming_uniform_(w_A.weight, a=math.sqrt(5))\n",
        "        for w_B in self.w_Bs:\n",
        "            nn.init.zeros_(w_B.weight)\n",
        "\n",
        "    def forward(self, batched_input, multimask_output, image_size):\n",
        "        return self.sam(batched_input, multimask_output, image_size)"
      ],
      "metadata": {
        "id": "EgNWBG2vjr9i"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Network Training Parameter:"
      ],
      "metadata": {
        "id": "bQLGnu6awRcz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import sys\n",
        "from segment_anything import sam_model_registry\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "# Add new arguments\n",
        "parser.add_argument('--batch_key', type=str, default='low_res_label_batch', help='Key for accessing label batch')\n",
        "parser.add_argument('--output_key', type=str, default='low_res_logits', help='Key for accessing model outputs')\n",
        "\n",
        "parser.add_argument('--dice_weight', type=float, default=0.8, help='Weight for dice loss in the loss calculation')\n",
        "parser.add_argument('--weights', type=int, nargs='+', default=None,\n",
        "                help='List of weights for each class. Provide space-separated values.')\n",
        "\n",
        "parser.add_argument('--config', type=str, default=None, help='The config file provided by the trained model')\n",
        "parser.add_argument('--volume_path', type=str, default='testset/test_vol_h5/')\n",
        "parser.add_argument('--data_path', type=str, default='Endonasal_Slices_Voxel')\n",
        "parser.add_argument('--dataset', type=str, default='Synapse', help='Experiment name')\n",
        "parser.add_argument('--num_classes', type=int, default=2)\n",
        "parser.add_argument('--list_dir', type=str, default='./lists/lists_Synapse/', help='list_dir')\n",
        "parser.add_argument('--output_dir', type=str, default='results')\n",
        "parser.add_argument('--output_file', type=str, default='Endo_best.pt')\n",
        "parser.add_argument('--img_size', type=int, default=512, help='Input image size of the network')\n",
        "parser.add_argument('--input_size', type=int, default=224, help='The input size for training SAM model')\n",
        "parser.add_argument('--seed', type=int,\n",
        "                    default=1234, help='random seed')\n",
        "parser.add_argument('--is_savenii', action='store_true', help='Whether to save results during inference')\n",
        "parser.add_argument('--deterministic', type=int, default=1, help='whether use deterministic training')\n",
        "parser.add_argument('--ckpt', type=str, default='checkpoints/sam_vit_b_01ec64.pth',\n",
        "                    help='Pretrained checkpoint')\n",
        "parser.add_argument('--lora_ckpt', type=str, default='checkpoints/epoch_159.pth', help='The checkpoint from LoRA')\n",
        "parser.add_argument('--vit_name', type=str, default='vit_b', help='Select one vit model')\n",
        "parser.add_argument('--rank', type=int, default=6, help='Rank for LoRA adaptation')\n",
        "parser.add_argument('--module', type=str, default='sam_lora_image_encoder')\n",
        "\n",
        "parser.add_argument('--base_lr', type=float, default=0.0005, help='segmentation network learning rate')\n",
        "parser.add_argument('--batch_size', type=int, default=10, help='batch_size per gpu')\n",
        "parser.add_argument('--warmup', type=bool, default=True, help='If activated, warp up the learning from a lower lr to the base_lr')\n",
        "parser.add_argument('--warmup_period', type=int, default=250, help='Warp up iterations, only valid whrn warmup is activated')\n",
        "parser.add_argument('--AdamW', type=bool, default=True, help='If activated, use AdamW to finetune SAM model')\n",
        "parser.add_argument('--max_epochs', type=int, default=1, help='maximum epoch number to train')\n",
        "parser.add_argument('--max_iterations', type=int, default=30000, help='maximum epoch number to train')\n",
        "\n",
        "if 'ipykernel' in sys.modules:\n",
        "    args = parser.parse_args([])\n",
        "else:\n",
        "    args = parser.parse_args()\n",
        "\n",
        "args.output_dir = 'results'\n",
        "args.ckpt = 'sam_vit_b_01ec64.pth'\n",
        "args.lora_ckpt = 'results/' + args.output_file\n",
        "os.makedirs(args.output_dir, exist_ok = True)\n",
        "\n",
        "sam, img_embedding_size = sam_model_registry[args.vit_name](image_size=args.img_size,\n",
        "                                                                num_classes=args.num_classes,\n",
        "                                                                checkpoint=args.ckpt, pixel_mean=[0, 0, 0],\n",
        "                                                                pixel_std=[1, 1, 1])\n",
        "\n",
        "net = LoRA_Sam_v0_v2(sam, args.rank).cuda()\n",
        "\n",
        "total_params = sum(p.numel() for p in net.parameters())\n",
        "print(f\"Number of total parameters PitSAM: {total_params}\")\n",
        "\n",
        "total_params_trainable = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
        "print(f\"Number of trainable (Gated Attention LoRA) parameters PitSAM: {total_params_trainable}\")\n",
        "print(f\"Percentage of parameters to train PitSAM: {round((total_params_trainable / total_params)*100, 2) } %\")\n",
        "\n",
        "\n",
        "########## vanilla SAM ###########\n",
        "from segment_anything import sam_model_registry\n",
        "from importlib import import_module\n",
        "sam, img_embedding_size = sam_model_registry[args.vit_name](image_size=args.img_size,\n",
        "                                                                num_classes=args.num_classes,\n",
        "                                                                checkpoint=args.ckpt, pixel_mean=[0, 0, 0],\n",
        "                                                                pixel_std=[1, 1, 1])\n",
        "pkg = import_module(args.module)\n",
        "net_samed = pkg.LoRA_Sam(sam, args.rank).cuda()\n",
        "\n",
        "total_params = sum(p.numel() for p in net_samed.parameters())\n",
        "print(f\"Number of total parameters SAMed: {total_params}\")\n",
        "\n",
        "total_params_trainable = sum(p.numel() for p in net_samed.parameters() if p.requires_grad)\n",
        "print(f\"Number of trainable (LoRA) parameters SAMed: {total_params_trainable}\")\n",
        "print(f\"Percentage of parameters to train SAMed: {round((total_params_trainable / total_params)*100, 2) } %\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9Qw0SMHwWp_",
        "outputId": "0e8926e4-7f4e-4002-f235-9d2dd0283d48"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of total parameters PitSAM: 91425135\n",
            "Number of trainable (Gated Attention LoRA) parameters PitSAM: 4146287\n",
            "Percentage of parameters to train PitSAM: 4.54 %\n",
            "Number of total parameters SAMed: 91424271\n",
            "Number of trainable (LoRA) parameters SAMed: 4145423\n",
            "Percentage of parameters to train SAMed: 4.53 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8j2gsPPfB45E"
      },
      "source": [
        "SAM Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXw9Wa1dDFET",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b780203-ab1d-4ab5-afeb-ec67ac9a49aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Training on: cuda train sample size: 798 test sample size: 342 batch: 10\n"
          ]
        }
      ],
      "source": [
        "%cd /content/samed_codes\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "import numpy as np\n",
        "import argparse\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.backends.cudnn as cudnn\n",
        "from importlib import import_module\n",
        "from segment_anything import sam_model_registry\n",
        "from datasets.dataset_synapse import Synapse_dataset\n",
        "from icecream import ic\n",
        "from medpy import metric\n",
        "from scipy.ndimage import zoom\n",
        "import torch.nn as nn\n",
        "import SimpleITK as sitk\n",
        "import torch.nn.functional as F\n",
        "import imageio\n",
        "from einops import repeat\n",
        "\n",
        "from torch.nn.modules.loss import CrossEntropyLoss\n",
        "from utils import DiceLoss\n",
        "import torch.optim as optim\n",
        "from collections import Counter\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def adjust_learning_rate(optimizer, iter_num, args):\n",
        "    if args.warmup and iter_num < args.warmup_period:\n",
        "        lr_ = args.base_lr * ((iter_num + 1) / args.warmup_period)\n",
        "    else:\n",
        "        if args.warmup:\n",
        "            shift_iter = iter_num - args.warmup_period\n",
        "            assert shift_iter >= 0, f'Shift iter is {shift_iter}, smaller than zero'\n",
        "        else:\n",
        "            shift_iter = iter_num\n",
        "        lr_ = args.base_lr * (1.0 - shift_iter / args.max_iterations) ** 0.9\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr_\n",
        "    return lr_\n",
        "\n",
        "def calculate_confusion_matrix_from_arrays(prediction, ground_truth, nr_labels):\n",
        "    replace_indices = np.vstack((\n",
        "        ground_truth.flatten(),\n",
        "        prediction.flatten())\n",
        "    ).T\n",
        "    confusion_matrix, _ = np.histogramdd(\n",
        "        replace_indices,\n",
        "        bins=(nr_labels, nr_labels),\n",
        "        range=[(0, nr_labels), (0, nr_labels)]\n",
        "    )\n",
        "    confusion_matrix = confusion_matrix.astype(np.uint32)\n",
        "    return confusion_matrix\n",
        "\n",
        "def calculate_dice(confusion_matrix):\n",
        "    dices = []\n",
        "    for index in range(confusion_matrix.shape[0]):\n",
        "        true_positives = confusion_matrix[index, index]\n",
        "        false_positives = confusion_matrix[:, index].sum() - true_positives\n",
        "        false_negatives = confusion_matrix[index, :].sum() - true_positives\n",
        "        denom = 2 * true_positives + false_positives + false_negatives\n",
        "        if denom == 0:\n",
        "            dice = 0\n",
        "        else:\n",
        "            dice = 2 * float(true_positives) / denom\n",
        "        dices.append(dice)\n",
        "    return dices\n",
        "\n",
        "def inference_per_epoch(model, testloader, ce_loss, dice_loss, multimask_output=True, args=None):\n",
        "    model.eval()\n",
        "    # fig, axs = plt.subplots(len(testloader), 3, figsize=(1*3, len(testloader)*1), subplot_kw=dict(xticks=[],yticks=[]))\n",
        "    loss_per_epoch, dice_per_epoch = [], []\n",
        "    num_classes = args.num_classes + 1\n",
        "    confusion_matrix = np.zeros((num_classes, num_classes), dtype=np.uint32)\n",
        "    class_wise_dice = []\n",
        "    with torch.no_grad():\n",
        "        for i_batch, sampled_batch in enumerate(testloader):\n",
        "            image_batch, label_batch, low_res_label_batch = sampled_batch['image'],sampled_batch['label'], sampled_batch['low_res_label']\n",
        "            image_batch, label_batch, low_res_label_batch = image_batch.to(device, dtype=torch.float32), label_batch.to(device, dtype=torch.long), low_res_label_batch.to(device, dtype=torch.long)\n",
        "            outputs = model(image_batch, multimask_output, args.img_size)\n",
        "            logits = outputs['masks']\n",
        "            prob = F.softmax(logits, dim=1)\n",
        "            pred_seg = torch.argmax(prob, dim=1)\n",
        "            confusion_matrix += calculate_confusion_matrix_from_arrays(pred_seg.cpu(), label_batch.cpu(), num_classes)\n",
        "            loss, loss_ce, loss_dice = calc_loss(logits, label_batch, ce_loss, dice_loss, args)\n",
        "            loss_per_epoch.append(loss.item())\n",
        "            dice_per_epoch.append(1-loss_dice.item())\n",
        "            low_res_logits = outputs['low_res_logits']\n",
        "            loss_dice = dice_loss(low_res_logits, low_res_label_batch, softmax=True)\n",
        "            img_num = 0\n",
        "            metric_list = []\n",
        "            pred_seg, label_batch = pred_seg.cpu().detach().numpy(), label_batch.cpu().detach().numpy()\n",
        "\n",
        "        confusion_matrix = confusion_matrix[1:, 1:]  # exclude background\n",
        "        dices_per_class = {'dice_cls:{}'.format(cls + 1): dice\n",
        "                    for cls, dice in enumerate(calculate_dice(confusion_matrix))}\n",
        "\n",
        "    return np.mean(loss_per_epoch), np.mean(dice_per_epoch), dices_per_class\n",
        "def seed_everything(seed=42):\n",
        "    cudnn.benchmark = False\n",
        "    cudnn.deterministic = True\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "\n",
        "def calc_loss(output, label_batch, ce_loss, dice_loss, args):\n",
        "    loss_ce = ce_loss(output, label_batch[:].long())\n",
        "    loss_dice = dice_loss(output, label_batch, softmax=True)\n",
        "    loss = (1 - args.dice_weight) * loss_ce + args.dice_weight * loss_dice\n",
        "    return loss, loss_ce, loss_dice\n",
        "\n",
        "\n",
        "def training_per_epoch(model, trainloader, optimizer, iter_num, ce_loss, dice_loss, multimask_output=True, args=None):\n",
        "    model.train()\n",
        "    loss_all = []\n",
        "\n",
        "    for i_batch, sampled_batch in enumerate(trainloader):\n",
        "        image_batch, label_batch, low_res_label_batch = sampled_batch['image'],sampled_batch['label'], sampled_batch['low_res_label']\n",
        "        image_batch, label_batch, low_res_label_batch = image_batch.to(device, dtype=torch.float32), label_batch.to(device, dtype=torch.long), low_res_label_batch.to(device, dtype=torch.long)\n",
        "        batch_dict = {'image_batch':label_batch, 'label_batch':label_batch, 'low_res_label_batch':low_res_label_batch}\n",
        "        outputs = model(image_batch, multimask_output, args.img_size)\n",
        "        output = outputs[args.output_key]\n",
        "        loss_label_batch = batch_dict[args.batch_key]\n",
        "        loss, loss_ce, loss_dice = calc_loss(output, loss_label_batch, ce_loss, dice_loss, args)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        # Update learning rate and increment iteration count\n",
        "        lr_current = adjust_learning_rate(optimizer, iter_num, args)\n",
        "        iter_num += 1\n",
        "\n",
        "        loss_all.append(loss.item())\n",
        "\n",
        "\n",
        "    return np.mean(loss_all), iter_num, lr_current\n",
        "\n",
        "\n",
        "def test_per_epoch(model, testloader, ce_loss, dice_loss, multimask_output=True, args=None):\n",
        "    model.eval()\n",
        "    loss_per_epoch, dice_per_epoch = [], []\n",
        "    with torch.no_grad():\n",
        "        for i_batch, sampled_batch in enumerate(testloader):\n",
        "            image_batch, label_batch, low_res_label_batch = sampled_batch['image'],sampled_batch['label'], sampled_batch['low_res_label']\n",
        "            image_batch, label_batch, low_res_label_batch = image_batch.to(device, dtype=torch.float32), label_batch.to(device, dtype=torch.long), low_res_label_batch.to(device, dtype=torch.long)\n",
        "            batch_dict = {'image_batch':label_batch, 'label_batch':label_batch, 'low_res_label_batch':low_res_label_batch}\n",
        "            outputs = model(image_batch, multimask_output, args.img_size)\n",
        "            output = outputs[args.output_key]\n",
        "            loss_label_batch = batch_dict[args.batch_key]\n",
        "            loss, loss_ce, loss_dice = calc_loss(output, loss_label_batch, ce_loss, dice_loss, args)\n",
        "            loss_per_epoch.append(loss.item())\n",
        "            dice_per_epoch.append(1-loss_dice.item())\n",
        "    return np.mean(loss_per_epoch), np.mean(dice_per_epoch)\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    # Add new arguments\n",
        "    parser.add_argument('--batch_key', type=str, default='low_res_label_batch', help='Key for accessing label batch')\n",
        "    parser.add_argument('--output_key', type=str, default='low_res_logits', help='Key for accessing model outputs')\n",
        "\n",
        "    parser.add_argument('--dice_weight', type=float, default=0.8, help='Weight for dice loss in the loss calculation')\n",
        "    parser.add_argument('--weights', type=int, nargs='+', default=None,\n",
        "                    help='List of weights for each class. Provide space-separated values.')\n",
        "\n",
        "    parser.add_argument('--config', type=str, default=None, help='The config file provided by the trained model')\n",
        "    parser.add_argument('--volume_path', type=str, default='testset/test_vol_h5/')\n",
        "    parser.add_argument('--data_path', type=str, default='Endonasal_Slices_Voxel')\n",
        "    parser.add_argument('--dataset', type=str, default='Synapse', help='Experiment name')\n",
        "    parser.add_argument('--num_classes', type=int, default=2)\n",
        "    parser.add_argument('--list_dir', type=str, default='./lists/lists_Synapse/', help='list_dir')\n",
        "    parser.add_argument('--output_dir', type=str, default='results')\n",
        "    parser.add_argument('--output_file', type=str, default='Endo_best.pt')\n",
        "    parser.add_argument('--img_size', type=int, default=512, help='Input image size of the network')\n",
        "    parser.add_argument('--input_size', type=int, default=224, help='The input size for training SAM model')\n",
        "    parser.add_argument('--seed', type=int,\n",
        "                        default=1234, help='random seed')\n",
        "    parser.add_argument('--is_savenii', action='store_true', help='Whether to save results during inference')\n",
        "    parser.add_argument('--deterministic', type=int, default=1, help='whether use deterministic training')\n",
        "    parser.add_argument('--ckpt', type=str, default='checkpoints/sam_vit_b_01ec64.pth',\n",
        "                        help='Pretrained checkpoint')\n",
        "    parser.add_argument('--lora_ckpt', type=str, default='checkpoints/epoch_159.pth', help='The checkpoint from LoRA')\n",
        "    parser.add_argument('--vit_name', type=str, default='vit_b', help='Select one vit model')\n",
        "    parser.add_argument('--rank', type=int, default=6, help='Rank for LoRA adaptation')\n",
        "    parser.add_argument('--module', type=str, default='sam_lora_image_encoder')\n",
        "\n",
        "    parser.add_argument('--base_lr', type=float, default=0.0005, help='segmentation network learning rate')\n",
        "    parser.add_argument('--batch_size', type=int, default=10, help='batch_size per gpu')\n",
        "    parser.add_argument('--warmup', type=bool, default=True, help='If activated, warp up the learning from a lower lr to the base_lr')\n",
        "    parser.add_argument('--warmup_period', type=int, default=250, help='Warp up iterations, only valid whrn warmup is activated')\n",
        "    parser.add_argument('--AdamW', type=bool, default=True, help='If activated, use AdamW to finetune SAM model')\n",
        "    parser.add_argument('--max_epochs', type=int, default=1, help='maximum epoch number to train')\n",
        "    parser.add_argument('--max_iterations', type=int, default=30000, help='maximum epoch number to train')\n",
        "\n",
        "    if 'ipykernel' in sys.modules:\n",
        "        args = parser.parse_args([])\n",
        "    else:\n",
        "        args = parser.parse_args()\n",
        "\n",
        "    args.output_dir = 'results'\n",
        "    args.ckpt = 'sam_vit_b_01ec64.pth'\n",
        "    args.lora_ckpt = 'results/' + args.output_file\n",
        "    os.makedirs(args.output_dir, exist_ok = True)\n",
        "\n",
        "    sam, img_embedding_size = sam_model_registry[args.vit_name](image_size=args.img_size,\n",
        "                                                                    num_classes=args.num_classes,\n",
        "                                                                    checkpoint=args.ckpt, pixel_mean=[0, 0, 0],\n",
        "                                                                    pixel_std=[1, 1, 1])\n",
        "\n",
        "    # pkg = import_module(args.module)\n",
        "    net = LoRA_Sam_v0_v2(sam, args.rank).cuda()\n",
        "    # net.load_lora_parameters(args.lora_ckpt)\n",
        "    multimask_output = True if args.num_classes > 1 else False\n",
        "    train_dataset = EndonasalDataset(root=(args.data_path+'/Train'), low_res=128, isTrain=True)\n",
        "    test_dataset = EndonasalDataset(root=(args.data_path+'/Test'), low_res=128)\n",
        "    trainloader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=2)\n",
        "    testloader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=2)\n",
        "    print('Training on:', device, 'train sample size:', len(train_dataset), 'test sample size:', len(test_dataset), 'batch:', args.batch_size)\n",
        "\n",
        "    ce_loss = CrossEntropyLoss()\n",
        "    dice_loss = DiceLoss(args.num_classes + 1)\n",
        "    b_lr = args.base_lr / args.warmup_period\n",
        "    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, net.parameters()), lr=b_lr, betas=(0.9, 0.999), weight_decay=0.1)\n",
        "    iter_num = 0\n",
        "\n",
        "    best_epoch, best_loss = 0.0, np.inf\n",
        "    for epoch in range(args.max_epochs):\n",
        "        loss_training, iter_num, lr_current = training_per_epoch(net, trainloader, optimizer, iter_num, ce_loss, dice_loss, multimask_output=multimask_output, args=args)\n",
        "        loss_testing, dice = test_per_epoch(net, testloader, ce_loss, dice_loss,multimask_output=True, args=args)\n",
        "\n",
        "        if loss_testing < best_loss:\n",
        "            best_loss = loss_testing\n",
        "            best_epoch = epoch\n",
        "            net.save_lora_parameters(os.path.join(args.output_dir, args.output_file))\n",
        "\n",
        "        print('--- Epoch {}/{}: Training loss = {:.4f}, Testing: [loss = {:.4f}, dice = {:.4f}], Best loss = {:.4f}, Best epoch = {}, lr = {:.6f}'.\\\n",
        "    format(epoch, args.max_epochs, loss_training, loss_testing, dice, best_loss, best_epoch, lr_current))\n",
        "\n",
        "    assert args.lora_ckpt is not None\n",
        "    net.load_lora_parameters(args.lora_ckpt)\n",
        "    testloader = DataLoader(test_dataset, batch_size=20, shuffle=False, num_workers=2)\n",
        "    test_loss, overall_dice, dices_per_class = inference_per_epoch(net, testloader, ce_loss, dice_loss, multimask_output=True, args=args)\n",
        "    dices_per_class_list = np.array(list(dices_per_class.values()))\n",
        "    print('Class Wise Dice:', dices_per_class)\n",
        "    print('Overall Dice:', np.mean(dices_per_class_list))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    seed_everything()\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    main()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dupTnYetNuNe"
      },
      "source": [
        "Inference: My Dice Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJEAXtUcLSVV"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def calculate_confusion_matrix_from_arrays(prediction, ground_truth, nr_labels):\n",
        "    replace_indices = np.vstack((\n",
        "        ground_truth.flatten(),\n",
        "        prediction.flatten())\n",
        "    ).T\n",
        "    confusion_matrix, _ = np.histogramdd(\n",
        "        replace_indices,\n",
        "        bins=(nr_labels, nr_labels),\n",
        "        range=[(0, nr_labels), (0, nr_labels)]\n",
        "    )\n",
        "    confusion_matrix = confusion_matrix.astype(np.uint32)\n",
        "    return confusion_matrix\n",
        "\n",
        "def calculate_dice(confusion_matrix):\n",
        "    dices = []\n",
        "    for index in range(confusion_matrix.shape[0]):\n",
        "        true_positives = confusion_matrix[index, index]\n",
        "        false_positives = confusion_matrix[:, index].sum() - true_positives\n",
        "        false_negatives = confusion_matrix[index, :].sum() - true_positives\n",
        "        denom = 2 * true_positives + false_positives + false_negatives\n",
        "        if denom == 0:\n",
        "            dice = 0\n",
        "        else:\n",
        "            dice = 2 * float(true_positives) / denom\n",
        "        dices.append(dice)\n",
        "    return dices\n",
        "\n",
        "def test_per_epoch(model, testloader, ce_loss, dice_loss, multimask_output=True, args=None):\n",
        "    model.eval()\n",
        "    fig, axs = plt.subplots(len(testloader), 3, figsize=(1*3, len(testloader)*1), subplot_kw=dict(xticks=[],yticks=[]))\n",
        "    loss_per_epoch, dice_per_epoch = [], []\n",
        "    num_classes = args.num_classes + 1\n",
        "    confusion_matrix = np.zeros((num_classes, num_classes), dtype=np.uint32)\n",
        "    class_wise_dice = []\n",
        "    with torch.no_grad():\n",
        "        for i_batch, sampled_batch in enumerate(testloader):\n",
        "            image_batch, label_batch, low_res_label_batch = sampled_batch['image'],sampled_batch['label'], sampled_batch['low_res_label']\n",
        "            image_batch, label_batch, low_res_label_batch = image_batch.to(device, dtype=torch.float32), label_batch.to(device, dtype=torch.long), low_res_label_batch.to(device, dtype=torch.long)\n",
        "            outputs = model(image_batch, multimask_output, args.img_size)\n",
        "            logits = outputs['masks']\n",
        "            prob = F.softmax(logits, dim=1)\n",
        "            pred_seg = torch.argmax(prob, dim=1)\n",
        "            confusion_matrix += calculate_confusion_matrix_from_arrays(pred_seg.cpu(), label_batch.cpu(), num_classes)\n",
        "            loss, loss_ce, loss_dice = calc_loss(outputs, low_res_label_batch, ce_loss, dice_loss)\n",
        "            loss_per_epoch.append(loss.item())\n",
        "            dice_per_epoch.append(1-loss_dice.item())\n",
        "            low_res_logits = outputs['low_res_logits']\n",
        "            loss_dice = dice_loss(low_res_logits, low_res_label_batch, softmax=True)\n",
        "            img_num = 0\n",
        "            axs[i_batch, 0].imshow(image_batch[img_num, 0].cpu().numpy(), cmap='gray')\n",
        "            axs[i_batch, 1].imshow(label_batch[img_num].cpu().numpy(), cmap='gray')\n",
        "            axs[i_batch, 2].imshow(pred_seg[img_num].cpu().numpy(), cmap='gray')\n",
        "            metric_list = []\n",
        "            pred_seg, label_batch = pred_seg.cpu().detach().numpy(), label_batch.cpu().detach().numpy()\n",
        "\n",
        "        confusion_matrix = confusion_matrix[1:, 1:]  # exclude background\n",
        "        dices_per_class = {'dice_cls:{}'.format(cls + 1): dice\n",
        "                    for cls, dice in enumerate(calculate_dice(confusion_matrix))}\n",
        "\n",
        "    return np.mean(loss_per_epoch), np.mean(dice_per_epoch), dices_per_class\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--config', type=str, default=None, help='The config file provided by the trained model')\n",
        "    parser.add_argument('--volume_path', type=str, default='testset/test_vol_h5/')\n",
        "    parser.add_argument('--dataset', type=str, default='Synapse', help='Experiment name')\n",
        "    parser.add_argument('--num_classes', type=int, default=2)\n",
        "    parser.add_argument('--list_dir', type=str, default='./lists/lists_Synapse/', help='list_dir')\n",
        "    parser.add_argument('--output_dir', type=str, default='results')\n",
        "    parser.add_argument('--img_size', type=int, default=512, help='Input image size of the network')\n",
        "    parser.add_argument('--input_size', type=int, default=224, help='The input size for training SAM model')\n",
        "    parser.add_argument('--seed', type=int,\n",
        "                        default=1234, help='random seed')\n",
        "    parser.add_argument('--is_savenii', action='store_true', help='Whether to save results during inference')\n",
        "    parser.add_argument('--deterministic', type=int, default=1, help='whether use deterministic training')\n",
        "    parser.add_argument('--ckpt', type=str, default='checkpoints/sam_vit_b_01ec64.pth',\n",
        "                        help='Pretrained checkpoint')\n",
        "    parser.add_argument('--lora_ckpt', type=str, default='checkpoints/epoch_159.pth', help='The checkpoint from LoRA')\n",
        "    parser.add_argument('--vit_name', type=str, default='vit_b', help='Select one vit model')\n",
        "    parser.add_argument('--rank', type=int, default=4, help='Rank for LoRA adaptation')\n",
        "    parser.add_argument('--module', type=str, default='sam_lora_image_encoder')\n",
        "\n",
        "    parser.add_argument('--base_lr', type=float, default=0.005, help='segmentation network learning rate')\n",
        "    parser.add_argument('--batch_size', type=int, default=12, help='batch_size per gpu')\n",
        "    parser.add_argument('--warmup', type=bool, default=True, help='If activated, warp up the learning from a lower lr to the base_lr')\n",
        "    parser.add_argument('--warmup_period', type=int, default=250, help='Warp up iterations, only valid whrn warmup is activated')\n",
        "    parser.add_argument('--AdamW', type=bool, default=True, help='If activated, use AdamW to finetune SAM model')\n",
        "    parser.add_argument('--max_epochs', type=int, default=10, help='maximum epoch number to train')\n",
        "    parser.add_argument('--max_iterations', type=int, default=30000, help='maximum epoch number to train')\n",
        "\n",
        "\n",
        "    if 'ipykernel' in sys.modules:\n",
        "        args = parser.parse_args([])\n",
        "    else:\n",
        "        args = parser.parse_args()\n",
        "\n",
        "    args.ckpt = 'sam_vit_b_01ec64.pth'\n",
        "    args.lora_ckpt = 'results/model_best.pt'\n",
        "    sam, img_embedding_size = sam_model_registry[args.vit_name](image_size=args.img_size,\n",
        "                                                                    num_classes=args.num_classes,\n",
        "                                                                    checkpoint=args.ckpt, pixel_mean=[0, 0, 0],\n",
        "                                                                    pixel_std=[1, 1, 1])\n",
        "\n",
        "    net = LoRA_Sam_v0_v2(sam, args.rank).cuda()\n",
        "    ce_loss = CrossEntropyLoss()\n",
        "    dice_loss = DiceLoss(args.num_classes + 1)\n",
        "\n",
        "    assert args.lora_ckpt is not None\n",
        "    net.load_lora_parameters(args.lora_ckpt)\n",
        "    testloader = DataLoader(test_dataset, batch_size=20, shuffle=False, num_workers=2)\n",
        "    test_loss, overall_dice, dices_per_class = test_per_epoch(net, testloader, ce_loss, dice_loss, multimask_output=True, args=args)\n",
        "    dices_per_class_list = np.array(list(dices_per_class.values()))\n",
        "    print('Class Wise Dice:', dices_per_class)\n",
        "    print('Overall Dice:', np.mean(dices_per_class_list))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference MRI to MRI<br>\n",
        "download test mri"
      ],
      "metadata": {
        "id": "OeaaFJN0iUvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "url = 'https://drive.google.com/uc?id=1zcvnBscFVI2v5ieAlGGnp0zpX8WmIrD4'\n",
        "gdown.download(url,'endonasal_mri_patients.zip',quiet=True)\n",
        "!unzip -q endonasal_mri_patients\n",
        "!rm -rf /content/endonasal_mri_patients/.DS_Store\n",
        "!rm -rf /content/endonasal_mri_patients/**/.DS_Store"
      ],
      "metadata": {
        "id": "u55aRX3KjbPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MRI to MRI"
      ],
      "metadata": {
        "id": "3mLQJNoU0rId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nibabel as nib\n",
        "import cv2\n",
        "import numpy as np\n",
        "from scipy.ndimage import zoom\n",
        "from einops import repeat\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def read_mri(mri_path):\n",
        "    img_meta = nib.load(mri_path)\n",
        "    array = img_meta.get_fdata()\n",
        "    return np.rot90(array)\n",
        "\n",
        "def normalise_intensity(image, ROI_thres=0.1):\n",
        "    pixel_thres = np.percentile(image, ROI_thres)\n",
        "    ROI = np.where(image > pixel_thres, image, 0) # If image value is greater than pixel threshold, return image value, otherwise return 0\n",
        "    mean = np.mean(ROI)\n",
        "    std = np.std(ROI)\n",
        "    ROI_norm = (ROI - mean) / (std + 1e-8) # Normalise ROI\n",
        "    return ROI_norm\n",
        "\n",
        "class EndonasalDataset_MRI(Dataset):\n",
        "    def __init__(self, root='endonasal_mri_patients', patient=None, low_res=None, isTrain=False):\n",
        "\n",
        "        mri_path = 'endonasal_mri_patients/mri0{}/mri0{}_t1c.nii.gz'.format(patient, patient)\n",
        "        self.mask_path = 'endonasal_mri_patients/mri0{}/mri0{}_mask.nii.gz'.format(patient, patient)\n",
        "        mri_array = read_mri(mri_path)\n",
        "        self.image_all = []\n",
        "        for z in range(mri_array.shape[2]):\n",
        "            normalized_slice = cv2.normalize(mri_array[:, :, z], None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
        "            self.image_all.append(normalise_intensity(normalized_slice))\n",
        "\n",
        "        self.mask_all = read_mri(self.mask_path)\n",
        "        self.isTrain = isTrain\n",
        "        self.low_res = low_res\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.image_all)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image = self.image_all[index]\n",
        "        image = zoom(image, (512/image.shape[0], 512/image.shape[1]), order=0)\n",
        "        label = self.mask_all[:,:,index]\n",
        "        label = zoom(label, (512/label.shape[0], 512/label.shape[1]), order=0)\n",
        "        if self.isTrain:\n",
        "            if random.random() > 0.5:\n",
        "                image, label = random_rot_flip(image, label)\n",
        "            elif random.random() > 0.5:\n",
        "                image, label = random_rotate(image, label)\n",
        "\n",
        "        image = repeat(np.expand_dims(image, axis=0), 'c h w -> (repeat c) h w', repeat=3)\n",
        "        sample = {'image': image, 'label': label}\n",
        "        if self.low_res:\n",
        "            low_res_label = zoom(label, (self.low_res/label.shape[0], self.low_res/label.shape[1]), order=0)\n",
        "            sample = {'image': image, 'label': label, 'low_res_label': low_res_label, 'maskpath': self.mask_path}\n",
        "\n",
        "        return sample\n",
        "\n",
        "\n",
        "def calculate_confusion_matrix_from_arrays(prediction, ground_truth, nr_labels):\n",
        "    replace_indices = np.vstack((\n",
        "        ground_truth.flatten(),\n",
        "        prediction.flatten())\n",
        "    ).T\n",
        "    confusion_matrix, _ = np.histogramdd(\n",
        "        replace_indices,\n",
        "        bins=(nr_labels, nr_labels),\n",
        "        range=[(0, nr_labels), (0, nr_labels)]\n",
        "    )\n",
        "    confusion_matrix = confusion_matrix.astype(np.uint32)\n",
        "    return confusion_matrix\n",
        "\n",
        "def calculate_dice(confusion_matrix):\n",
        "    dices = []\n",
        "    for index in range(confusion_matrix.shape[0]):\n",
        "        true_positives = confusion_matrix[index, index]\n",
        "        false_positives = confusion_matrix[:, index].sum() - true_positives\n",
        "        false_negatives = confusion_matrix[index, :].sum() - true_positives\n",
        "        denom = 2 * true_positives + false_positives + false_negatives\n",
        "        if denom == 0:\n",
        "            dice = 0\n",
        "        else:\n",
        "            dice = 2 * float(true_positives) / denom\n",
        "        dices.append(dice)\n",
        "    return dices\n",
        "\n",
        "def pred_to_mri(pred_seg_all, mask_path):\n",
        "    os.makedirs('predicted_mri', mode = 0o777, exist_ok = True)\n",
        "    img_meta = nib.load(mask_path)\n",
        "    pred_seg_all = np.rot90(np.array(pred_seg_all).transpose(1,2,0), k=-1)\n",
        "    img_nifti = nib.Nifti1Image(pred_seg_all, img_meta.affine, header=img_meta.header)\n",
        "    nib.save(img_nifti,'predicted_mri/'+os.path.basename(mask_path))\n",
        "\n",
        "def test_per_epoch(model, testloader, ce_loss, dice_loss, multimask_output=True, args=None):\n",
        "    model.eval()\n",
        "    loss_per_epoch, dice_per_epoch = [], []\n",
        "    num_classes = args.num_classes + 1\n",
        "    confusion_matrix = np.zeros((num_classes, num_classes), dtype=np.uint32)\n",
        "    class_wise_dice = []\n",
        "    pred_seg_all = []\n",
        "    with torch.no_grad():\n",
        "        for i_batch, sampled_batch in enumerate(testloader):\n",
        "            image_batch, label_batch, low_res_label_batch = sampled_batch['image'],sampled_batch['label'], sampled_batch['low_res_label']\n",
        "            image_batch, label_batch, low_res_label_batch = image_batch.to(device, dtype=torch.float32), label_batch.to(device, dtype=torch.long), low_res_label_batch.to(device, dtype=torch.long)\n",
        "            outputs = model(image_batch, multimask_output, args.img_size)\n",
        "            logits = outputs['masks']\n",
        "            prob = F.softmax(logits, dim=1)\n",
        "            pred_seg = torch.argmax(prob, dim=1)\n",
        "            pred_seg_all.extend(pred_seg.detach().cpu().numpy())\n",
        "            confusion_matrix += calculate_confusion_matrix_from_arrays(pred_seg.cpu(), label_batch.cpu(), num_classes)\n",
        "            loss, loss_ce, loss_dice = calc_loss(outputs, low_res_label_batch, ce_loss, dice_loss)\n",
        "            loss_per_epoch.append(loss.item())\n",
        "            dice_per_epoch.append(1-loss_dice.item())\n",
        "            low_res_logits = outputs['low_res_logits']\n",
        "            loss_dice = dice_loss(low_res_logits, low_res_label_batch, softmax=True)\n",
        "            metric_list = []\n",
        "            pred_seg, label_batch = pred_seg.cpu().detach().numpy(), label_batch.cpu().detach().numpy()\n",
        "\n",
        "        pred_to_mri(np.array(pred_seg_all), sampled_batch['maskpath'][0])\n",
        "        confusion_matrix = confusion_matrix[1:, 1:]  # exclude background\n",
        "        dices_per_class = {'dice_cls:{}'.format(cls + 1): round(dice, 4)\n",
        "                    for cls, dice in enumerate(calculate_dice(confusion_matrix))}\n",
        "\n",
        "    return np.mean(loss_per_epoch), np.mean(dice_per_epoch), dices_per_class\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--config', type=str, default=None, help='The config file provided by the trained model')\n",
        "    parser.add_argument('--volume_path', type=str, default='testset/test_vol_h5/')\n",
        "    parser.add_argument('--dataset', type=str, default='Synapse', help='Experiment name')\n",
        "    parser.add_argument('--num_classes', type=int, default=2)\n",
        "    parser.add_argument('--list_dir', type=str, default='./lists/lists_Synapse/', help='list_dir')\n",
        "    parser.add_argument('--output_dir', type=str, default='results')\n",
        "    parser.add_argument('--img_size', type=int, default=512, help='Input image size of the network')\n",
        "    parser.add_argument('--input_size', type=int, default=224, help='The input size for training SAM model')\n",
        "    parser.add_argument('--seed', type=int,\n",
        "                        default=1234, help='random seed')\n",
        "    parser.add_argument('--is_savenii', action='store_true', help='Whether to save results during inference')\n",
        "    parser.add_argument('--deterministic', type=int, default=1, help='whether use deterministic training')\n",
        "    parser.add_argument('--ckpt', type=str, default='checkpoints/sam_vit_b_01ec64.pth',\n",
        "                        help='Pretrained checkpoint')\n",
        "    parser.add_argument('--lora_ckpt', type=str, default='checkpoints/epoch_159.pth', help='The checkpoint from LoRA')\n",
        "    parser.add_argument('--vit_name', type=str, default='vit_b', help='Select one vit model')\n",
        "    parser.add_argument('--rank', type=int, default=4, help='Rank for LoRA adaptation')\n",
        "    parser.add_argument('--module', type=str, default='sam_lora_image_encoder')\n",
        "\n",
        "    parser.add_argument('--base_lr', type=float, default=0.005, help='segmentation network learning rate')\n",
        "    parser.add_argument('--batch_size', type=int, default=12, help='batch_size per gpu')\n",
        "    parser.add_argument('--warmup', type=bool, default=True, help='If activated, warp up the learning from a lower lr to the base_lr')\n",
        "    parser.add_argument('--warmup_period', type=int, default=250, help='Warp up iterations, only valid whrn warmup is activated')\n",
        "    parser.add_argument('--AdamW', type=bool, default=True, help='If activated, use AdamW to finetune SAM model')\n",
        "    parser.add_argument('--max_epochs', type=int, default=10, help='maximum epoch number to train')\n",
        "    parser.add_argument('--max_iterations', type=int, default=30000, help='maximum epoch number to train')\n",
        "\n",
        "\n",
        "    if 'ipykernel' in sys.modules:\n",
        "        args = parser.parse_args([])\n",
        "    else:\n",
        "        args = parser.parse_args()\n",
        "\n",
        "    args.ckpt = 'sam_vit_b_01ec64.pth'\n",
        "    args.lora_ckpt = 'results/model_best.pt'\n",
        "    sam, img_embedding_size = sam_model_registry[args.vit_name](image_size=args.img_size,\n",
        "                                                                    num_classes=args.num_classes,\n",
        "                                                                    checkpoint=args.ckpt, pixel_mean=[0, 0, 0],\n",
        "                                                                    pixel_std=[1, 1, 1])\n",
        "\n",
        "    net = LoRA_Sam_v0_v2(sam, args.rank).cuda()\n",
        "    ce_loss = CrossEntropyLoss()\n",
        "    dice_loss = DiceLoss(args.num_classes + 1)\n",
        "\n",
        "    assert args.lora_ckpt is not None\n",
        "    net.load_lora_parameters(args.lora_ckpt)\n",
        "\n",
        "    patients = ['154', '169', '170']\n",
        "    mean_overall = []\n",
        "    tumor_overall = []\n",
        "    carotid_overall = []\n",
        "    for patient in patients:\n",
        "        test_dataset = EndonasalDataset_MRI(root='endonasal_mri_patients', patient=patient, low_res=128)\n",
        "        testloader = DataLoader(test_dataset, batch_size=20, shuffle=False, num_workers=2)\n",
        "        test_loss, overall_dice, dices_per_class = test_per_epoch(net, testloader, ce_loss, dice_loss, multimask_output=True, args=args)\n",
        "        dices_per_class_list = np.array(list(dices_per_class.values()))\n",
        "        overall = round(np.mean(dices_per_class_list),4)\n",
        "        mean_overall.append(overall)\n",
        "        tumor_overall.append(dices_per_class['dice_cls:1'])\n",
        "        carotid_overall.append(dices_per_class['dice_cls:2'])\n",
        "        print('Patient:', patient, ',Class Wise:', dices_per_class, ',Overall :', overall)\n",
        "\n",
        "    print('Overall Model Performance [Mean Overall]:', round(np.mean(mean_overall),4), '[cls-1:{}]'.format(round(np.mean(tumor_overall),4)),\\\n",
        "            '[cls-2:{}]'.format(round(np.mean(carotid_overall),4)))\n"
      ],
      "metadata": {
        "id": "IJKKAr8UiTry",
        "outputId": "1bbecab0-dff9-4478-d4f4-6c5916cb1ca9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Patient: 154 ,Class Wise: {'dice_cls:1': 0.9737, 'dice_cls:2': 0.7039} ,Overall : 0.8388\n",
            "Patient: 169 ,Class Wise: {'dice_cls:1': 0.991, 'dice_cls:2': 0.9621} ,Overall : 0.9766\n",
            "Patient: 170 ,Class Wise: {'dice_cls:1': 0.9894, 'dice_cls:2': 0.8928} ,Overall : 0.9411\n",
            "Overall Model Performance [Mean Overall]: 0.9188 [cls-1:0.9847] [cls-2:0.8529]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Post Processing to remove outliers (small seg)"
      ],
      "metadata": {
        "id": "_dBB8gMP8yHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage import morphology\n",
        "import nibabel as nib\n",
        "mask_meta = nib.load('/content/samed_codes/endonasal_mri_patients/mri0169/mri0169_mask.nii.gz')\n",
        "mask = nib.load('/content/samed_codes/predicted_mri/mri0169_mask.nii.gz').get_fdata()\n",
        "\n",
        "binary_mask = morphology.remove_small_objects(mask>0, 50)\n",
        "mask[binary_mask==0] = 0\n",
        "img_nifti = nib.Nifti1Image(mask, mask_meta.affine, header=mask_meta.header)\n",
        "nib.save(img_nifti,'mri0169_mask_post_processed.nii.gz')"
      ],
      "metadata": {
        "id": "c8y3CsOW1O6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Er8Wnp5M85Zd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}