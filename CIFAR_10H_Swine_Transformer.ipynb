{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/tutorial_notebooks/blob/main/CIFAR_10H_Swine_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vL2RMWddkaQ"
      },
      "source": [
        "# Cloning repository of CIFAR-10H Annotation\n",
        "Paper: Human uncertainty makes classification more robust (https://arxiv.org/pdf/1908.07086.pdf)\n",
        "\n",
        "Label:  CIFAR10 [0: airplane, 1: automobile, 2: bird, 3: cat, 4: deer, 5: dog, 6: frog, 7: horse, 8: ship, 9: truck] <br>\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1010/1*r8S5tF_6naagKOnlIcGXoQ.png\" alt=\"alternatetext\">\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KrQIqW6_NYl",
        "outputId": "7c08862e-b96a-40f0-a45a-cee020e83cd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'cifar-10h'...\n",
            "remote: Enumerating objects: 49, done.\u001b[K\n",
            "remote: Counting objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 49 (delta 0), reused 0 (delta 0), pack-reused 48\u001b[K\n",
            "Unpacking objects: 100% (49/49), done.\n",
            "/content/cifar-10h\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/jcpeterson/cifar-10h\n",
        "%cd cifar-10h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3cVy5LVFfrZ"
      },
      "source": [
        "## Installing huggingface transformer "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIzJpadXFmUC",
        "outputId": "7508788b-5b76-4e0f-c3f7-b827f1cbb9f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 3.8 MB 5.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 325 kB 50.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 47.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 67 kB 5.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 895 kB 45.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 37.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 134 kB 47.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 41.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 212 kB 50.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 127 kB 36.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 144 kB 46.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 271 kB 46.5 MB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "! pip -q install transformers datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKubHBaKHriM",
        "outputId": "ba6ce52f-f5ca-4ca1-d6b5-47b37b2ab983"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/cifar-10h\n"
          ]
        }
      ],
      "source": [
        "%cd cifar-10h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNN4TybGd2Q_"
      },
      "source": [
        "# main script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "NN371ewSCWX7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import models\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "import argparse\n",
        "import copy\n",
        "import random\n",
        "import numpy as np\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "def seed_everything(seed=12):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "parser = argparse.ArgumentParser(description='CIFAR-10H Training')\n",
        "parser.add_argument('--lr', default=3e-2, type=float, help='learning rate')\n",
        "parser.add_argument('--lr_schedule', default=0, type=int, help='lr scheduler')\n",
        "parser.add_argument('--batch_size', default=32, type=int, help='batch size')\n",
        "parser.add_argument('--test_batch_size', default=64, type=int, help='batch size')\n",
        "parser.add_argument('--num_epoch', default=10, type=int, help='epoch number')\n",
        "parser.add_argument('--num_classes', type=int, default=10, help='number classes')\n",
        "args = parser.parse_args(args=[])\n",
        "\n",
        "def train(model, trainloader, criterion, optimizer):\n",
        "    model.train()\n",
        "    for batch_idx, (inputs, targets, ad) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)#.logits\n",
        "        #print(outputs.shape)\n",
        "        #print(outputs.last_hidden_state.shape, outputs.pooler_output.shape)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx%100 == 0:\n",
        "            print(batch_idx,'/',len(trainloader),'loss:',loss.item())\n",
        "\n",
        "def test(model, testloader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)#.logits\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "    return correct / total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHKUA3yBd87G"
      },
      "source": [
        "# CIFAR-10H dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "z4uiWMiyCkjk"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from transformers import AutoFeatureExtractor, SwinForImageClassification\n",
        "#from transformers import ViTFeatureExtractor, ViTForImageClassification, BatchFeature\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.transforms import ToTensor, Normalize, Resize, Compose\n",
        "\n",
        "import torch\n",
        "\n",
        "class AutoFeatureExtractorTransforms:\n",
        "    def __init__(self, model_name_or_path):\n",
        "        feature_extractor = AutoFeatureExtractor.from_pretrained(model_name_or_path)\n",
        "        transform = []\n",
        "\n",
        "        if feature_extractor.do_resize:\n",
        "            transform.append(Resize(feature_extractor.size))\n",
        "\n",
        "        transform.append(ToTensor())\n",
        "\n",
        "        if feature_extractor.do_normalize:\n",
        "            transform.append(Normalize(feature_extractor.image_mean, feature_extractor.image_std))\n",
        "\n",
        "        self.transform = Compose(transform)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.transform(x)\n",
        "\n",
        "class CIFAR10H(torchvision.datasets.CIFAR10):\n",
        "\n",
        "    def __init__(self, root,  rand_number=0, train=False, transform=None, target_transform=None,\n",
        "                 download=False):\n",
        "        super(CIFAR10H, self).__init__(root, train, transform, target_transform, download) \n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        self.ad = np.load(os.path.join(root,'cifar10h-probs.npy'))\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        img, target = self.data[index], self.targets[index]\n",
        "        img = Image.fromarray(img)\n",
        "        ad = self.ad[index]\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        "        return img, target, ad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRQEQIdaerKa"
      },
      "source": [
        "# Run script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rpOrSuEFY2N",
        "outputId": "b2cf8436-6fa6-41cd-85ae-118ff7fa0512"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "train samples: 10000 test samples: 50000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at microsoft/swin-tiny-patch4-window7-224 were not used when initializing SwinModel: ['classifier.bias', 'classifier.weight']\n",
            "- This IS expected if you are initializing SwinModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing SwinModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 / 313 loss: 2.280766010284424\n",
            "100 / 313 loss: 2.408679723739624\n",
            "200 / 313 loss: 2.114832639694214\n",
            "300 / 313 loss: 2.3013603687286377\n",
            "epoch: 0  acc: 0.2184  best epoch: 0  best acc: 0.2184\n",
            "0 / 313 loss: 2.143496036529541\n",
            "100 / 313 loss: 1.7713689804077148\n",
            "200 / 313 loss: 1.8079752922058105\n",
            "300 / 313 loss: 2.0463404655456543\n",
            "epoch: 1  acc: 0.3143  best epoch: 1  best acc: 0.3143\n",
            "0 / 313 loss: 1.7374987602233887\n",
            "100 / 313 loss: 1.8105531930923462\n",
            "200 / 313 loss: 2.0506858825683594\n",
            "300 / 313 loss: 1.822037696838379\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "from transformers import AutoFeatureExtractor, SwinModel\n",
        "class SwineTransformer(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(SwineTransformer, self).__init__()\n",
        "        \n",
        "        self.base_model = SwinModel.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\")\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.linear = nn.Linear(768, num_classes) # output features from bert is 768 and 2 is ur number of labels\n",
        "        \n",
        "    def forward(self, input_ids):\n",
        "        outputs = self.base_model(input_ids)\n",
        "        outputs = self.dropout(outputs[1])\n",
        "        outputs = self.linear(outputs)\n",
        "        \n",
        "        return outputs\n",
        "\n",
        "def main():\n",
        "    seed_everything()\n",
        "    #2e-5,\n",
        "    model_name_or_path = 'microsoft/swin-tiny-patch4-window7-224'\n",
        "\n",
        "\n",
        "    train_dataset = CIFAR10H(root='./data', train=False, download=True, transform=AutoFeatureExtractorTransforms(model_name_or_path))\n",
        "    test_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=AutoFeatureExtractorTransforms(model_name_or_path))\n",
        "    print('train samples:',len(train_dataset), 'test samples:',len(test_dataset))\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=2)\n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.test_batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    #model = SwinForImageClassification.from_pretrained(model_name_or_path, num_labels=args.num_classes)\n",
        "\n",
        "    # model = models.resnet34(pretrained=True).to(device)\n",
        "    # model.fc = nn.Linear(model.fc.in_features, args.num_classes)\n",
        "    model = model = SwineTransformer(num_classes=args.num_classes)\n",
        "    model = model.to(device)\n",
        "\n",
        "    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=0.9, nesterov=False, weight_decay=0.0001)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_epoch, best_acc = 0.0, 0\n",
        "    for epoch in range(args.num_epoch):\n",
        "        train(model, train_loader, criterion, optimizer)\n",
        "        accuracy = test(model, test_loader)\n",
        "        if accuracy > best_acc:\n",
        "            patience = 0\n",
        "            best_acc = accuracy\n",
        "            best_epoch = epoch\n",
        "            best_model = copy.deepcopy(model)\n",
        "            torch.save(best_model.state_dict(), 'best_model_cifar10h_vit.pth.tar')\n",
        "        print('epoch: {}  acc: {:.4f}  best epoch: {}  best acc: {:.4f}'.format(\n",
        "                epoch, accuracy, best_epoch, best_acc, optimizer.param_groups[0]['lr']))\n",
        "        \n",
        "main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "CIFAR-10H_Swine_Transformer.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}