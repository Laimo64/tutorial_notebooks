{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/tutorial_notebooks/blob/main/vector_mora.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install peft"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swpvJW1UfKea",
        "outputId": "d2b1e757-4f28-455f-ceea-2499694aba55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.4.0+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.44.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.5)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.33.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.24.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.19.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ly = nn.Conv1d(128, 128, kernel_size=1, bias=False)"
      ],
      "metadata": {
        "id": "_zqqRzo0bsjx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "pgKHxoaDfHSD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from transformers import ViTModel, BlipConfig, BlipTextModel\n",
        "\n",
        "# from peft import LoraConfig, get_peft_model\n",
        "import math\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class Vector_MoRA(nn.Module):\n",
        "    def __init__(self, w_qkv, mora_rank, lora_dropout):\n",
        "        super().__init__()\n",
        "        self.base_layer = w_qkv  # original c_atten layer\n",
        "        self.r = mora_rank  # one of mora rank elements in the list\n",
        "        self.in_features = w_qkv.weight.shape[0]  # 768\n",
        "        self.out_features = w_qkv.weight.shape[1]  # 2304\n",
        "\n",
        "        # LoRA dropout\n",
        "        self.lora_dropout = nn.ModuleDict({\n",
        "            'default': nn.Dropout(p=lora_dropout)\n",
        "        })\n",
        "\n",
        "        # LoRA A and B matrices\n",
        "        # self.lora_A = nn.ModuleDict({\n",
        "        #     'default': nn.Linear(self.r, self.r, bias=False)\n",
        "        # })\n",
        "\n",
        "        self.lora_A = nn.ModuleDict({\n",
        "            'default': nn.Conv1d(self.r, self.r, bias=False, kernel_size=1)\n",
        "        })\n",
        "\n",
        "        nn.init.zeros_(self.lora_A['default'].weight)\n",
        "        self.lora_B = self.lora_A  # not for use\n",
        "\n",
        "        # For Embedding layer\n",
        "        self.lora_embedding_A = nn.ParameterDict({})\n",
        "        self.lora_embedding_B = nn.ParameterDict({})\n",
        "\n",
        "    def forward(self, x):\n",
        "    # x: torch.Size([1, 32, 768])\n",
        "    # in_x: torch.Size([1, 32, 5, 176])\n",
        "    # in_x_2: torch.Size([1, 32, 5, 176])\n",
        "    # out_x: torch.Size([1, 32, 5, 176])\n",
        "    # out_x_2: torch.Size([1, 32, 2304])\n",
        "    # logits: torch.Size([1, 32, 50257])\n",
        "\n",
        "        # Original output\n",
        "        result = self.base_layer(x)\n",
        "        x = self.lora_dropout['default'](x)  # x is the input for mora\n",
        "        print('-------- new block start --------')\n",
        "        print(f'x: {x.shape}')\n",
        "\n",
        "        '''apply compression before lora_A'''  # RoPE\n",
        "        in_f, out_f = self.in_features, self.out_features\n",
        "        r = self.r\n",
        "        # suppose mora_type = 6\n",
        "        sum_inter = in_f // r\n",
        "        rb1 = in_f // r if in_f % r == 0 else in_f // r + 1\n",
        "\n",
        "        if in_f % r != 0:\n",
        "            pad_size = r - in_f % r\n",
        "            x = torch.cat([x, x[..., :pad_size]], dim=-1)  # [32, 50, 780]\n",
        "            sum_inter += 1\n",
        "        in_x = x.view(*x.shape[:-1], sum_inter, r)  # [32, 50, 5, 156]\n",
        "        print(f'in_x after reshape: {in_x.shape}')\n",
        "\n",
        "        if not hasattr(self, 'cos') and not hasattr(self, 'sin'):\n",
        "            inv_freq = 1.0 / (10000 ** (torch.arange(0, r, 2).float() / r))  # torch.Size([78])\n",
        "            t = torch.arange(rb1)  # tensor([0, 1, 2, 3, 4])\n",
        "            freqs = torch.outer(t, inv_freq)  # [5, 78]\n",
        "            emb = torch.cat((freqs, freqs), dim=-1)  # [5, 156]\n",
        "            self.cos = emb.cos().unsqueeze(0).to(x.device).to(x.dtype)  # [1, 5, 156]\n",
        "            self.sin = emb.sin().unsqueeze(0).to(x.device).to(x.dtype)\n",
        "\n",
        "        rh_in_x = torch.cat((-in_x[..., r // 2:], in_x[..., :r // 2]), dim=-1)  # [32, 50, 5, 156]\n",
        "        # rh_in_x 最后一个维度的前 r//2 个元素是 in_x 后半部分的负值, rh_in_x 最后一个维度的后 r//2 个元素是 in_x 前半部分的原值\n",
        "        in_x = in_x * self.cos + rh_in_x * self.sin  # [32, 50, 5, 156]\n",
        "        print(f'in_x after RoPE: {in_x.shape}')\n",
        "\n",
        "        '''apply lora_A'''\n",
        "        out_x = self.lora_A['default'](in_x)  # [32, 50, 5, 156]\n",
        "        print(f'out_x after lora_A: {out_x.shape}')\n",
        "\n",
        "        '''apply decompression after lora_A'''\n",
        "        # suppose mora_type = 6\n",
        "        out_x = out_x.view(*x.shape[:-1], -1)[..., :out_f]  # [32, 50, 780]\n",
        "        if out_x.shape[-1] < out_f:\n",
        "            repeat_time = out_f // out_x.shape[-1]\n",
        "            if out_f % out_x.shape[-1] != 0:\n",
        "                repeat_time += 1\n",
        "            out_x = torch.cat([out_x] * repeat_time, dim=-1)[..., :out_f]  # [32, 50, 2304]\n",
        "        print(f'out_x after decompression: {out_x.shape}')\n",
        "        print('-------- block end here --------')\n",
        "\n",
        "        return result + out_x\n",
        "\n",
        "class VectorMoRAInitializer:\n",
        "    def __init__(self, model, base_rank=8, mora_rank_coefficients=None, lora_dropout=0.01):\n",
        "        self.model = model\n",
        "        self.base_rank = base_rank\n",
        "        self.lora_dropout = lora_dropout\n",
        "        if mora_rank_coefficients is None:\n",
        "            self.mora_rank_coefficients = [32, 32, 30, 30, 28, 28, 26, 26, 24, 24, 22, 22]\n",
        "        else:\n",
        "            self.mora_rank_coefficients = mora_rank_coefficients\n",
        "\n",
        "    def calculate_mora_ranks(self):\n",
        "        return [self.base_rank * coeff for coeff in self.mora_rank_coefficients]\n",
        "\n",
        "    def initialize_mora(self):\n",
        "        mora_ranks = self.calculate_mora_ranks()\n",
        "\n",
        "        for param in self.model.transformer.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        for t_layer_i, blk in enumerate(self.model.transformer.h):\n",
        "            w_qkv = blk.attn.c_attn\n",
        "            mora_rank = mora_ranks[t_layer_i]\n",
        "            print(f'-------- layer: {t_layer_i}, current mora rank: {mora_rank }--------')\n",
        "            blk.attn.c_attn = Vector_MoRA(w_qkv, mora_rank, self.lora_dropout)\n",
        "\n",
        "        print(\"Vector MoRA params initialized!\")\n",
        "        return self.model\n",
        "\n",
        "class PitVQAGen(nn.Module):\n",
        "    def __init__(self, base_rank=8, mora_rank_coefficients=None):\n",
        "        super(PitVQAGen, self).__init__()\n",
        "\n",
        "        # gpt2 decoder\n",
        "        self.gpt = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "        if mora_rank_coefficients is None:\n",
        "            mora_rank_coefficients = [32, 32, 30, 30, 28, 28, 26, 26, 24, 24, 22, 22]\n",
        "        self.gpt = VectorMoRAInitializer(self.gpt, base_rank=base_rank,\n",
        "                        mora_rank_coefficients = mora_rank_coefficients\n",
        "                        ).initialize_mora()\n",
        "        # print(f'after mora: {self.gpt}')\n",
        "\n",
        "        # visual encoder\n",
        "        model_name = \"google/vit-base-patch16-224-in21k\"\n",
        "        self.visual_encoder = ViTModel.from_pretrained(model_name)\n",
        "\n",
        "        # tokenizer\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token  # end of string\n",
        "\n",
        "        # text encoder\n",
        "        config = BlipConfig.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
        "        self.text_encoder = BlipTextModel(config.text_config, add_pooling_layer=False)\n",
        "\n",
        "        # modify embedding layer\n",
        "        new_vocab_size = len(self.tokenizer)\n",
        "        embedding_dim = self.text_encoder.embeddings.word_embeddings.embedding_dim\n",
        "        self.text_encoder.embeddings.word_embeddings = nn.Embedding(new_vocab_size, embedding_dim)  # He init\n",
        "\n",
        "    def forward(self, image, question_inputs):\n",
        "        # visual encoder\n",
        "        image = image.to(device)\n",
        "        image_embeds = self.visual_encoder(image).last_hidden_state  # torch.Size([bs, 197, 768])\n",
        "        image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to(image.device)  # torch.Size([bs, 197])\n",
        "\n",
        "        question_input_ids = question_inputs['input_ids']  # torch.Size([bs, 25])\n",
        "        question_att_mask = question_inputs['attention_mask']\n",
        "\n",
        "        # multimodal encoder\n",
        "        text_output = self.text_encoder(input_ids=question_input_ids,\n",
        "                        attention_mask=question_att_mask,\n",
        "                        encoder_hidden_states=image_embeds,\n",
        "                        encoder_attention_mask=image_atts,\n",
        "                        return_dict=True)\n",
        "        text_embeds = text_output.last_hidden_state  # torch.Size([bs, 25, 768]), args.question_len=25\n",
        "\n",
        "        # text decoder\n",
        "        gpt_output = self.gpt(inputs_embeds=text_embeds,\n",
        "                    encoder_attention_mask=question_att_mask)  # torch.Size([bs, 25, 50257])\n",
        "        return gpt_output.logits"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "mora_rank_coefficients = [32, 32, 30, 30, 28, 28, 26, 26, 24, 24, 22, 22]\n",
        "model = PitVQAGen(base_rank=8, mora_rank_coefficients=mora_rank_coefficients)\n",
        "\n",
        "\n",
        "!gdown 1Kg-dwCsKivNKubEPXWmOopuw91v3megC\n",
        "transform = transforms.Compose([\n",
        "       transforms.Resize((224, 224), interpolation=InterpolationMode.BICUBIC),\n",
        "       transforms.ToTensor(),\n",
        "       ])\n",
        "raw_image = Image.open('frame052.png').convert('RGB')\n",
        "image = transform(raw_image)\n",
        "if image.dim() == 3:\n",
        "    image = image.unsqueeze(0)  # add batch size\n",
        "\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "question = 'what is this?'\n",
        "question_inputs = tokenizer(question, padding=\"max_length\", max_length=32, return_tensors=\"pt\", truncation=True)\n",
        "\n",
        "\n",
        "logits = model(image, question_inputs)\n",
        "print(f'logits: {logits.shape}')\n",
        "\n",
        "# x: torch.Size([1, 32, 768])\n",
        "# in_x: torch.Size([1, 32, 5, 176])\n",
        "# in_x_2: torch.Size([1, 32, 5, 176])\n",
        "# out_x: torch.Size([1, 32, 5, 176])\n",
        "# out_x_2: torch.Size([1, 32, 2304])\n",
        "# logits: torch.Size([1, 32, 50257])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlyYSLnmfQyw",
        "outputId": "04d1774e-0abe-4df3-e125-272915212dae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------- layer: 0, current mora rank: 256--------\n",
            "-------- layer: 1, current mora rank: 256--------\n",
            "-------- layer: 2, current mora rank: 240--------\n",
            "-------- layer: 3, current mora rank: 240--------\n",
            "-------- layer: 4, current mora rank: 224--------\n",
            "-------- layer: 5, current mora rank: 224--------\n",
            "-------- layer: 6, current mora rank: 208--------\n",
            "-------- layer: 7, current mora rank: 208--------\n",
            "-------- layer: 8, current mora rank: 192--------\n",
            "-------- layer: 9, current mora rank: 192--------\n",
            "-------- layer: 10, current mora rank: 176--------\n",
            "-------- layer: 11, current mora rank: 176--------\n",
            "Vector MoRA params initialized!\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Kg-dwCsKivNKubEPXWmOopuw91v3megC\n",
            "To: /content/frame052.png\n",
            "100% 1.44M/1.44M [00:00<00:00, 114MB/s]\n",
            "-------- new block start --------\n",
            "x: torch.Size([1, 32, 768])\n",
            "in_x after reshape: torch.Size([1, 32, 3, 256])\n",
            "in_x after RoPE: torch.Size([1, 32, 3, 256])\n",
            "out_x after lora_A: torch.Size([1, 32, 3, 256])\n",
            "out_x after decompression: torch.Size([1, 32, 2304])\n",
            "-------- block end here --------\n",
            "-------- new block start --------\n",
            "x: torch.Size([1, 32, 768])\n",
            "in_x after reshape: torch.Size([1, 32, 3, 256])\n",
            "in_x after RoPE: torch.Size([1, 32, 3, 256])\n",
            "out_x after lora_A: torch.Size([1, 32, 3, 256])\n",
            "out_x after decompression: torch.Size([1, 32, 2304])\n",
            "-------- block end here --------\n",
            "-------- new block start --------\n",
            "x: torch.Size([1, 32, 768])\n",
            "in_x after reshape: torch.Size([1, 32, 4, 240])\n",
            "in_x after RoPE: torch.Size([1, 32, 4, 240])\n",
            "out_x after lora_A: torch.Size([1, 32, 4, 240])\n",
            "out_x after decompression: torch.Size([1, 32, 2304])\n",
            "-------- block end here --------\n",
            "-------- new block start --------\n",
            "x: torch.Size([1, 32, 768])\n",
            "in_x after reshape: torch.Size([1, 32, 4, 240])\n",
            "in_x after RoPE: torch.Size([1, 32, 4, 240])\n",
            "out_x after lora_A: torch.Size([1, 32, 4, 240])\n",
            "out_x after decompression: torch.Size([1, 32, 2304])\n",
            "-------- block end here --------\n",
            "-------- new block start --------\n",
            "x: torch.Size([1, 32, 768])\n",
            "in_x after reshape: torch.Size([1, 32, 4, 224])\n",
            "in_x after RoPE: torch.Size([1, 32, 4, 224])\n",
            "out_x after lora_A: torch.Size([1, 32, 4, 224])\n",
            "out_x after decompression: torch.Size([1, 32, 2304])\n",
            "-------- block end here --------\n",
            "-------- new block start --------\n",
            "x: torch.Size([1, 32, 768])\n",
            "in_x after reshape: torch.Size([1, 32, 4, 224])\n",
            "in_x after RoPE: torch.Size([1, 32, 4, 224])\n",
            "out_x after lora_A: torch.Size([1, 32, 4, 224])\n",
            "out_x after decompression: torch.Size([1, 32, 2304])\n",
            "-------- block end here --------\n",
            "-------- new block start --------\n",
            "x: torch.Size([1, 32, 768])\n",
            "in_x after reshape: torch.Size([1, 32, 4, 208])\n",
            "in_x after RoPE: torch.Size([1, 32, 4, 208])\n",
            "out_x after lora_A: torch.Size([1, 32, 4, 208])\n",
            "out_x after decompression: torch.Size([1, 32, 2304])\n",
            "-------- block end here --------\n",
            "-------- new block start --------\n",
            "x: torch.Size([1, 32, 768])\n",
            "in_x after reshape: torch.Size([1, 32, 4, 208])\n",
            "in_x after RoPE: torch.Size([1, 32, 4, 208])\n",
            "out_x after lora_A: torch.Size([1, 32, 4, 208])\n",
            "out_x after decompression: torch.Size([1, 32, 2304])\n",
            "-------- block end here --------\n",
            "-------- new block start --------\n",
            "x: torch.Size([1, 32, 768])\n",
            "in_x after reshape: torch.Size([1, 32, 4, 192])\n",
            "in_x after RoPE: torch.Size([1, 32, 4, 192])\n",
            "out_x after lora_A: torch.Size([1, 32, 4, 192])\n",
            "out_x after decompression: torch.Size([1, 32, 2304])\n",
            "-------- block end here --------\n",
            "-------- new block start --------\n",
            "x: torch.Size([1, 32, 768])\n",
            "in_x after reshape: torch.Size([1, 32, 4, 192])\n",
            "in_x after RoPE: torch.Size([1, 32, 4, 192])\n",
            "out_x after lora_A: torch.Size([1, 32, 4, 192])\n",
            "out_x after decompression: torch.Size([1, 32, 2304])\n",
            "-------- block end here --------\n",
            "-------- new block start --------\n",
            "x: torch.Size([1, 32, 768])\n",
            "in_x after reshape: torch.Size([1, 32, 5, 176])\n",
            "in_x after RoPE: torch.Size([1, 32, 5, 176])\n",
            "out_x after lora_A: torch.Size([1, 32, 5, 176])\n",
            "out_x after decompression: torch.Size([1, 32, 2304])\n",
            "-------- block end here --------\n",
            "-------- new block start --------\n",
            "x: torch.Size([1, 32, 768])\n",
            "in_x after reshape: torch.Size([1, 32, 5, 176])\n",
            "in_x after RoPE: torch.Size([1, 32, 5, 176])\n",
            "out_x after lora_A: torch.Size([1, 32, 5, 176])\n",
            "out_x after decompression: torch.Size([1, 32, 2304])\n",
            "-------- block end here --------\n",
            "logits: torch.Size([1, 32, 50257])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# copy from vector_lora_2.ipynb\n",
        "# this is the structure of GPT2 with peft-MoRA lib\n",
        "\n",
        "# PeftModelForSeq2SeqLM(\n",
        "#   (base_model): LoraModel(\n",
        "#     (model): GPT2LMHeadModel(\n",
        "#       (transformer): GPT2Model(\n",
        "#         (wte): Embedding(50257, 768)\n",
        "#         (wpe): Embedding(1024, 768)\n",
        "#         (drop): Dropout(p=0.1, inplace=False)\n",
        "#         (h): ModuleList(\n",
        "#           (0-11): 12 x GPT2Block(\n",
        "#             (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
        "#             (attn): GPT2SdpaAttention(\n",
        "#               (c_attn): lora.Linear(\n",
        "#                 (base_layer): Conv1D()\n",
        "#                 (lora_dropout): ModuleDict(\n",
        "#                   (default): Dropout(p=0.01, inplace=False)\n",
        "#                 )\n",
        "#                 (lora_A): ModuleDict(\n",
        "#                   (default): Linear(in_features=156, out_features=156, bias=False)\n",
        "#                 )\n",
        "#                 (lora_B): ModuleDict(\n",
        "#                   (default): Linear(in_features=156, out_features=156, bias=False)\n",
        "#                 )\n",
        "#                 (lora_embedding_A): ParameterDict()\n",
        "#                 (lora_embedding_B): ParameterDict()\n",
        "#               )\n",
        "#               (c_proj): Conv1D()\n",
        "#               (attn_dropout): Dropout(p=0.1, inplace=False)\n",
        "#               (resid_dropout): Dropout(p=0.1, inplace=False)\n",
        "#             )\n",
        "#             (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
        "#             (mlp): GPT2MLP(\n",
        "#               (c_fc): Conv1D()\n",
        "#               (c_proj): Conv1D()\n",
        "#               (act): NewGELUActivation()\n",
        "#               (dropout): Dropout(p=0.1, inplace=False)\n",
        "#             )\n",
        "#           )\n",
        "#         )\n",
        "#         (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
        "#       )\n",
        "#       (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
        "#     )\n",
        "#   )\n",
        "# )"
      ],
      "metadata": {
        "id": "OWxiOZTFArSR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}