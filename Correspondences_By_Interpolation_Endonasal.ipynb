{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/tutorial_notebooks/blob/main/Correspondences_By_Interpolation_Endonasal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CONFIG"
      ],
      "metadata": {
        "id": "NR_F6ye12voU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import config"
      ],
      "metadata": {
        "id": "qegQKMHV4mgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config.DATASET_PATH"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "zTacj9P-4sxV",
        "outputId": "57dfa0ae-613a-4074-d6b1-a0874edfaeab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'dataset/endo_videos'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# https://drive.google.com/file/d/1q-952htGhoEDVlYmnqkAA1UheCg-Yalg/view?usp=sharing\n",
        "# https://drive.google.com/file/d/1sV4j-wGBpfvL0O-VtTcyN6nygNjhaeuQ/view?usp=sharing\n",
        "import gdown\n",
        "url = 'https://drive.google.com/uc?id=1q-952htGhoEDVlYmnqkAA1UheCg-Yalg'\n",
        "gdown.download(url,'10.mp4',quiet=True) \n",
        "\n",
        "url = 'https://drive.google.com/uc?id=1sV4j-wGBpfvL0O-VtTcyN6nygNjhaeuQ'\n",
        "gdown.download(url,'09.mp4',quiet=True) "
      ],
      "metadata": {
        "id": "LVY6GiJ_2vAS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "37b5f04e-93d8-4ff6-dc2c-bb7a77e01b50"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'09.mp4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        " \n",
        "# Function to extract frames\n",
        "def FrameCapture(video_path=None, save_path=None): \n",
        "    # Path to video file\n",
        "    vidObj = cv2.VideoCapture(video_path) \n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    print(video_path, 'original fps:',fps)\n",
        "    # Used as counter variable\n",
        "    count = 0\n",
        "    # checks whether frames were extracted\n",
        "    success = 1 \n",
        "    while success:\n",
        "        # vidObj object calls read\n",
        "        # function extract frames\n",
        "        success, image = vidObj.read()\n",
        "        # Saves the frames with frame-count\n",
        "        try:\n",
        "            cv2.imwrite(save_path+\"/frame%06d.jpg\" % count, image)\n",
        "        except:\n",
        "            print('Done!')\n",
        "            continue\n",
        "        count += 1\n",
        "\n",
        "if not os.path.isdir(\"endonasal\"):\n",
        "    os.makedirs('endonasal/10')\n",
        "    os.makedirs('endonasal/09')\n",
        "\n",
        "\n",
        "FrameCapture(video_path='10.mp4', save_path='endonasal/10')\n",
        "print('total frames:')\n",
        "!ls endonasal | wc -l\n",
        "\n",
        "FrameCapture(video_path='09.mp4', save_path='endonasal/09')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJ71EoSnRG4t",
        "outputId": "ffe9b14a-ccd7-45d1-cfe1-6fee36b914da"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.mp4 original fps: 25.025126033800067\n",
            "Done!\n",
            "total frames:\n",
            "2\n",
            "09.mp4 original fps: 25.025126033800067\n",
            "Done!\n",
            "total frames:\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('total frames in endonasal/10:')\n",
        "!ls endonasal/10 | wc -l\n",
        "\n",
        "print('total frames in endonasal/09:')\n",
        "!ls endonasal/09 | wc -l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoTqcExOagFj",
        "outputId": "a17b9871-0493-4d3e-9ea4-61569f36b141"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total frames in endonasal/10:\n",
            "1984\n",
            "total frames in endonasal/09:\n",
            "1041\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "sequence_path = '09'\n",
        "img_paths = sorted(glob.glob(f'{sequence_path}/*.jpg'))\n",
        "img_paths"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LxftGoPcE5k",
        "outputId": "94b46b5c-5a25-427f-c803-65dadd41495f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PIL.Image\n",
        "from torch.utils.data import Dataset\n",
        "import cv2\n",
        "from torch import concat\n",
        "import PIL\n",
        "#from utils import config\n",
        "from torchvision import transforms\n",
        "import glob\n",
        "import numpy as np\n",
        "\n",
        "class endonasal(Dataset):\n",
        "    def __init__(self, sequence_paths=None, transform=None):\n",
        "        self.transform = transform\n",
        "        self.training_triplet_paths = []\n",
        "        # store the image and mask filepaths, and augmentation\n",
        "        # transforms\n",
        "        for sequence_path in sequence_paths:\n",
        "            # in each sequence there is many folders. We are interested in the image_02\n",
        "            # and image_03 which are the RGB. They represent stereo images so we simply take\n",
        "            # each as a new training example.\n",
        "            img_paths = sorted(glob.glob(f'{sequence_path}/*.jpg'))\n",
        "            # print(f'found {len(img_paths)} image paths for {sequence_path}/{mono_folder}')\n",
        "            for idx in range(len(img_paths)-2):\n",
        "                image_path_1 = img_paths[idx]\n",
        "                label_path = img_paths[idx + 1]  # middle image acts as interpolated version of images\n",
        "                image_path_2 = img_paths[idx + 2]\n",
        "                self.training_triplet_paths.append([image_path_1, label_path, image_path_2])\n",
        "\n",
        "    def __len__(self):\n",
        "        # return the number of total samples contained in the dataset\n",
        "        # print(f'found {len(self.training_triplet_paths)} examples')\n",
        "        return int(len(self.training_triplet_paths))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # grab the triplet of training data:\n",
        "        image_path_1 = self.training_triplet_paths[idx][0]\n",
        "        label_path = self.training_triplet_paths[idx][1]  # middle image acts as interpolated version of images\n",
        "        image_path_2 = self.training_triplet_paths[idx][2]\n",
        "\n",
        "        # load the 3 images from disk, swap its channels from BGR to RGB,\n",
        "        image1 = PIL.Image.open(image_path_1).convert('RGB')\n",
        "        image2 = PIL.Image.open(image_path_2).convert('RGB')\n",
        "        label = PIL.Image.open(label_path).convert('RGB')\n",
        "\n",
        "        # check to see if we are applying any transformations (eg. resize, convert to tensor etc\n",
        "        if self.transform:\n",
        "            image1, image2, label = self.transform(image1), self.transform(image2), self.transform(label)\n",
        "\n",
        "        # concat first and 3rd images to create input. Output is the middle img (label)\n",
        "        input = concat([image1, image2])\n",
        "        return input, label\n",
        "\n",
        "root = 'endonasal'\n",
        "sequences_train = ['09', '10']\n",
        "\n",
        "transform_all = transforms.Compose([\n",
        "    transforms.Resize((128, 384)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "train_dataset = endonasal(root=root, sequences=sequences_train, transform=transform_all)\n"
      ],
      "metadata": {
        "id": "FPw5ZTtYauzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import PIL.Image\n",
        "from torch.utils.data import Dataset\n",
        "import cv2\n",
        "from torch import concat\n",
        "import PIL\n",
        "#from utils import config\n",
        "from torchvision import transforms\n",
        "import glob\n",
        "import numpy as np\n",
        "\n",
        "class KITTI(Dataset):\n",
        "    def __init__(self, sequence_paths=None, transform=None):\n",
        "        self.transform = transform\n",
        "        self.training_triplet_paths = []\n",
        "        # store the image and mask filepaths, and augmentation\n",
        "        # transforms\n",
        "        for sequence_path in sequence_paths:\n",
        "            # in each sequence there is many folders. We are interested in the image_02\n",
        "            # and image_03 which are the RGB. They represent stereo images so we simply take\n",
        "            # each as a new training example.\n",
        "            for mono_folder in ['image_02', 'image_03']:\n",
        "                img_paths = sorted(glob.glob(f'{sequence_path}/{mono_folder}/data/*.png'))\n",
        "                # print(f'found {len(img_paths)} image paths for {sequence_path}/{mono_folder}')\n",
        "                for idx in range(len(img_paths)-2):\n",
        "                    image_path_1 = img_paths[idx]\n",
        "                    label_path = img_paths[idx + 1]  # middle image acts as interpolated version of images\n",
        "                    image_path_2 = img_paths[idx + 2]\n",
        "                    self.training_triplet_paths.append([image_path_1, label_path, image_path_2])\n",
        "\n",
        "    def __len__(self):\n",
        "        # return the number of total samples contained in the dataset\n",
        "        # print(f'found {len(self.training_triplet_paths)} examples')\n",
        "        return int(len(self.training_triplet_paths))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # grab the triplet of training data:\n",
        "        image_path_1 = self.training_triplet_paths[idx][0]\n",
        "        label_path = self.training_triplet_paths[idx][1]  # middle image acts as interpolated version of images\n",
        "        image_path_2 = self.training_triplet_paths[idx][2]\n",
        "\n",
        "        # load the 3 images from disk, swap its channels from BGR to RGB,\n",
        "        image1 = PIL.Image.open(image_path_1).convert('RGB')\n",
        "        image2 = PIL.Image.open(image_path_2).convert('RGB')\n",
        "        label = PIL.Image.open(label_path).convert('RGB')\n",
        "\n",
        "        # check to see if we are applying any transformations (eg. resize, convert to tensor etc\n",
        "        if self.transform:\n",
        "            image1, image2, label = self.transform(image1), self.transform(image2), self.transform(label)\n",
        "\n",
        "        # concat first and 3rd images to create input. Output is the middle img (label)\n",
        "        input = concat([image1, image2])\n",
        "        return input, label\n",
        "\n",
        "class ENDO(Dataset):\n",
        "    def __init__(self, sequence_paths=None, transform=None):\n",
        "        self.transform = transform\n",
        "        self.training_triplet_paths = []\n",
        "        # store the image and mask filepaths, and augmentation\n",
        "        # transforms\n",
        "        for sequence_path in sequence_paths:\n",
        "            #for mono_folder in ['image_02', 'image_03']:\n",
        "            img_paths = sorted(glob.glob(f'{sequence_path}/*.*'))\n",
        "            print(f'found {len(img_paths)} image paths for {sequence_path}')\n",
        "            for idx in range(len(img_paths)-300): # 2\n",
        "                image_path_1 = img_paths[idx]\n",
        "                label_path = img_paths[idx + 150] # 1 # middle image acts as interpolated version of images\n",
        "                image_path_2 = img_paths[idx + 300] # 2\n",
        "                self.training_triplet_paths.append([image_path_1, label_path, image_path_2])\n",
        "\n",
        "    def __len__(self):\n",
        "        # return the number of total samples contained in the dataset\n",
        "        # print(f'found {len(self.training_triplet_paths)} examples')\n",
        "        return int(len(self.training_triplet_paths))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # grab the triplet of training data:\n",
        "        image_path_1 = self.training_triplet_paths[idx][0]\n",
        "        label_path = self.training_triplet_paths[idx][1]  # middle image acts as interpolated version of images\n",
        "        image_path_2 = self.training_triplet_paths[idx][2]\n",
        "\n",
        "        # load the 3 images from disk, swap its channels from BGR to RGB,\n",
        "        image1 = PIL.Image.open(image_path_1).convert('RGB')\n",
        "        image2 = PIL.Image.open(image_path_2).convert('RGB')\n",
        "        label = PIL.Image.open(label_path).convert('RGB')\n",
        "\n",
        "        # check to see if we are applying any transformations (eg. resize, convert to tensor etc\n",
        "        if self.transform:\n",
        "            image1, image2, label = self.transform(image1), self.transform(image2), self.transform(label)\n",
        "\n",
        "        # concat first and 3rd images to create input. Output is the middle img (label)\n",
        "        input = concat([image1, image2])\n",
        "        return input, label\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ENDO_VIDEO(Dataset):\n",
        "    def __init__(self, video_paths=None, transform=None):\n",
        "        self.transform = transform\n",
        "        self.training_triplet_paths = []\n",
        "        # store the image and mask filepaths, and augmentation\n",
        "        # transforms\n",
        "\n",
        "        skip_factor = 1\n",
        "\n",
        "        for video_path in video_paths:\n",
        "            vid_name = video_path.split('/')[-1]\n",
        "\n",
        "            # count num frames in vid\n",
        "            video = cv2.VideoCapture(video_path)\n",
        "            num_of_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "            print(f'found {num_of_frames} frames for video {vid_name}')\n",
        "\n",
        "            \n",
        "            images_1 = np.arange(0,num_of_frames-2)\n",
        "            images_2 = np.arange(1,num_of_frames-1)\n",
        "            images_3 = np.arange(2,num_of_frames)\n",
        "            vid_path_name = [video_path]*(num_of_frames-2)\n",
        "\n",
        "            self.training_triplet_paths += list(zip(vid_path_name,images_1, images_2, images_3))\n",
        "\n",
        "        video.release()  \n",
        "\n",
        "    def __len__(self):\n",
        "        # return the number of total samples contained in the dataset\n",
        "        # print(f'found {len(self.training_triplet_paths)} examples')\n",
        "        return int(len(self.training_triplet_paths))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \n",
        "        print(f'index {idx}')\n",
        "        # grab the triplet of training data:\n",
        "        vid_path_name = self.training_triplet_paths[idx][0]\n",
        "\n",
        "        cap = cv2.VideoCapture(vid_path_name)\n",
        "\n",
        "        image_frame_1 = self.training_triplet_paths[idx][1]\n",
        "        label_frame = self.training_triplet_paths[idx][2]  # middle image acts as interpolated version of images\n",
        "        image_frame_2 = self.training_triplet_paths[idx][3]\n",
        "\n",
        "        # load the 3 images from disk, swap its channels from BGR to RGB,\n",
        "        image1 = cap.set(cv2.CAP_PROP_POS_FRAMES, image_frame_1) # .convert('RGB')\n",
        "        res, image1 = cap.read()\n",
        "        image2 = cap.set(cv2.CAP_PROP_POS_FRAMES, image_frame_2)\n",
        "        res, image2 = cap.read()\n",
        "        label = cap.set(cv2.CAP_PROP_POS_FRAMES, label_frame) # .COLOR_BGR2RGB\n",
        "        res, label = cap.read()\n",
        "\n",
        "        # convert to PIL\n",
        "        image1 = PIL.Image.fromarray(image1).convert('RGB')\n",
        "        image2 = PIL.Image.fromarray(image2).convert('RGB')\n",
        "        label = PIL.Image.fromarray(label).convert('RGB')\n",
        "\n",
        "        # check to see if we are applying any transformations (eg. resize, convert to tensor etc\n",
        "        if self.transform:\n",
        "            image1, image2, label = self.transform(image1), self.transform(image2), self.transform(label)\n",
        "\n",
        "        # concat first and 3rd images to create input. Output is the middle img (label)\n",
        "        input = concat([image1, image2])\n",
        "\n",
        "        # When everything done, release the capture\n",
        "        cap.release()\n",
        "        cv2.destroyAllWindows()\n",
        "        return input, label"
      ],
      "metadata": {
        "id": "p2-iQpCG220j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODEL"
      ],
      "metadata": {
        "id": "Dpnh1gxl23s8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Parts of the U-Net model \"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
        "        super().__init__()\n",
        "        if not mid_channels:\n",
        "            mid_channels = out_channels\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(mid_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "\n",
        "class Down(nn.Module):\n",
        "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_channels, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.maxpool_conv(x)\n",
        "\n",
        "\n",
        "class Up(nn.Module):\n",
        "    \"\"\"Upscaling then double conv\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
        "        super().__init__()\n",
        "\n",
        "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
        "        if bilinear:\n",
        "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
        "            self.conv = DoubleConv(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        # input is CHW\n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "\n",
        "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
        "                        diffY // 2, diffY - diffY // 2])\n",
        "        # if you have padding issues, see\n",
        "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
        "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class OutConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(OutConv, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)"
      ],
      "metadata": {
        "id": "BLn_tk8q3DLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\" Full assembly of the parts to form the complete network \"\"\"\n",
        "\n",
        "#from utils.unet_parts import *\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_channels, n_classes, bilinear=False):\n",
        "        super(UNet, self).__init__()\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.bilinear = bilinear\n",
        "\n",
        "        self.inc = DoubleConv(n_channels, 64) # 6 channels (\"stack inputs\")\n",
        "        self.down1 = Down(64, 128)\n",
        "        self.down2 = Down(128, 256)\n",
        "        self.down3 = Down(256, 512)\n",
        "        factor = 2 if bilinear else 1\n",
        "        self.down4 = Down(512, 1024 // factor)\n",
        "        self.up1 = Up(1024, 512 // factor, bilinear)\n",
        "        self.up2 = Up(512, 256 // factor, bilinear)\n",
        "        self.up3 = Up(256, 128 // factor, bilinear)\n",
        "        self.up4 = Up(128, 64, bilinear)\n",
        "        self.outc = OutConv(64, n_classes) # 3 classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        logits = self.outc(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "RGb53jQs2-A9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "q2yplRnj2_Ro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TRAIN"
      ],
      "metadata": {
        "id": "9TkRUGRC3Ep7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXI63Wds3nma",
        "outputId": "c17df9c5-71e0-41c0-b3e0-7f7cb77c1f2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.8/dist-packages (0.11.0)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (1.21.6)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (1.13.0+cu116)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->torchmetrics) (3.0.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from utils import config\n",
        "import glob\n",
        "import os\n",
        "from urllib import request, error\n",
        "import zipfile\n",
        "import shutil\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import nn\n",
        "from torch.optim import AdamW\n",
        "import torch\n",
        "\n",
        "#from utils import config\n",
        "#from utils.dataset import KITTI, ENDO, ENDO_VIDEO\n",
        "#from utils.model import UNet\n",
        "from torchmetrics import StructuralSimilarityIndexMeasure\n",
        "\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "class SSIM_Loss_Lib(nn.Module):\n",
        "    def __init__(self, data_range=1):\n",
        "        super().__init__()\n",
        "        self.ssim = StructuralSimilarityIndexMeasure(data_range=data_range)\n",
        "\n",
        "    def forward(self, img1, img2):\n",
        "        return 1 - self.ssim(img1, img2)\n",
        "\n",
        "\n",
        "def download_data(file):\n",
        "    print(f'downloading {file}...')\n",
        "    try:\n",
        "        URL = f'https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/{file}/{file}_sync.zip'\n",
        "        request.urlretrieve(URL, f'dataset/{file}.zip')\n",
        "        print('download complete')\n",
        "    except error.HTTPError:\n",
        "        print(f'could not find file {file}')\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "def unzip_data(file):\n",
        "    try:\n",
        "        print(f'unzipping {file}...')\n",
        "        with zipfile.ZipFile(f'dataset/{file}.zip', 'r') as zip_ref:\n",
        "            zip_ref.extractall('dataset')\n",
        "        os.remove(f'dataset/{file}.zip')\n",
        "        print('file unzipped')\n",
        "    except FileNotFoundError:\n",
        "        print(f'file {file} not found')\n",
        "\n",
        "\n",
        "def cleaning_data_dirs(new_data_dir):\n",
        "    # dirs_02 = glob.glob('dataset/*_*_*/*/image_02/data')\n",
        "    # dirs_03 = glob.glob('dataset/*_*_*/*/image_03/data')\n",
        "    # dirs = glob.glob('dataset/*_*_*/*/image_0[2-3]/data')\n",
        "\n",
        "    dirs = glob.glob('dataset/*_*_*/*')\n",
        "    for source_file in dirs:\n",
        "        shutil.move(source_file, new_data_dir)\n",
        "\n",
        "    # remove empty year dirs we've just moved all the files out of\n",
        "    main_dirs = glob.glob('dataset/*_*_*')\n",
        "    for dir in main_dirs:\n",
        "        os.rmdir(f\"{dir}\")\n",
        "    return\n",
        "\n",
        "\n",
        "def train(unet, train_loader, loss_function, optimizer):\n",
        "    # set the model in training mode\n",
        "    unet.train()\n",
        "\n",
        "    # initialize the total training loss\n",
        "    # total_train_loss = 0\n",
        "\n",
        "    ssim = StructuralSimilarityIndexMeasure(data_range=1)\n",
        "\n",
        "    train_loss = []\n",
        "    ssim_all = []\n",
        "\n",
        "    # loop over the training set\n",
        "    # we iterate over our trainLoader dataloader, which provides a batch of samples at a time\n",
        "    for (i_batch, (input_imgs, label)) in enumerate(train_loader):\n",
        "        # send the input to the device we are training our model on\n",
        "        (input_imgs, label) = (input_imgs.to(config.DEVICE), label.to(config.DEVICE))\n",
        "\n",
        "        # pass input images through unet to get prediction of interpolation\n",
        "        prediction = unet(input_imgs)\n",
        "\n",
        "        # compute loss between model prediction and ground truth label\n",
        "        # print(f'label min: {torch.min(label)}, max of label: {torch.max(label)}')\n",
        "        # print(f'label min: {torch.min(prediction)}, max of label: {torch.max(prediction)}')\n",
        "        loss = loss_function(prediction, label)\n",
        "\n",
        "        # update the parameters of model\n",
        "        optimizer.zero_grad()  # getting rid of previously accumulated gradients from previous steps\n",
        "        loss.backward()  # backpropagation\n",
        "        optimizer.step()  # update model params\n",
        "\n",
        "        # add the loss to the total training loss so far\n",
        "        #total_train_loss += loss.item()\n",
        "        train_loss.append(loss.item())\n",
        "        ssim_all.append(ssim(prediction.detach().cpu(), label.detach().cpu()).item())\n",
        "\n",
        "    return np.mean(train_loss), np.mean(ssim_all)\n",
        "\n",
        "\n",
        "def test(unet, test_loader, loss_function):\n",
        "    # set the model in evaluation mode\n",
        "    unet.eval()\n",
        "    ssim = StructuralSimilarityIndexMeasure(data_range=1)\n",
        "\n",
        "    test_loss = []\n",
        "    ssim_all = []\n",
        "\n",
        "    # switch off gradient computation (as during testing we don't want to get weights.\n",
        "    with torch.no_grad():\n",
        "        # loop over the validation set\n",
        "        for (input_imgs, label) in test_loader:\n",
        "            # send the input and label to the device\n",
        "            (test_input_imgs, label) = (input_imgs.to(config.DEVICE), label.to(config.DEVICE))\n",
        "\n",
        "            # make the predictions and calculate the validation loss\n",
        "            prediction = unet(test_input_imgs)\n",
        "            loss = loss_function(prediction, label)\n",
        "            test_loss.append(loss.item())\n",
        "            ssim_all.append(ssim(prediction.detach().cpu(), label.detach().cpu()).item())\n",
        "\n",
        "    return np.mean(test_loss), np.mean(ssim_all)\n",
        "\n",
        "\n",
        "def get_kitti_data():\n",
        "    new_data_dir = 'dataset/kitti_raw'\n",
        "    if not os.path.exists(new_data_dir):\n",
        "        print('creating dir...')\n",
        "        # make directory where data will be (inside dataset)\n",
        "        os.mkdir('dataset')\n",
        "        os.mkdir(new_data_dir)\n",
        "\n",
        "    if len(os.listdir(new_data_dir)) == 0:\n",
        "        # loop through each filename to download and unzip data\n",
        "        file_names = open('filenames.txt', 'r')\n",
        "        for file in file_names.readlines():\n",
        "            download_data(file[:-1])\n",
        "            unzip_data(file[:-1])\n",
        "            print(file)\n",
        "\n",
        "        # move all files to corresponding folder\n",
        "        cleaning_data_dirs(new_data_dir)\n",
        "\n",
        "def main():\n",
        "    print('seeding...')\n",
        "    seed_everything()\n",
        "\n",
        "    print('downloading data...')\n",
        "\n",
        "    load_weights = True\n",
        "\n",
        "    # transforms that need to be done on the data when retrieved from the disk as PIL Image\n",
        "    transform_all = transforms.Compose([\n",
        "        transforms.Resize((128, 384)),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    # loading train and test dataset\n",
        "    dataset_paths = glob.glob(f'{config.DATASET_PATH}/*_*/*')\n",
        "    \n",
        "    # splitting to test and train sequences\n",
        "    if len(dataset_paths) == 1:\n",
        "        print('only one vid found')\n",
        "        training_sequence_paths = dataset_paths\n",
        "        testing_sequence_paths = dataset_paths\n",
        "    else:\n",
        "        split_loc = int(len(dataset_paths) / 2)  # finding location where to split train and test\n",
        "        training_sequence_paths = dataset_paths[:split_loc]\n",
        "        testing_sequence_paths = dataset_paths[split_loc:]\n",
        "\n",
        "    # -------- DATA PREP\n",
        "    if config.data == 'kitti_raw':\n",
        "        # --- DATA DOWNLOAD\n",
        "        get_kitti_data()\n",
        "\n",
        "        # loading paths\n",
        "        dataset_train = KITTI(sequence_paths=training_sequence_paths, transform=transform_all)\n",
        "        dataset_test = KITTI(sequence_paths=testing_sequence_paths, transform=transform_all)\n",
        "\n",
        "    elif config.data == 'endo_data':\n",
        "        \n",
        "        dataset_train = ENDO(sequence_paths=training_sequence_paths, transform=transform_all)\n",
        "        dataset_test = ENDO(sequence_paths=testing_sequence_paths, transform=transform_all)\n",
        "        \n",
        "    elif config.data == 'endo_videos':\n",
        "        dataset_train = ENDO_VIDEO(video_paths=training_sequence_paths, transform=transform_all)\n",
        "        dataset_test = ENDO_VIDEO(video_paths=testing_sequence_paths, transform=transform_all)\n",
        "\n",
        "    print(f\"[INFO] found {len(dataset_train)} examples in the training set...\")\n",
        "    print(f\"[INFO] found {len(dataset_test)} examples in the test set...\")\n",
        "\n",
        "    # create the training and test data loaders\n",
        "\n",
        "    # We make shuffle = True since we want samples from all classes to be uniformly present in a\n",
        "    # batch which is important for optimal learning and convergence of batch\n",
        "    # gradient-based optimization approaches.\n",
        "    train_loader = DataLoader(dataset_train, shuffle=True,\n",
        "                              batch_size=config.BATCH_SIZE,\n",
        "                              pin_memory=config.PIN_MEMORY,\n",
        "                              num_workers=os.cpu_count())\n",
        "    test_loader = DataLoader(dataset_test, shuffle=False,\n",
        "                             batch_size=config.BATCH_SIZE,\n",
        "                             pin_memory=config.PIN_MEMORY,\n",
        "                             num_workers=os.cpu_count())\n",
        "\n",
        "    # initialize our UNet model:\n",
        "    # # n_channels=3 for RGB images\n",
        "    # n_classes is the number of probabilities you want to get per pixel\n",
        "    unet = UNet(n_channels=6, n_classes=3, bilinear=False).to(config.DEVICE)\n",
        "    # print(unet)\n",
        "\n",
        "    if load_weights:\n",
        "        # weights\n",
        "        checkpoint = torch.load(f'{config.BASE_OUTPUT}/unet_interpolation.pth.tar', map_location=torch.device(config.DEVICE))\n",
        "        # load weights to model\n",
        "        unet.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "    # choosing which loss to train UNET with (set this in config file)\n",
        "    if config.LOSS == 'mse':\n",
        "        criterion = nn.MSELoss()\n",
        "    elif config.LOSS == 'l1':\n",
        "        criterion = nn.L1Loss()\n",
        "    elif config.LOSS == 'bce':\n",
        "        criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    # initialize loss function and optimizer\n",
        "    loss_function = criterion.to(config.DEVICE)\n",
        "    opt = AdamW(unet.parameters(), lr=config.INIT_LR)\n",
        "\n",
        "    # ---------- TRAINING\n",
        "    # calculate steps per epoch for training and test set\n",
        "    num_steps_train = len(dataset_train) // config.BATCH_SIZE\n",
        "    num_steps_test = len(dataset_test) // config.BATCH_SIZE\n",
        "\n",
        "    # initialize a dictionary to store training history\n",
        "    train_history = {\"train_loss\": [], \"test_loss\": [], \"train_ssim\":[], 'test_ssim':[]}\n",
        "\n",
        "    print(f'[INFO] training the network...')\n",
        "\n",
        "    start_time = time.time()\n",
        "    best_epoch, best_loss = 0, np.inf\n",
        "    for epoch in tqdm(range(config.NUM_EPOCHS)):\n",
        "\n",
        "        # training unet on data\n",
        "        # total_train_loss = train(unet, train_loader, loss_function, opt)\n",
        "        avg_train_loss, avg_train_ssim = train(unet, train_loader, loss_function, opt)\n",
        "        # Once we have processed our entire training set,\n",
        "        # evaluate our model on the test set to monitor test loss and\n",
        "        # ensure that our model is not overfitting to the training set.\n",
        "        # total_test_loss = test(unet, test_loader, loss_function)\n",
        "        avg_test_loss, avg_test_ssim = test(unet, test_loader, loss_function)\n",
        "        # calculate the average training and validation loss\n",
        "        #avg_train_loss = total_train_loss / num_steps_train\n",
        "        #avg_test_loss = total_test_loss / num_steps_test\n",
        "\n",
        "        # update our training history\n",
        "        # train_history[\"train_loss\"].append(avg_train_loss.cpu().detach().numpy())\n",
        "        # train_history[\"test_loss\"].append(avg_test_loss.cpu().detach().numpy())\n",
        "        train_history[\"train_loss\"].append(avg_train_loss)\n",
        "        train_history[\"test_loss\"].append(avg_test_loss)\n",
        "\n",
        "        train_history[\"train_ssim\"].append(avg_train_ssim)\n",
        "        train_history[\"test_ssim\"].append(avg_test_ssim)\n",
        "\n",
        "        # training indo\n",
        "        print(f'[INFO] EPOCH: {epoch + 1}/{config.NUM_EPOCHS}')\n",
        "        print(\"Train loss: {:.6f}, Test loss: {:.4f}\".format(\n",
        "            avg_train_loss, avg_test_loss))\n",
        "        print(\"Train ssim: {:.6f}, Test ssim: {:.4f}\".format(\n",
        "            avg_train_ssim, avg_test_ssim))\n",
        "\n",
        "        # saving model\n",
        "        if avg_test_loss < best_loss:\n",
        "            print('saving model...')\n",
        "\n",
        "            checkpoint = {}\n",
        "\n",
        "            best_loss = avg_test_loss\n",
        "            checkpoint['best_epoch'] = epoch\n",
        "\n",
        "            checkpoint['best_train_loss'] = avg_train_loss\n",
        "            checkpoint['best_train_ssim'] = avg_train_ssim\n",
        "\n",
        "            checkpoint['best_test_loss'] = avg_test_loss\n",
        "            checkpoint['best_test_ssim'] = avg_test_ssim\n",
        "\n",
        "            checkpoint['state_dict'] = unet.state_dict()\n",
        "            # torch.save(unet.state_dict(), config.MODEL_PATH)\n",
        "            torch.save(checkpoint, config.MODEL_PATH)\n",
        "\n",
        "    # display the total time needed to perform the training\n",
        "    end_time = time.time()\n",
        "    print(\"[INFO] total time taken to train the model: {:.2f}s\".format(\n",
        "        end_time - start_time))\n",
        "\n",
        "    # plot the training loss\n",
        "    plt.style.use(\"ggplot\")\n",
        "    plt.figure()\n",
        "    plt.plot(train_history[\"train_loss\"], label=\"train_loss\")\n",
        "    plt.plot(train_history[\"test_loss\"], label=\"test_loss\")\n",
        "    plt.title(\"Training Loss on Dataset\")\n",
        "    plt.xlabel(\"Epoch #\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend(loc=\"lower left\")\n",
        "    plt.savefig(config.PLOT_PATH)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "H4FlEvlB3Lzp",
        "outputId": "68c31f22-2caa-4a81-df71-1fb3e3842e40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "seeding...\n",
            "downloading data...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-96ab61039940>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-21-96ab61039940>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'endo_videos'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         \u001b[0mdataset_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mENDO_VIDEO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_paths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_sequence_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform_all\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m         \u001b[0mdataset_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mENDO_VIDEO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_paths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtesting_sequence_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform_all\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-61c4463152f0>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, video_paths, transform)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_triplet_paths\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvid_path_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimages_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages_3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0mvideo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'video' referenced before assignment"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "BJaxTsMS5iT2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}