{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/tutorial_notebooks/blob/main/CIFAR_10H_Swine_Transformer_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vL2RMWddkaQ"
      },
      "source": [
        "# Cloning repository of CIFAR-10H Annotation\n",
        "Paper: Human uncertainty makes classification more robust (https://arxiv.org/pdf/1908.07086.pdf)\n",
        "\n",
        "Label:  CIFAR10 [0: airplane, 1: automobile, 2: bird, 3: cat, 4: deer, 5: dog, 6: frog, 7: horse, 8: ship, 9: truck] <br>\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1010/1*r8S5tF_6naagKOnlIcGXoQ.png\" alt=\"alternatetext\">\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KrQIqW6_NYl",
        "outputId": "d68db0ce-06d3-4870-c698-eaa990276379"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'cifar-10h'...\n",
            "remote: Enumerating objects: 49, done.\u001b[K\n",
            "remote: Counting objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 49 (delta 0), reused 0 (delta 0), pack-reused 48\u001b[K\n",
            "Unpacking objects: 100% (49/49), done.\n",
            "/content/cifar-10h\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/jcpeterson/cifar-10h\n",
        "%cd cifar-10h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3cVy5LVFfrZ"
      },
      "source": [
        "## Installing huggingface transformer "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIzJpadXFmUC",
        "outputId": "3d969645-5ba4-43bd-b2ee-0a50266b0522"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 3.8 MB 4.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 9.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 67 kB 3.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 895 kB 33.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 34.7 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "! pip -q install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKubHBaKHriM",
        "outputId": "ba6ce52f-f5ca-4ca1-d6b5-47b37b2ab983"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/cifar-10h\n"
          ]
        }
      ],
      "source": [
        "%cd cifar-10h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNN4TybGd2Q_"
      },
      "source": [
        "# main script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "NN371ewSCWX7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import models\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "import argparse\n",
        "import copy\n",
        "import random\n",
        "import numpy as np\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "def seed_everything(seed=12):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "parser = argparse.ArgumentParser(description='CIFAR-10H Training')\n",
        "parser.add_argument('--lr', default=3e-2, type=float, help='learning rate')\n",
        "parser.add_argument('--lr_schedule', default=0, type=int, help='lr scheduler')\n",
        "parser.add_argument('--batch_size', default=32, type=int, help='batch size')\n",
        "parser.add_argument('--test_batch_size', default=64, type=int, help='batch size')\n",
        "parser.add_argument('--num_epoch', default=10, type=int, help='epoch number')\n",
        "parser.add_argument('--num_classes', type=int, default=10, help='number classes')\n",
        "args = parser.parse_args(args=[])\n",
        "\n",
        "def train(model, trainloader, criterion, optimizer):\n",
        "    model.train()\n",
        "    for batch_idx, (inputs, targets, ad) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs).logits\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx%100 == 0:\n",
        "            print(batch_idx,'/',len(trainloader),'loss:',loss.item())\n",
        "\n",
        "def test(model, testloader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs).logits\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "    return correct / total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHKUA3yBd87G"
      },
      "source": [
        "# CIFAR-10H dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "z4uiWMiyCkjk"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from transformers import AutoFeatureExtractor, SwinForImageClassification\n",
        "#from transformers import ViTFeatureExtractor, ViTForImageClassification, BatchFeature\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.transforms import ToTensor, Normalize, Resize, Compose\n",
        "\n",
        "import torch\n",
        "\n",
        "class AutoFeatureExtractorTransforms:\n",
        "    def __init__(self, model_name_or_path):\n",
        "        feature_extractor = AutoFeatureExtractor.from_pretrained(model_name_or_path)\n",
        "        transform = []\n",
        "\n",
        "        if feature_extractor.do_resize:\n",
        "            transform.append(Resize(feature_extractor.size))\n",
        "\n",
        "        transform.append(ToTensor())\n",
        "\n",
        "        if feature_extractor.do_normalize:\n",
        "            transform.append(Normalize(feature_extractor.image_mean, feature_extractor.image_std))\n",
        "\n",
        "        self.transform = Compose(transform)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.transform(x)\n",
        "\n",
        "class CIFAR10H(torchvision.datasets.CIFAR10):\n",
        "\n",
        "    def __init__(self, root,  rand_number=0, train=False, transform=None, target_transform=None,\n",
        "                 download=False):\n",
        "        super(CIFAR10H, self).__init__(root, train, transform, target_transform, download) \n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        self.ad = np.load(os.path.join(root,'cifar10h-probs.npy'))\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        img, target = self.data[index], self.targets[index]\n",
        "        img = Image.fromarray(img)\n",
        "        ad = self.ad[index]\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        "        return img, target, ad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRQEQIdaerKa"
      },
      "source": [
        "# Run script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rpOrSuEFY2N",
        "outputId": "8f32d703-f166-4e32-950d-d8727f44b236"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "train samples: 10000 test samples: 50000\n",
            "0 / 313 loss: 2.414297580718994\n",
            "100 / 313 loss: 2.076033353805542\n",
            "200 / 313 loss: 2.1324853897094727\n",
            "300 / 313 loss: 1.9993892908096313\n",
            "epoch: 0  acc: 0.2242  best epoch: 0  best acc: 0.2242\n",
            "0 / 313 loss: 1.8297532796859741\n",
            "100 / 313 loss: 1.886186122894287\n",
            "200 / 313 loss: 1.9788832664489746\n",
            "300 / 313 loss: 1.372273564338684\n",
            "epoch: 1  acc: 0.3924  best epoch: 1  best acc: 0.3924\n",
            "0 / 313 loss: 1.5487279891967773\n",
            "100 / 313 loss: 1.8245866298675537\n",
            "200 / 313 loss: 1.564370036125183\n",
            "300 / 313 loss: 1.503949522972107\n",
            "epoch: 2  acc: 0.4190  best epoch: 2  best acc: 0.4190\n",
            "0 / 313 loss: 1.834380030632019\n",
            "100 / 313 loss: 1.3016908168792725\n",
            "200 / 313 loss: 1.2501261234283447\n",
            "300 / 313 loss: 1.158250093460083\n",
            "epoch: 3  acc: 0.6294  best epoch: 3  best acc: 0.6294\n",
            "0 / 313 loss: 1.0765539407730103\n",
            "100 / 313 loss: 1.3277177810668945\n",
            "200 / 313 loss: 1.3941457271575928\n",
            "300 / 313 loss: 0.8711541295051575\n",
            "epoch: 4  acc: 0.5685  best epoch: 3  best acc: 0.6294\n",
            "0 / 313 loss: 0.8725075721740723\n",
            "100 / 313 loss: 0.8407339453697205\n",
            "200 / 313 loss: 0.9375340938568115\n",
            "300 / 313 loss: 1.1186721324920654\n",
            "epoch: 5  acc: 0.7196  best epoch: 5  best acc: 0.7196\n",
            "0 / 313 loss: 0.6815183758735657\n",
            "100 / 313 loss: 0.5880421996116638\n",
            "200 / 313 loss: 0.7880982756614685\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def main():\n",
        "    seed_everything()\n",
        "    #2e-5,\n",
        "    model_name_or_path = 'microsoft/swin-tiny-patch4-window7-224'\n",
        "\n",
        "    train_dataset = CIFAR10H(root='./data', train=False, download=True, transform=AutoFeatureExtractorTransforms(model_name_or_path))\n",
        "    test_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=AutoFeatureExtractorTransforms(model_name_or_path))\n",
        "    print('train samples:',len(train_dataset), 'test samples:',len(test_dataset))\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=2)\n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.test_batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    model = SwinForImageClassification.from_pretrained(model_name_or_path)\n",
        "    model.classifier = nn.Linear(model.classifier.in_features, args.num_classes)\n",
        "    model = model.to(device)\n",
        "\n",
        "    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=0.9, nesterov=False, weight_decay=0.0001)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_epoch, best_acc = 0.0, 0\n",
        "    for epoch in range(args.num_epoch):\n",
        "        train(model, train_loader, criterion, optimizer)\n",
        "        accuracy = test(model, test_loader)\n",
        "        if accuracy > best_acc:\n",
        "            patience = 0\n",
        "            best_acc = accuracy\n",
        "            best_epoch = epoch\n",
        "            best_model = copy.deepcopy(model)\n",
        "            torch.save(best_model.state_dict(), 'best_model_cifar10h_vit.pth.tar')\n",
        "        print('epoch: {}  acc: {:.4f}  best epoch: {}  best acc: {:.4f}'.format(\n",
        "                epoch, accuracy, best_epoch, best_acc, optimizer.param_groups[0]['lr']))\n",
        "        \n",
        "main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Changing configuration in Swine Transformer"
      ],
      "metadata": {
        "id": "lh-_o4NVyyQL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krBAqR41UU7p"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoFeatureExtractor, SwinForImageClassification, SwinConfig\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "dataset = load_dataset(\"huggingface/cats-image\")\n",
        "image = dataset[\"test\"][\"image\"][0]\n",
        "\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\")\n",
        "#model = SwinForImageClassification.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\")\n",
        "configuration = SwinConfig()\n",
        "configuration.num_labels = 10\n",
        "model = SwinForImageClassification(configuration)\n",
        "model.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\", num_labels=100);\n",
        "\n",
        "inputs = feature_extractor(image, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    logits = model(**inputs).logits\n",
        "\n",
        "# model predicts one of the 1000 ImageNet classes\n",
        "predicted_label = logits.argmax(-1).item()\n",
        "print(model.config.id2label[predicted_label])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "CIFAR-10H_Swine_Transformer_V2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}