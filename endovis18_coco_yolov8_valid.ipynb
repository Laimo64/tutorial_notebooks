{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/tutorial_notebooks/blob/main/endovis18_coco_yolov8_valid.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSqNLlYbObia"
      },
      "source": [
        "#YoloV8:Surgical Instrument Detection\n",
        "src:https://github.com/Andrewhsin/YOLO-NAS-pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7se7AqjHgRm"
      },
      "source": [
        "#Download Endovis18 dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yqdaUNcU6uup"
      },
      "outputs": [],
      "source": [
        "import gdown\n",
        "\n",
        "#endovis18 dataset\n",
        "url = 'https://drive.google.com/uc?id=1lRNAgC-6QgIQd-vum-jr523tYPr6yNM7'\n",
        "gdown.download(url,'endovis18.zip',quiet=True) \n",
        "!unzip -q endovis18.zip "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOLPGe0g7hgN"
      },
      "source": [
        "#Converting endovis18 to COCO format and folder structure<br>\n",
        "datatset-><br>\n",
        "&emsp;    images-><br>\n",
        "&emsp; &emsp; &emsp; train->\n",
        "&emsp; &emsp;&emsp; &emsp;seq_2_frame000.png, \n",
        " seq_2_frame001.png ... <br>\n",
        "&emsp; &emsp; &emsp; val->\n",
        "&emsp; &emsp;&emsp; &emsp;seq_1_frame000.png, \n",
        " seq_1_frame001.png ... <br>\n",
        "&emsp;    labels-><br>\n",
        "&emsp; &emsp; &emsp; train->\n",
        "&emsp; &emsp;&emsp; &emsp;seq_2_frame000.txt, \n",
        " seq_2_frame001.txt ... <br>\n",
        "&emsp; &emsp; &emsp; val->\n",
        "&emsp; &emsp;&emsp; &emsp;seq_1_frame000.txt, \n",
        " seq_1_frame001.txt ... <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5732ZuN90SR"
      },
      "source": [
        "1. Converting .xml of [(x1, y1), (x2, y2)] to .txt (xc, yc, h, w)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uo8N-Z9q7hCE",
        "outputId": "a2b798ec-3baf-4527-f92e-777089667383"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2007/2007 [00:00<00:00, 3809.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "import os\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "\n",
        "root_dir = \"endovis18/\"\n",
        "dest_dir = root_dir\n",
        "\n",
        "class_name_to_id_mapping = {\n",
        "    \"kidney\": 0,\n",
        "    \"bipolar_forceps\": 1,\n",
        "    \"prograsp_forceps\": 2,\n",
        "    \"large_needle_driver\": 3,\n",
        "    \"monopolar_curved_scissors\": 4,\n",
        "    \"ultrasound_probe\": 5,\n",
        "    \"suction\": 6,\n",
        "    \"clip_applier\": 7,\n",
        "    \"stapler\": 8,\n",
        "}\n",
        "\n",
        "\n",
        "# Function to get the data from XML Annotation\n",
        "def extract_info_from_xml(xml_file):\n",
        "    root = ET.parse(xml_file).getroot()\n",
        "\n",
        "    # Initialise the info dict\n",
        "    info_dict = {}\n",
        "    info_dict[\"bboxes\"] = []\n",
        "\n",
        "    # Parse the XML Tree\n",
        "    for elem in root:\n",
        "        # Get the file name\n",
        "        if elem.tag == \"filename\":\n",
        "            info_dict[\"filename\"] = elem.text\n",
        "\n",
        "        # Get the image size\n",
        "        elif elem.tag == \"size\":\n",
        "            image_size = []\n",
        "            for subelem in elem:\n",
        "                image_size.append(int(subelem.text))\n",
        "\n",
        "            info_dict[\"image_size\"] = tuple(image_size)\n",
        "\n",
        "        # Get details of the bounding box\n",
        "        elif elem.tag == \"objects\":\n",
        "            bbox = {}\n",
        "            for subelem in elem:\n",
        "                if subelem.tag == \"name\":\n",
        "                    bbox[\"class\"] = subelem.text\n",
        "\n",
        "                elif subelem.tag == \"bndbox\":\n",
        "                    for subsubelem in subelem:\n",
        "                        bbox[subsubelem.tag] = int(subsubelem.text)\n",
        "            info_dict[\"bboxes\"].append(bbox)\n",
        "\n",
        "    info_dict[\"image_size\"] = tuple([1280, 1024, 3])\n",
        "\n",
        "    return info_dict\n",
        "\n",
        "\n",
        "# print(extract_info_from_xml('dataset/instruments18/seq_1/xml/frame000.xml'))\n",
        "\n",
        "\n",
        "def convert_to_yolov5(info_dict, ann):\n",
        "    print_buffer = []\n",
        "\n",
        "    # For each bounding box\n",
        "    for b in info_dict[\"bboxes\"]:\n",
        "        try:\n",
        "            class_id = class_name_to_id_mapping[b[\"class\"]]\n",
        "        except KeyError:\n",
        "            print(\"Invalid Class. Must be one from \", class_name_to_id_mapping.keys())\n",
        "\n",
        "        # Transform the bbox co-ordinates as per the format required by YOLO v5\n",
        "        b_center_x = (b[\"xmin\"] + b[\"xmax\"]) / 2\n",
        "        b_center_y = (b[\"ymin\"] + b[\"ymax\"]) / 2\n",
        "        b_width = b[\"xmax\"] - b[\"xmin\"]\n",
        "        b_height = b[\"ymax\"] - b[\"ymin\"]\n",
        "\n",
        "        # Normalise the co-ordinates by the dimensions of the image\n",
        "        image_w, image_h, image_c = info_dict[\"image_size\"]\n",
        "        b_center_x /= image_w\n",
        "        b_center_y /= image_h\n",
        "        b_width /= image_w\n",
        "        b_height /= image_h\n",
        "\n",
        "        # Write the bbox details to the file\n",
        "        print_buffer.append(\n",
        "            \"{} {:.3f} {:.3f} {:.3f} {:.3f}\".format(\n",
        "                class_id, b_center_x, b_center_y, b_width, b_height\n",
        "            )\n",
        "        )\n",
        "\n",
        "    # Name of the file which we have to save\n",
        "    save_file_name = os.path.splitext(ann)[0] + \".txt\"\n",
        "\n",
        "    # Save the annotation to disk\n",
        "    print(\"\\n\".join(print_buffer), file=open(save_file_name, \"w\"))\n",
        "\n",
        "\n",
        "# Get the annotations\n",
        "annotations = glob(root_dir + \"*/xml/*.xml\")\n",
        "\n",
        "# # Convert and save the annotations\n",
        "for ann in tqdm(annotations):\n",
        "    info_dict = extract_info_from_xml(ann)\n",
        "    convert_to_yolov5(info_dict, ann)\n",
        "\n",
        "annotations = glob(root_dir + \"*/xml/*.txt\")\n",
        "print('done!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_j1kYAQ-hIr"
      },
      "source": [
        "2. Rearranging folders as COCO dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0CXh5jlh-k62"
      },
      "outputs": [],
      "source": [
        "from glob import glob\n",
        "import shutil\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "root_dir = \"endovis18/\"\n",
        "path = glob(root_dir + \"*/xml/*.txt\")\n",
        "endovis_coco_img_path_train = 'endovis18_coco/images/train'\n",
        "endovis_coco_img_path_val = 'endovis18_coco/images/val'\n",
        "endovis_coco_label_path_train = 'endovis18_coco/labels/train'\n",
        "endovis_coco_label_path_val = 'endovis18_coco/labels/val'\n",
        "\n",
        "os.makedirs(endovis_coco_img_path_train, exist_ok=True)\n",
        "os.makedirs(endovis_coco_img_path_val, exist_ok=True)\n",
        "os.makedirs(endovis_coco_label_path_train, exist_ok=True)\n",
        "os.makedirs(endovis_coco_label_path_val, exist_ok=True)\n",
        "\n",
        "#validation set with the seq 1, 5, 16 following: https://ieeexplore.ieee.org/abstract/document/9944843\n",
        "val_seq = [1, 5, 16]\n",
        "path_all = []\n",
        "for seq in val_seq:\n",
        "    path_all.extend(glob(root_dir + \"seq_{}/xml/*.txt\".format(seq)))\n",
        "\n",
        "for path in path_all:\n",
        "    endovis_coco_label_path_val_full = os.path.join(endovis_coco_label_path_val, path.split('/')[1] + '_' + os.path.basename(path))\n",
        "    shutil.copyfile(path, endovis_coco_label_path_val_full)\n",
        "    img_path = os.path.join(path.split('/')[0], path.split('/')[1],'left_frames', os.path.basename(path[:-3])+'png')\n",
        "    endovis_coco_img_path_val_full = os.path.join(endovis_coco_img_path_val, img_path.split('/')[1] + '_' + os.path.basename(img_path))\n",
        "    shutil.copyfile(img_path, endovis_coco_img_path_val_full)\n",
        "    \n",
        "\n",
        "# Training set with the remaining seq following: https://ieeexplore.ieee.org/abstract/document/9944843\n",
        "train_seq = [2, 3, 4, 6, 7, 9, 10, 11, 12, 14, 15]\n",
        "path_all = []\n",
        "for seq in train_seq:\n",
        "    path_all.extend(glob(root_dir + \"seq_{}/xml/*.txt\".format(seq)))\n",
        "\n",
        "for path in path_all:\n",
        "    endovis_coco_label_path_train_full = os.path.join(endovis_coco_label_path_train, path.split('/')[1] + '_' + os.path.basename(path))\n",
        "    shutil.copyfile(path, endovis_coco_label_path_train_full)\n",
        "    img_path = os.path.join(path.split('/')[0], path.split('/')[1],'left_frames', os.path.basename(path[:-3])+'png')\n",
        "    endovis_coco_img_path_train_full = os.path.join(endovis_coco_img_path_train, img_path.split('/')[1] + '_' + os.path.basename(img_path))\n",
        "    shutil.copyfile(img_path, endovis_coco_img_path_train_full)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86tya0cOHoAG"
      },
      "source": [
        "#Download Code and Trained Weights\n",
        "\n",
        "installation \"super_gradients\" lib\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install super-gradients"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUjF283vyFfs",
        "outputId": "c99f8a1a-0ebc-44bf-e38a-ab3de88cf55f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m684.5/684.5 kB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m101.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m96.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m408.6/408.6 kB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m108.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m91.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.0/68.0 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m94.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m95.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m113.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for termcolor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for treelib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for coverage (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for stringcase (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unfortunately, need to edit super_gradients/training/utils/utils.py.<br>\n",
        "To edit the file please pressed cmd (in mac) or ctrl in windows + click on this file dir"
      ],
      "metadata": {
        "id": "wn9aj7rE0ltM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1. To edit the file please pressed cmd (in mac) or ctrl in windows + click on this file dir. To get python version you need to run the code or check the installed directory for super_gradients\n",
        "/usr/local/lib/python3.10/dist-packages/super_gradients/training/utils/utils.py\n",
        "\n",
        "# Go to line 595:\n",
        "#current: if isinstance(inputs, collections.Iterable) and not isinstance(inputs, str):\n",
        "#modify to: if isinstance(inputs, collections.abc.Iterable) and not isinstance(inputs, str):"
      ],
      "metadata": {
        "id": "iZVitaPL0vS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the code:"
      ],
      "metadata": {
        "id": "MiDS5WmK1WUh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Andrewhsin/YOLO-NAS-pytorch\n",
        "%cd YOLO-NAS-pytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xoaANmfqvuqi",
        "outputId": "4cedb6ff-4b31-4d3c-c8f2-fb1a4324fe1f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'YOLO-NAS-pytorch'...\n",
            "remote: Enumerating objects: 1636, done.\u001b[K\n",
            "remote: Counting objects: 100% (77/77), done.\u001b[K\n",
            "remote: Compressing objects: 100% (73/73), done.\u001b[K\n",
            "remote: Total 1636 (delta 16), reused 0 (delta 0), pack-reused 1559\u001b[K\n",
            "Receiving objects: 100% (1636/1636), 80.76 MiB | 15.29 MiB/s, done.\n",
            "Resolving deltas: 100% (367/367), done.\n",
            "Updating files: 100% (1350/1350), done.\n",
            "/content/YOLO-NAS-pytorch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdIkGOxQHF_n"
      },
      "source": [
        "#Training\n",
        "Validation on trained weights following: https://ieeexplore.ieee.org/abstract/document/9944843"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7DkpMavJNDj"
      },
      "source": [
        "1. Creating yml file to set all dataset dirs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HlumeQgpJSwr"
      },
      "outputs": [],
      "source": [
        "import yaml\n",
        "\n",
        "with open(\"dataset/data.yaml\") as f:\n",
        "     list_doc = yaml.safe_load(f)\n",
        "\n",
        "list_doc['Dir'] = '/content/endovis18_coco/'\n",
        "list_doc['images']['train'] = 'images/train'\n",
        "list_doc['images']['val'] = 'images/val'\n",
        "list_doc['images']['test'] = 'images/val'\n",
        "\n",
        "list_doc['labels']['train'] = 'labels/train'\n",
        "list_doc['labels']['val'] = 'labels/val'\n",
        "list_doc['labels']['test'] = 'labels/val'\n",
        "\n",
        "with open(\"dataset/data.yaml\", \"w\") as f:\n",
        "    yaml.dump(list_doc, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pO84Cj0zJ_8d"
      },
      "source": [
        "2. Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 train.py --data dataset/data.yaml --batch 6 --epoch 100 --model yolo_nas_m"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wx9uiU1x2SD",
        "outputId": "4f365af5-0ab9-44e7-8e8f-f760e2b3a53a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The console stream is logged into /root/sg_logs/console.log\n",
            "[2023-06-10 14:59:22] INFO - crash_tips_setup.py - Crash tips is enabled. You can set your environment variable to CRASH_HANDLER=FALSE to disable it\n",
            "/usr/local/lib/python3.10/dist-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
            "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
            "2023-06-10 14:59:28.526731: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-06-10 14:59:32.771467: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[2023-06-10 14:59:36] INFO - utils.py - NumExpr defaulting to 2 threads.\n",
            "[2023-06-10 14:59:37] WARNING - __init__.py - Failed to import pytorch_quantization\n",
            "[2023-06-10 14:59:37] WARNING - calibrator.py - Failed to import pytorch_quantization\n",
            "[2023-06-10 14:59:37] WARNING - export.py - Failed to import pytorch_quantization\n",
            "[2023-06-10 14:59:37] WARNING - selective_quantization_utils.py - Failed to import pytorch_quantization\n",
            "[INFO] Checkpoints saved in runs/train2\n",
            "Caching annotations: 100% 1560/1560 [00:00<00:00, 6974.37it/s]\n",
            "Caching annotations: 100% 447/447 [00:00<00:00, 7315.05it/s]\n",
            "Caching annotations: 100% 447/447 [00:00<00:00, 8163.86it/s]\n",
            "[2023-06-10 14:59:38] INFO - checkpoint_utils.py - License Notification: YOLO-NAS pre-trained weights are subjected to the specific license terms and conditions detailed in \n",
            "https://github.com/Deci-AI/super-gradients/blob/master/LICENSE.YOLONAS.md\n",
            "By downloading the pre-trained weight files you agree to comply with these terms.\n",
            "Downloading: \"https://sghub.deci.ai/models/yolo_nas_m_coco.pth\" to /root/.cache/torch/hub/checkpoints/yolo_nas_m_coco.pth\n",
            "100% 196M/196M [00:12<00:00, 16.7MB/s]\n",
            "The console stream is now moved to runs/train2/console_Jun10_14_59_59.txt\n",
            "[2023-06-10 14:59:59] INFO - sg_trainer.py - Using EMA with params {'decay': 0.9, 'decay_type': 'threshold'}\n",
            "[2023-06-10 15:00:03] INFO - sg_trainer_utils.py - TRAINING PARAMETERS:\n",
            "    - Mode:                         Single GPU\n",
            "    - Number of GPUs:               1          (1 available on the machine)\n",
            "    - Dataset size:                 1560       (len(train_set))\n",
            "    - Batch size per GPU:           6          (batch_size)\n",
            "    - Batch Accumulate:             1          (batch_accumulate)\n",
            "    - Total batch size:             6          (num_gpus * batch_size)\n",
            "    - Effective Batch size:         6          (num_gpus * batch_size * batch_accumulate)\n",
            "    - Iterations per epoch:         260        (len(train_loader))\n",
            "    - Gradient updates per epoch:   260        (len(train_loader) / batch_accumulate)\n",
            "\n",
            "[2023-06-10 15:00:03] INFO - sg_trainer.py - Started training for 100 epochs (0/99)\n",
            "\n",
            "Train epoch 0: 100% 260/260 [04:50<00:00,  1.12s/it, PPYoloELoss/loss=3.69, PPYoloELoss/loss_cls=2.18, PPYoloELoss/loss_dfl=1.65, PPYoloELoss/loss_iou=0.272, gpu_mem=4.42]\n",
            "Validation epoch 0: 100% 75/75 [00:24<00:00,  3.01it/s]\n",
            "===========================================================\n",
            "SUMMARY OF EPOCH 0\n",
            "├── Training\n",
            "│   ├── Ppyoloeloss/loss = 3.6856\n",
            "│   ├── Ppyoloeloss/loss_cls = 2.1821\n",
            "│   ├── Ppyoloeloss/loss_dfl = 1.6486\n",
            "│   └── Ppyoloeloss/loss_iou = 0.2717\n",
            "└── Validation\n",
            "    ├── F1@0.50 = 0.0016\n",
            "    ├── Map@0.50 = 0.0012\n",
            "    ├── Ppyoloeloss/loss = 3.4652\n",
            "    ├── Ppyoloeloss/loss_cls = 2.3682\n",
            "    ├── Ppyoloeloss/loss_dfl = 1.3163\n",
            "    ├── Ppyoloeloss/loss_iou = 0.1755\n",
            "    ├── Precision@0.50 = 0.0026\n",
            "    └── Recall@0.50 = 0.0037\n",
            "\n",
            "===========================================================\n",
            "[2023-06-10 15:05:28] INFO - base_sg_logger.py - Checkpoint saved in runs/train2/ckpt_best.pth\n",
            "[2023-06-10 15:05:28] INFO - sg_trainer.py - Best checkpoint overriden: validation mAP@0.50: 0.001248706947080791\n",
            "Train epoch 1:  19% 50/260 [00:54<03:22,  1.04it/s, PPYoloELoss/loss=2.98, PPYoloELoss/loss_cls=1.6, PPYoloELoss/loss_dfl=1.56, PPYoloELoss/loss_iou=0.242, gpu_mem=4.5]"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}