{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMHocAPP6dI5UYwCOvMs+X7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/tutorial_notebooks/blob/main/endovis18_coco_yolov7_valid.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Surgical Instrument Detection\n",
        "src:https://github.com/mobarakol/tool_detection.git"
      ],
      "metadata": {
        "id": "NSqNLlYbObia"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Download Endovis18 dataset"
      ],
      "metadata": {
        "id": "Q7se7AqjHgRm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yqdaUNcU6uup"
      },
      "outputs": [],
      "source": [
        "import gdown\n",
        "\n",
        "#endovis18 dataset\n",
        "url = 'https://drive.google.com/uc?id=1lRNAgC-6QgIQd-vum-jr523tYPr6yNM7'\n",
        "gdown.download(url,'endovis18.zip',quiet=True) \n",
        "!unzip -q endovis18.zip \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Converting endovis18 to COCO format and folder structure<br>\n",
        "datatset-><br>\n",
        "&emsp;    images-><br>\n",
        "&emsp; &emsp; &emsp; train->\n",
        "&emsp; &emsp;&emsp; &emsp;seq_2_frame000.png, \n",
        " seq_2_frame001.png ... <br>\n",
        "&emsp; &emsp; &emsp; val->\n",
        "&emsp; &emsp;&emsp; &emsp;seq_1_frame000.png, \n",
        " seq_1_frame001.png ... <br>\n",
        "&emsp;    labels-><br>\n",
        "&emsp; &emsp; &emsp; train->\n",
        "&emsp; &emsp;&emsp; &emsp;seq_2_frame000.txt, \n",
        " seq_2_frame001.txt ... <br>\n",
        "&emsp; &emsp; &emsp; val->\n",
        "&emsp; &emsp;&emsp; &emsp;seq_1_frame000.txt, \n",
        " seq_1_frame001.txt ... <br>"
      ],
      "metadata": {
        "id": "kOLPGe0g7hgN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Converting .xml of [(x1, y1), (x2, y2)] to .txt (xc, yc, h, w)"
      ],
      "metadata": {
        "id": "n5732ZuN90SR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "import os\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "\n",
        "root_dir = \"endovis18/\"\n",
        "dest_dir = root_dir\n",
        "\n",
        "class_name_to_id_mapping = {\n",
        "    \"kidney\": 0,\n",
        "    \"bipolar_forceps\": 1,\n",
        "    \"prograsp_forceps\": 2,\n",
        "    \"large_needle_driver\": 3,\n",
        "    \"monopolar_curved_scissors\": 4,\n",
        "    \"ultrasound_probe\": 5,\n",
        "    \"suction\": 6,\n",
        "    \"clip_applier\": 7,\n",
        "    \"stapler\": 8,\n",
        "}\n",
        "\n",
        "\n",
        "# Function to get the data from XML Annotation\n",
        "def extract_info_from_xml(xml_file):\n",
        "    root = ET.parse(xml_file).getroot()\n",
        "\n",
        "    # Initialise the info dict\n",
        "    info_dict = {}\n",
        "    info_dict[\"bboxes\"] = []\n",
        "\n",
        "    # Parse the XML Tree\n",
        "    for elem in root:\n",
        "        # Get the file name\n",
        "        if elem.tag == \"filename\":\n",
        "            info_dict[\"filename\"] = elem.text\n",
        "\n",
        "        # Get the image size\n",
        "        elif elem.tag == \"size\":\n",
        "            image_size = []\n",
        "            for subelem in elem:\n",
        "                image_size.append(int(subelem.text))\n",
        "\n",
        "            info_dict[\"image_size\"] = tuple(image_size)\n",
        "\n",
        "        # Get details of the bounding box\n",
        "        elif elem.tag == \"objects\":\n",
        "            bbox = {}\n",
        "            for subelem in elem:\n",
        "                if subelem.tag == \"name\":\n",
        "                    bbox[\"class\"] = subelem.text\n",
        "\n",
        "                elif subelem.tag == \"bndbox\":\n",
        "                    for subsubelem in subelem:\n",
        "                        bbox[subsubelem.tag] = int(subsubelem.text)\n",
        "            info_dict[\"bboxes\"].append(bbox)\n",
        "\n",
        "    info_dict[\"image_size\"] = tuple([1280, 1024, 3])\n",
        "\n",
        "    return info_dict\n",
        "\n",
        "\n",
        "# print(extract_info_from_xml('dataset/instruments18/seq_1/xml/frame000.xml'))\n",
        "\n",
        "\n",
        "def convert_to_yolov5(info_dict, ann):\n",
        "    print_buffer = []\n",
        "\n",
        "    # For each bounding box\n",
        "    for b in info_dict[\"bboxes\"]:\n",
        "        try:\n",
        "            class_id = class_name_to_id_mapping[b[\"class\"]]\n",
        "        except KeyError:\n",
        "            print(\"Invalid Class. Must be one from \", class_name_to_id_mapping.keys())\n",
        "\n",
        "        # Transform the bbox co-ordinates as per the format required by YOLO v5\n",
        "        b_center_x = (b[\"xmin\"] + b[\"xmax\"]) / 2\n",
        "        b_center_y = (b[\"ymin\"] + b[\"ymax\"]) / 2\n",
        "        b_width = b[\"xmax\"] - b[\"xmin\"]\n",
        "        b_height = b[\"ymax\"] - b[\"ymin\"]\n",
        "\n",
        "        # Normalise the co-ordinates by the dimensions of the image\n",
        "        image_w, image_h, image_c = info_dict[\"image_size\"]\n",
        "        b_center_x /= image_w\n",
        "        b_center_y /= image_h\n",
        "        b_width /= image_w\n",
        "        b_height /= image_h\n",
        "\n",
        "        # Write the bbox details to the file\n",
        "        print_buffer.append(\n",
        "            \"{} {:.3f} {:.3f} {:.3f} {:.3f}\".format(\n",
        "                class_id, b_center_x, b_center_y, b_width, b_height\n",
        "            )\n",
        "        )\n",
        "\n",
        "    # Name of the file which we have to save\n",
        "    save_file_name = os.path.splitext(ann)[0] + \".txt\"\n",
        "\n",
        "    # Save the annotation to disk\n",
        "    print(\"\\n\".join(print_buffer), file=open(save_file_name, \"w\"))\n",
        "\n",
        "\n",
        "# Get the annotations\n",
        "annotations = glob(root_dir + \"*/xml/*.xml\")\n",
        "\n",
        "# # Convert and save the annotations\n",
        "for ann in tqdm(annotations):\n",
        "    info_dict = extract_info_from_xml(ann)\n",
        "    convert_to_yolov5(info_dict, ann)\n",
        "\n",
        "annotations = glob(root_dir + \"*/xml/*.txt\")\n",
        "print('done!')"
      ],
      "metadata": {
        "id": "Uo8N-Z9q7hCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Rearranging folders as COCO dataset"
      ],
      "metadata": {
        "id": "o_j1kYAQ-hIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from glob import glob\n",
        "import shutil\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "root_dir = \"endovis18/\"\n",
        "path = glob(root_dir + \"*/xml/*.txt\")\n",
        "endovis_coco_img_path_train = 'endovis18_coco/images/train'\n",
        "endovis_coco_img_path_val = 'endovis18_coco/images/val'\n",
        "endovis_coco_label_path_train = 'endovis18_coco/labels/train'\n",
        "endovis_coco_label_path_val = 'endovis18_coco/labels/val'\n",
        "\n",
        "os.makedirs(endovis_coco_img_path_train, exist_ok=True)\n",
        "os.makedirs(endovis_coco_img_path_val, exist_ok=True)\n",
        "os.makedirs(endovis_coco_label_path_train, exist_ok=True)\n",
        "os.makedirs(endovis_coco_label_path_val, exist_ok=True)\n",
        "\n",
        "#validation set with the seq 1, 5, 16 following: https://ieeexplore.ieee.org/abstract/document/9944843\n",
        "val_seq = [1, 5, 16]\n",
        "path_all = []\n",
        "for seq in val_seq:\n",
        "    path_all.extend(glob(root_dir + \"seq_{}/xml/*.txt\".format(seq)))\n",
        "\n",
        "for path in path_all:\n",
        "    endovis_coco_label_path_val_full = os.path.join(endovis_coco_label_path_val, path.split('/')[1] + '_' + os.path.basename(path))\n",
        "    shutil.copyfile(path, endovis_coco_label_path_val_full)\n",
        "    img_path = os.path.join(path.split('/')[0], path.split('/')[1],'left_frames', os.path.basename(path[:-3])+'png')\n",
        "    endovis_coco_img_path_val_full = os.path.join(endovis_coco_img_path_val, img_path.split('/')[1] + '_' + os.path.basename(img_path))\n",
        "    shutil.copyfile(img_path, endovis_coco_img_path_val_full)\n",
        "    \n",
        "\n",
        "#Training set with the remaining seq following: https://ieeexplore.ieee.org/abstract/document/9944843\n",
        "# train_seq = [2, 3, 4, 6, 7, 9, 10, 11, 12, 14, 15]\n",
        "# path_all = []\n",
        "# for seq in train_seq:\n",
        "#     path_all.extend(glob(root_dir + \"seq_{}/xml/*.txt\".format(seq)))\n",
        "\n",
        "# for path in path_all:\n",
        "#     endovis_coco_label_path_train_full = os.path.join(endovis_coco_label_path_train, path.split('/')[1] + '_' + os.path.basename(path))\n",
        "#     shutil.copyfile(path, endovis_coco_label_path_train_full)\n",
        "#     img_path = os.path.join(path.split('/')[0], path.split('/')[1],'left_frames', os.path.basename(path[:-3])+'png')\n",
        "#     endovis_coco_img_path_train_full = os.path.join(endovis_coco_img_path_train, img_path.split('/')[1] + '_' + os.path.basename(img_path))\n",
        "#     shutil.copyfile(img_path, endovis_coco_img_path_train_full)"
      ],
      "metadata": {
        "id": "0CXh5jlh-k62"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Download Code and Trained Weights\n"
      ],
      "metadata": {
        "id": "86tya0cOHoAG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "\n",
        "!git clone https://github.com/mobarakol/tool_detection.git\n",
        "%cd tool_detection/yolov7\n",
        "\n",
        "#weights\n",
        "url = 'https://drive.google.com/uc?id=1G4bLXEUErGrMR5bNrZ8JCjX-hLttRq3h'\n",
        "gdown.download(url,'weights.zip',quiet=True) \n",
        "!unzip -q weights.zip "
      ],
      "metadata": {
        "id": "6KX4w8dLHwDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Validation\n",
        "Validation on trained weights following: https://ieeexplore.ieee.org/abstract/document/9944843"
      ],
      "metadata": {
        "id": "ZdIkGOxQHF_n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Creating yml file for dataset dir:"
      ],
      "metadata": {
        "id": "P7DkpMavJNDj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "\n",
        "with open(\"data/endovis18.yaml\") as f:\n",
        "     list_doc = yaml.safe_load(f)\n",
        "\n",
        "list_doc['train'] = '/content/endovis18_coco/images/train/'\n",
        "list_doc['val'] = '/content/endovis18_coco/images/val/'\n",
        "list_doc['test'] = '/content/endovis18_coco/images/val/'\n",
        "\n",
        "with open(\"data/endovis18.yaml\", \"w\") as f:\n",
        "    yaml.dump(list_doc, f)"
      ],
      "metadata": {
        "id": "HlumeQgpJSwr"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Inialization"
      ],
      "metadata": {
        "id": "pO84Cj0zJ_8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "from threading import Thread\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import yaml\n",
        "from tqdm import tqdm\n",
        "\n",
        "from models.experimental import attempt_load\n",
        "from utils.datasets import create_dataloader\n",
        "from utils.general import coco80_to_coco91_class, check_dataset, check_file, check_img_size, check_requirements, \\\n",
        "    box_iou, non_max_suppression, scale_coords, xyxy2xywh, xywh2xyxy, set_logging, increment_path, colorstr\n",
        "from utils.metrics import ap_per_class, ConfusionMatrix\n",
        "from utils.plots import plot_images, output_to_target, plot_study_txt\n",
        "from utils.torch_utils import select_device, time_synchronized, TracedModel\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser(prog='test.py')\n",
        "    parser.add_argument('--weights', nargs='+', type=str, default='yolov7.pt', help='model.pt path(s)')\n",
        "    parser.add_argument('--data', type=str, default='data/coco.yaml', help='*.data path')\n",
        "    parser.add_argument('--batch-size', type=int, default=32, help='size of each image batch')\n",
        "    parser.add_argument('--img-size', type=int, default=640, help='inference size (pixels)')\n",
        "    parser.add_argument('--conf-thres', type=float, default=0.001, help='object confidence threshold')\n",
        "    parser.add_argument('--iou-thres', type=float, default=0.65, help='IOU threshold for NMS')\n",
        "    parser.add_argument('--task', default='val', help='train, val, test, speed or study')\n",
        "    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n",
        "    parser.add_argument('--single-cls', action='store_true', help='treat as single-class dataset')\n",
        "    parser.add_argument('--augment', action='store_true', help='augmented inference')\n",
        "    parser.add_argument('--verbose', action='store_true', help='report mAP by class')\n",
        "    parser.add_argument('--save-txt', action='store_true', help='save results to *.txt')\n",
        "    parser.add_argument('--save-hybrid', action='store_true', help='save label+prediction hybrid results to *.txt')\n",
        "    parser.add_argument('--save-conf', action='store_true', help='save confidences in --save-txt labels')\n",
        "    parser.add_argument('--save-json', action='store_true', help='save a cocoapi-compatible JSON results file')\n",
        "    parser.add_argument('--project', default='runs/test', help='save to project/name')\n",
        "    parser.add_argument('--name', default='exp', help='save to project/name')\n",
        "    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')\n",
        "    parser.add_argument('--no-trace', action='store_true', help='don`t trace model')\n",
        "    opt = parser.parse_args([])\n",
        "    opt.save_json |= opt.data.endswith('coco.yaml')\n",
        "    opt.data = check_file(opt.data)  # check file\n",
        "    \n",
        "    opt.data = 'data/endovis18.yaml' \n",
        "    opt.img_size = 320 \n",
        "    opt.batch_size = 32 \n",
        "    opt.conf_thres = 0.001 \n",
        "    opt.iou_thres = 0.5 \n",
        "    opt.device = '0' \n",
        "    opt.weights = 'weights/best.pt' \n",
        "    opt.name = 'yolov7_endo18_320_val'\n",
        "    print(opt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDB6t6hKEZIx",
        "outputId": "83827b84-df89-4607-ef49-c2375ad2fe5c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(weights='weights/best.pt', data='data/endovis18.yaml', batch_size=32, img_size=320, conf_thres=0.001, iou_thres=0.5, task='val', device='0', single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=True, project='runs/test', name='yolov7_endo18_320_val', exist_ok=False, no_trace=False)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Validation"
      ],
      "metadata": {
        "id": "O0mE4GRnKF4P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(data,\n",
        "         weights=None,\n",
        "         batch_size=32,\n",
        "         imgsz=640,\n",
        "         conf_thres=0.001,\n",
        "         iou_thres=0.6,  # for NMS\n",
        "         save_json=False,\n",
        "         single_cls=False,\n",
        "         augment=False,\n",
        "         verbose=False,\n",
        "         model=None,\n",
        "         dataloader=None,\n",
        "         save_dir=Path(''),  # for saving images\n",
        "         save_txt=False,  # for auto-labelling\n",
        "         save_hybrid=False,  # for hybrid auto-labelling\n",
        "         save_conf=False,  # save auto-label confidences\n",
        "         plots=True,\n",
        "         wandb_logger=None,\n",
        "         compute_loss=None,\n",
        "         half_precision=True,\n",
        "         trace=False,\n",
        "         is_coco=False):\n",
        "    # Initialize/load model and set device\n",
        "    training = model is not None\n",
        "    if training:  # called by train.py\n",
        "        device = next(model.parameters()).device  # get model device\n",
        "\n",
        "    else:  # called directly\n",
        "        set_logging()\n",
        "        device = select_device(opt.device, batch_size=batch_size)\n",
        "        # device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        # Directories\n",
        "        save_dir = Path(increment_path(Path(opt.project) / opt.name, exist_ok=opt.exist_ok))  # increment run\n",
        "        (save_dir / 'labels' if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir\n",
        "\n",
        "        # Load model\n",
        "        model = attempt_load(weights, map_location=device)  # load FP32 model\n",
        "        gs = max(int(model.stride.max()), 32)  # grid size (max stride)\n",
        "        imgsz = check_img_size(imgsz, s=gs)  # check img_size\n",
        "\n",
        "    # Half\n",
        "    half = device.type != 'cpu' and half_precision  # half precision only supported on CUDA\n",
        "    if half:\n",
        "        model.half()\n",
        "\n",
        "    # Configure\n",
        "    model.eval()\n",
        "    if isinstance(data, str):\n",
        "        with open(data) as f:\n",
        "            data = yaml.load(f, Loader=yaml.SafeLoader)\n",
        "    check_dataset(data)  # check\n",
        "    nc = 1 if single_cls else int(data['nc'])  # number of classes\n",
        "    iouv = torch.linspace(0.5, 0.95, 10).to(device)  # iou vector for mAP@0.5:0.95\n",
        "    niou = iouv.numel()\n",
        "\n",
        "    # Dataloader\n",
        "    if not training:\n",
        "        if device.type != 'cpu':\n",
        "            model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))  # run once\n",
        "        task = opt.task if opt.task in ('train', 'val', 'test') else 'val'  # path to train/val/test images\n",
        "        dataloader = create_dataloader(data[task], imgsz, batch_size, gs, opt, pad=0.5, rect=True,\n",
        "                                       prefix=colorstr(f'{task}: '))[0]\n",
        "    seen = 0\n",
        "    confusion_matrix = ConfusionMatrix(nc=nc)\n",
        "    names = {k: v for k, v in enumerate(model.names if hasattr(model, 'names') else model.module.names)}\n",
        "    coco91class = coco80_to_coco91_class()\n",
        "    s = ('%20s' + '%12s' * 6) % ('Class', 'Images', 'Labels', 'P', 'R', 'mAP@.5', 'mAP@.5:.95')\n",
        "    p, r, f1, mp, mr, map50, map, t0, t1 = 0., 0., 0., 0., 0., 0., 0., 0., 0.\n",
        "    loss = torch.zeros(3, device=device)\n",
        "    jdict, stats, ap, ap_class, wandb_images = [], [], [], [], []\n",
        "    for batch_i, (img, targets, paths, shapes) in enumerate(tqdm(dataloader, desc=s)):\n",
        "        img = img.to(device, non_blocking=True)\n",
        "        img = img.half() if half else img.float()  # uint8 to fp16/32\n",
        "        img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
        "        targets = targets.to(device)\n",
        "        nb, _, height, width = img.shape  # batch size, channels, height, width\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Run model\n",
        "            t = time_synchronized()\n",
        "            out, train_out = model(img, augment=augment)  # inference and training outputs\n",
        "            t0 += time_synchronized() - t\n",
        "\n",
        "            # Compute loss\n",
        "            if compute_loss:\n",
        "                loss += compute_loss([x.float() for x in train_out], targets)[1][:3]  # box, obj, cls\n",
        "\n",
        "            # Run NMS\n",
        "            targets[:, 2:] *= torch.Tensor([width, height, width, height]).to(device)  # to pixels\n",
        "            lb = [targets[targets[:, 0] == i, 1:] for i in range(nb)] if save_hybrid else []  # for autolabelling\n",
        "            t = time_synchronized()\n",
        "            out = non_max_suppression(out, conf_thres=conf_thres, iou_thres=iou_thres, labels=lb, multi_label=True)\n",
        "            t1 += time_synchronized() - t\n",
        "\n",
        "        # Statistics per image\n",
        "        for si, pred in enumerate(out):\n",
        "            labels = targets[targets[:, 0] == si, 1:]\n",
        "            nl = len(labels)\n",
        "            tcls = labels[:, 0].tolist() if nl else []  # target class\n",
        "            path = Path(paths[si])\n",
        "            seen += 1\n",
        "\n",
        "            if len(pred) == 0:\n",
        "                if nl:\n",
        "                    stats.append((torch.zeros(0, niou, dtype=torch.bool), torch.Tensor(), torch.Tensor(), tcls))\n",
        "                continue\n",
        "\n",
        "            # Predictions\n",
        "            predn = pred.clone()\n",
        "            scale_coords(img[si].shape[1:], predn[:, :4], shapes[si][0], shapes[si][1])  # native-space pred\n",
        "\n",
        "            # Assign all predictions as incorrect\n",
        "            correct = torch.zeros(pred.shape[0], niou, dtype=torch.bool, device=device)\n",
        "            if nl:\n",
        "                detected = []  # target indices\n",
        "                tcls_tensor = labels[:, 0]\n",
        "\n",
        "                # target boxes\n",
        "                tbox = xywh2xyxy(labels[:, 1:5])\n",
        "                scale_coords(img[si].shape[1:], tbox, shapes[si][0], shapes[si][1])  # native-space labels\n",
        "                if plots:\n",
        "                    confusion_matrix.process_batch(predn, torch.cat((labels[:, 0:1], tbox), 1))\n",
        "\n",
        "                # Per target class\n",
        "                for cls in torch.unique(tcls_tensor):\n",
        "                    ti = (cls == tcls_tensor).nonzero(as_tuple=False).view(-1)  # prediction indices\n",
        "                    pi = (cls == pred[:, 5]).nonzero(as_tuple=False).view(-1)  # target indices\n",
        "\n",
        "                    # Search for detections\n",
        "                    if pi.shape[0]:\n",
        "                        # Prediction to target ious\n",
        "                        ious, i = box_iou(predn[pi, :4], tbox[ti]).max(1)  # best ious, indices\n",
        "\n",
        "                        # Append detections\n",
        "                        detected_set = set()\n",
        "                        for j in (ious > iouv[0]).nonzero(as_tuple=False):\n",
        "                            d = ti[i[j]]  # detected target\n",
        "                            if d.item() not in detected_set:\n",
        "                                detected_set.add(d.item())\n",
        "                                detected.append(d)\n",
        "                                correct[pi[j]] = ious[j] > iouv  # iou_thres is 1xn\n",
        "                                if len(detected) == nl:  # all targets already located in image\n",
        "                                    break\n",
        "\n",
        "            # Append statistics (correct, conf, pcls, tcls)\n",
        "            stats.append((correct.cpu(), pred[:, 4].cpu(), pred[:, 5].cpu(), tcls))\n",
        "\n",
        "        # Plot images\n",
        "        if plots and batch_i < 3:\n",
        "            f = save_dir / f'test_batch{batch_i}_labels.jpg'  # labels\n",
        "            Thread(target=plot_images, args=(img, targets, paths, f, names), daemon=True).start()\n",
        "            f = save_dir / f'test_batch{batch_i}_pred.jpg'  # predictions\n",
        "            Thread(target=plot_images, args=(img, output_to_target(out), paths, f, names), daemon=True).start()\n",
        "\n",
        "    # Compute statistics\n",
        "    stats = [np.concatenate(x, 0) for x in zip(*stats)]  # to numpy\n",
        "    if len(stats) and stats[0].any():\n",
        "        p, r, ap, f1, ap_class = ap_per_class(*stats, plot=plots, save_dir=save_dir, names=names)\n",
        "        ap50, ap = ap[:, 0], ap.mean(1)  # AP@0.5, AP@0.5:0.95\n",
        "        mp, mr, map50, map = p.mean(), r.mean(), ap50.mean(), ap.mean()\n",
        "        nt = np.bincount(stats[3].astype(np.int64), minlength=nc)  # number of targets per class\n",
        "    else:\n",
        "        nt = torch.zeros(1)\n",
        "\n",
        "    # Print results\n",
        "    pf = '%20s' + '%12i' * 2 + '%12.3g' * 4  # print format\n",
        "    print(pf % ('all', seen, nt.sum(), mp, mr, map50, map))\n",
        "\n",
        "    # Print results per class\n",
        "    if (verbose or (nc < 50 and not training)) and nc > 1 and len(stats):\n",
        "        for i, c in enumerate(ap_class):\n",
        "            print(pf % (names[c], seen, nt[c], p[i], r[i], ap50[i], ap[i]))\n",
        "\n",
        "    # Print speeds\n",
        "    t = tuple(x / seen * 1E3 for x in (t0, t1, t0 + t1)) + (imgsz, imgsz, batch_size)  # tuple\n",
        "    if not training:\n",
        "        print('Speed: %.1f/%.1f/%.1f ms inference/NMS/total per %gx%g image at batch-size %g' % t)\n",
        "\n",
        "    # Plots\n",
        "    if plots:\n",
        "        confusion_matrix.plot(save_dir=save_dir, names=list(names.values()))\n",
        "        \n",
        "\n",
        "    # Return results\n",
        "    model.float()  # for training\n",
        "    if not training:\n",
        "        s = f\"\\n{len(list(save_dir.glob('labels/*.txt')))} labels saved to {save_dir / 'labels'}\" if save_txt else ''\n",
        "        print(f\"Results saved to {save_dir}{s}\")\n",
        "    maps = np.zeros(nc) + map\n",
        "    for i, c in enumerate(ap_class):\n",
        "        maps[c] = ap[i]\n",
        "    return (mp, mr, map50, map, *(loss.cpu() / len(dataloader)).tolist()), maps, t\n",
        "\n",
        "\n",
        "\n",
        "if opt.task in ('train', 'val', 'test'):  # run normally\n",
        "    test(opt.data,\n",
        "         opt.weights,\n",
        "         opt.batch_size,\n",
        "         opt.img_size,\n",
        "         opt.conf_thres,\n",
        "         opt.iou_thres,\n",
        "         opt.save_json,\n",
        "         opt.single_cls,\n",
        "         opt.augment,\n",
        "         opt.verbose,\n",
        "         save_txt=opt.save_txt | opt.save_hybrid,\n",
        "         save_hybrid=opt.save_hybrid,\n",
        "         save_conf=opt.save_conf,\n",
        "         trace=not opt.no_trace,\n",
        "         )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9YT0sIMIosT",
        "outputId": "e0cbc799-2e19-4369-a95c-abbdd4fc2cd7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fusing layers... \n",
            "RepConv.fuse_repvgg_block\n",
            "RepConv.fuse_repvgg_block\n",
            "RepConv.fuse_repvgg_block\n",
            "IDetect.fuse\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '/content/endovis18_coco/labels/val.cache' images and labels... 447 found, 0 missing, 0 empty, 0 corrupted: 100%|██████████| 447/447 [00:00<?, ?it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 14/14 [03:13<00:00, 13.80s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 all         447        1608        0.46       0.368       0.286       0.218\n",
            "              kidney         447         447       0.626       0.622       0.586       0.485\n",
            "     bipolar_forceps         447         419       0.453       0.542        0.31       0.193\n",
            "    prograsp_forceps         447         173       0.274       0.607       0.341       0.248\n",
            "monopolar_curved_scissors         447         418       0.646        0.66       0.503        0.37\n",
            "    ultrasound_probe         447          20           0           0           0           0\n",
            "             suction         447         117       0.239      0.0769      0.0389      0.0267\n",
            "        clip_applier         447          14        0.98      0.0714       0.225       0.201\n",
            "Speed: 422.3/0.6/422.9 ms inference/NMS/total per 320x320 image at batch-size 32\n",
            "Results saved to runs/test/yolov7_endo18_320_val5\n"
          ]
        }
      ]
    }
  ]
}