{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOur37gukpPzRik1LMT+jru",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/tutorial_notebooks/blob/main/Vision_Mamba.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install vision-mamba"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6DKPogBPEpI",
        "outputId": "4440e624-d61d-492a-e9a2-b47980273d31"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m523.9/523.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m872.4/872.4 kB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.1/542.1 kB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m339.6/339.6 kB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m106.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from vision_mamba import Vim\n",
        "\n",
        "# Forward pass\n",
        "x = torch.randn(1, 3, 224, 224)  # Input tensor with shape (batch_size, channels, height, width)\n",
        "\n",
        "# Model\n",
        "model = Vim(\n",
        "    dim=256,  # Dimension of the transformer model\n",
        "    dt_rank=32,  # Rank of the dynamic routing matrix\n",
        "    dim_inner=256,  # Inner dimension of the transformer model\n",
        "    d_state=256,  # Dimension of the state vector\n",
        "    num_classes=1000,  # Number of output classes\n",
        "    image_size=224,  # Size of the input image\n",
        "    patch_size=16,  # Size of each image patch\n",
        "    channels=3,  # Number of input channels\n",
        "    dropout=0.1,  # Dropout rate\n",
        "    depth=12,  # Depth of the transformer model\n",
        ")\n",
        "\n",
        "# Forward pass\n",
        "out = model(x)  # Output tensor from the model\n",
        "print(out.shape)  # Print the shape of the output tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aU0znhoYPLw6",
        "outputId": "0acc22ba-8109-4c00-bd77-6f7533e9c73d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Patch embedding: torch.Size([1, 196, 256])\n",
            "Cls tokens: torch.Size([1, 1, 256])\n",
            "torch.Size([1, 196, 256])\n",
            "Conv1d: tensor([[[0.4517, 0.6841, 0.7698,  ..., 0.6088, 1.0765, 0.7307],\n",
            "         [0.4053, 0.7306, 1.0093,  ..., 0.4668, 1.0130, 0.9004],\n",
            "         [1.0687, 0.6215, 0.6449,  ..., 0.9588, 0.4652, 0.7067],\n",
            "         ...,\n",
            "         [0.7493, 0.7872, 0.5844,  ..., 0.5783, 0.9043, 0.7951],\n",
            "         [0.7921, 0.6989, 0.8491,  ..., 0.4104, 0.7214, 0.9231],\n",
            "         [0.7577, 0.8537, 1.0432,  ..., 1.1400, 0.6921, 0.7480]]],\n",
            "       grad_fn=<SoftplusBackward0>)\n",
            "Conv1d: tensor([[[0.8267, 0.7584, 0.7060,  ..., 0.5434, 0.4920, 0.5936],\n",
            "         [0.7811, 0.7924, 0.7602,  ..., 0.9239, 0.8586, 0.5750],\n",
            "         [0.7763, 0.5292, 0.7099,  ..., 0.5793, 0.8866, 1.0291],\n",
            "         ...,\n",
            "         [0.7711, 0.8039, 0.7002,  ..., 0.6898, 0.6695, 0.6178],\n",
            "         [0.6955, 0.4337, 0.6126,  ..., 0.7806, 0.7681, 0.5613],\n",
            "         [0.9360, 0.5118, 1.0539,  ..., 1.0881, 0.6978, 0.6951]]],\n",
            "       grad_fn=<SoftplusBackward0>)\n",
            "Layer: torch.Size([1, 196, 256])\n",
            "Conv1d: tensor([[[0.5112, 0.7389, 0.6661,  ..., 0.6352, 0.6341, 0.9518],\n",
            "         [0.4566, 0.8517, 0.6609,  ..., 0.5350, 0.8327, 0.8478],\n",
            "         [1.1495, 0.8766, 0.8030,  ..., 0.6593, 0.5911, 0.6944],\n",
            "         ...,\n",
            "         [1.0276, 0.6606, 0.7661,  ..., 0.6834, 0.4077, 0.5841],\n",
            "         [0.5475, 0.5622, 0.5578,  ..., 0.6929, 0.6092, 0.5639],\n",
            "         [0.8362, 0.8097, 1.1324,  ..., 0.4847, 0.5393, 0.7130]]],\n",
            "       grad_fn=<SoftplusBackward0>)\n",
            "Conv1d: tensor([[[0.6383, 0.5648, 0.4231,  ..., 0.7371, 0.7039, 0.4939],\n",
            "         [1.2120, 0.6803, 1.1623,  ..., 0.9214, 0.8999, 0.6596],\n",
            "         [0.5579, 0.8574, 0.7102,  ..., 0.6767, 0.6901, 0.8821],\n",
            "         ...,\n",
            "         [1.0316, 0.6984, 1.1979,  ..., 0.7498, 1.1967, 0.5861],\n",
            "         [0.7797, 0.5753, 0.6917,  ..., 0.8451, 0.5819, 0.6891],\n",
            "         [0.6093, 0.5064, 0.3695,  ..., 0.7305, 0.6756, 0.6071]]],\n",
            "       grad_fn=<SoftplusBackward0>)\n",
            "Layer: torch.Size([1, 196, 256])\n",
            "Conv1d: tensor([[[0.7407, 0.9278, 0.6638,  ..., 0.5925, 0.6534, 0.6621],\n",
            "         [0.8202, 0.6784, 0.6196,  ..., 0.6852, 0.9895, 0.5518],\n",
            "         [0.6158, 0.8744, 0.7955,  ..., 0.5933, 0.7710, 0.4812],\n",
            "         ...,\n",
            "         [0.6771, 0.6502, 0.6964,  ..., 1.2341, 0.9761, 0.4767],\n",
            "         [0.7285, 0.5906, 0.6337,  ..., 0.5549, 0.6574, 0.6174],\n",
            "         [0.9617, 0.5940, 0.6487,  ..., 0.4387, 0.9159, 0.5123]]],\n",
            "       grad_fn=<SoftplusBackward0>)\n",
            "Conv1d: tensor([[[0.8919, 0.8045, 0.7008,  ..., 0.6372, 0.6133, 1.0770],\n",
            "         [0.8140, 0.5002, 0.5055,  ..., 0.8321, 0.5987, 0.8291],\n",
            "         [0.5046, 0.4938, 0.6712,  ..., 0.6970, 0.4788, 0.8087],\n",
            "         ...,\n",
            "         [0.6454, 0.9479, 0.6827,  ..., 0.6342, 0.5908, 0.9653],\n",
            "         [0.4146, 0.6807, 0.6493,  ..., 0.7518, 0.5705, 0.5168],\n",
            "         [0.7843, 0.6773, 0.8850,  ..., 0.5259, 0.5199, 0.7013]]],\n",
            "       grad_fn=<SoftplusBackward0>)\n",
            "Layer: torch.Size([1, 196, 256])\n",
            "Conv1d: tensor([[[0.6743, 0.7675, 0.8442,  ..., 0.6370, 0.6480, 0.9010],\n",
            "         [0.5501, 0.5944, 0.6109,  ..., 0.6669, 0.7197, 0.6567],\n",
            "         [0.7135, 0.4664, 0.6503,  ..., 0.5208, 0.6131, 0.6224],\n",
            "         ...,\n",
            "         [0.8854, 0.5698, 0.6583,  ..., 0.8511, 0.8011, 0.6497],\n",
            "         [0.8580, 1.0699, 1.0517,  ..., 0.3488, 0.9189, 0.6767],\n",
            "         [0.9674, 0.8040, 0.6089,  ..., 0.7663, 1.1318, 0.7880]]],\n",
            "       grad_fn=<SoftplusBackward0>)\n",
            "Conv1d: tensor([[[0.4648, 0.5338, 0.6610,  ..., 0.6263, 0.9909, 0.6577],\n",
            "         [0.5224, 0.5130, 0.7127,  ..., 0.8159, 0.8071, 0.5940],\n",
            "         [0.9773, 1.0687, 0.9131,  ..., 0.9316, 0.6122, 0.5455],\n",
            "         ...,\n",
            "         [0.6634, 0.7108, 0.7853,  ..., 0.6795, 0.7833, 0.6410],\n",
            "         [0.9666, 0.6118, 0.4949,  ..., 0.6213, 0.7458, 0.7507],\n",
            "         [0.8933, 0.6082, 0.5960,  ..., 0.5216, 0.5753, 0.7837]]],\n",
            "       grad_fn=<SoftplusBackward0>)\n",
            "Layer: torch.Size([1, 196, 256])\n",
            "Conv1d: tensor([[[0.5615, 0.6268, 0.4966,  ..., 0.4119, 0.7879, 0.6743],\n",
            "         [0.7487, 1.0572, 0.6225,  ..., 0.5203, 0.8872, 1.0068],\n",
            "         [0.6479, 0.5826, 0.7555,  ..., 0.9173, 0.5313, 0.4906],\n",
            "         ...,\n",
            "         [0.6246, 0.8539, 1.3269,  ..., 0.5376, 0.6255, 0.7811],\n",
            "         [0.9373, 0.4835, 0.8529,  ..., 0.8900, 0.4508, 0.7028],\n",
            "         [0.5859, 0.5659, 0.5784,  ..., 0.8841, 0.6719, 0.5556]]],\n",
            "       grad_fn=<SoftplusBackward0>)\n",
            "Conv1d: tensor([[[0.8072, 0.5761, 0.7393,  ..., 0.6131, 0.7757, 0.5185],\n",
            "         [0.7195, 0.6693, 1.0573,  ..., 0.7466, 0.9272, 0.6263],\n",
            "         [0.7692, 0.7954, 0.7915,  ..., 0.7396, 0.7045, 1.0557],\n",
            "         ...,\n",
            "         [0.7059, 0.9607, 0.6863,  ..., 1.0478, 0.6987, 1.2679],\n",
            "         [0.6989, 0.7649, 0.8106,  ..., 0.9158, 0.3879, 1.2782],\n",
            "         [0.7732, 0.8735, 0.7649,  ..., 0.6524, 0.7251, 0.8335]]],\n",
            "       grad_fn=<SoftplusBackward0>)\n",
            "Layer: torch.Size([1, 196, 256])\n",
            "Conv1d: tensor([[[0.6272, 0.7941, 0.5524,  ..., 0.4594, 0.3664, 0.7109],\n",
            "         [0.8869, 0.6633, 0.9698,  ..., 0.2810, 0.7857, 0.6213],\n",
            "         [0.6869, 0.4936, 0.9563,  ..., 0.7592, 0.7858, 0.7442],\n",
            "         ...,\n",
            "         [0.5929, 0.4787, 0.6873,  ..., 1.0045, 1.0240, 0.7854],\n",
            "         [0.6409, 0.6071, 0.4683,  ..., 0.5647, 0.7546, 0.6747],\n",
            "         [0.7682, 0.8346, 0.6450,  ..., 0.9036, 0.5317, 0.6608]]],\n",
            "       grad_fn=<SoftplusBackward0>)\n",
            "Conv1d: tensor([[[0.6718, 0.7442, 0.6482,  ..., 1.2161, 0.5768, 0.8313],\n",
            "         [0.6642, 0.8506, 0.8265,  ..., 0.7361, 0.6043, 0.6917],\n",
            "         [0.5172, 0.8237, 0.6948,  ..., 0.7863, 0.7502, 0.7007],\n",
            "         ...,\n",
            "         [1.1476, 0.7737, 0.7306,  ..., 0.6734, 0.4802, 0.6865],\n",
            "         [0.9616, 0.4469, 0.8175,  ..., 0.6723, 0.9334, 0.7172],\n",
            "         [0.5757, 0.5541, 0.5714,  ..., 0.5836, 0.6895, 0.8662]]],\n",
            "       grad_fn=<SoftplusBackward0>)\n",
            "Layer: torch.Size([1, 196, 256])\n",
            "Conv1d: tensor([[[1.1988, 0.6153, 0.9883,  ..., 0.5057, 0.7343, 0.9008],\n",
            "         [1.0627, 0.5828, 0.8364,  ..., 1.0407, 0.9216, 0.8078],\n",
            "         [0.9679, 0.7063, 0.6801,  ..., 0.4261, 0.6291, 0.7968],\n",
            "         ...,\n",
            "         [0.7441, 0.3898, 0.6322,  ..., 0.5900, 0.7067, 0.4421],\n",
            "         [0.7589, 0.6177, 0.6412,  ..., 0.8247, 0.4657, 0.7658],\n",
            "         [0.6783, 0.6808, 0.7220,  ..., 0.6672, 0.7013, 0.6085]]],\n",
            "       grad_fn=<SoftplusBackward0>)\n",
            "Conv1d: tensor([[[0.9203, 0.8104, 0.5816,  ..., 0.6569, 0.7730, 0.9695],\n",
            "         [0.9479, 0.6242, 0.9399,  ..., 0.9830, 0.6396, 0.6741],\n",
            "         [0.6945, 0.7386, 0.5458,  ..., 1.0550, 0.4449, 0.4131],\n",
            "         ...,\n",
            "         [0.5657, 0.6872, 0.7319,  ..., 0.6223, 1.0299, 0.5898],\n",
            "         [0.7142, 0.5542, 0.9405,  ..., 0.6813, 0.7418, 0.7227],\n",
            "         [0.8363, 0.7281, 0.5899,  ..., 1.0918, 0.6454, 0.5364]]],\n",
            "       grad_fn=<SoftplusBackward0>)\n",
            "Layer: torch.Size([1, 196, 256])\n",
            "Conv1d: tensor([[[0.7675, 0.9128, 0.8600,  ..., 0.5190, 0.4956, 0.6504],\n",
            "         [0.5972, 0.8760, 1.3350,  ..., 0.6911, 0.9415, 0.5224],\n",
            "         [0.9828, 0.7897, 0.6430,  ..., 0.8480, 0.7130, 0.8016],\n",
            "         ...,\n",
            "         [0.6535, 0.6478, 0.6491,  ..., 0.6748, 0.6938, 0.4472],\n",
            "         [0.6753, 0.6257, 0.6258,  ..., 0.8359, 0.4682, 0.7180],\n",
            "         [0.8764, 0.6879, 0.4842,  ..., 0.7775, 0.5524, 0.6697]]],\n",
            "       grad_fn=<SoftplusBackward0>)\n",
            "Conv1d: tensor([[[0.6809, 0.7983, 0.6994,  ..., 0.7051, 0.4876, 0.7061],\n",
            "         [0.4265, 0.6048, 0.6390,  ..., 0.4501, 0.6939, 0.7480],\n",
            "         [0.5151, 0.6989, 0.6553,  ..., 1.2833, 0.6746, 0.6017],\n",
            "         ...,\n",
            "         [0.9012, 0.8713, 0.9341,  ..., 0.7216, 0.8111, 0.6461],\n",
            "         [0.5223, 1.0074, 1.0646,  ..., 1.0158, 0.6425, 0.7774],\n",
            "         [1.0517, 0.6990, 0.6303,  ..., 0.6155, 0.7146, 0.5272]]],\n",
            "       grad_fn=<SoftplusBackward0>)\n",
            "Layer: torch.Size([1, 196, 256])\n",
            "Conv1d: tensor([[[0.7996, 0.4596, 0.7957,  ..., 0.4447, 0.5827, 0.8226],\n",
            "         [0.6074, 0.5138, 0.5379,  ..., 0.4420, 0.7212, 0.9843],\n",
            "         [0.6480, 0.5388, 0.8852,  ..., 0.5865, 0.6267, 0.6701],\n",
            "         ...,\n",
            "         [0.7335, 0.6362, 0.7121,  ..., 0.6596, 0.9721, 0.6631],\n",
            "         [0.7707, 1.0392, 0.7492,  ..., 0.8605, 0.8909, 0.9526],\n",
            "         [0.8498, 0.9419, 0.7529,  ..., 0.8025, 0.6976, 1.0939]]],\n",
            "       grad_fn=<SoftplusBackward0>)\n",
            "Conv1d: tensor([[[0.8347, 0.5158, 0.5981,  ..., 0.5106, 0.7372, 0.5706],\n",
            "         [0.7756, 0.5932, 0.6199,  ..., 0.8312, 0.8328, 0.6095],\n",
            "         [0.7533, 0.6645, 0.7374,  ..., 0.6692, 0.5769, 0.6306],\n",
            "         ...,\n",
            "         [0.7690, 0.7162, 0.7638,  ..., 0.7352, 0.9559, 1.1692],\n",
            "         [0.7239, 0.5212, 0.7246,  ..., 0.6407, 0.4515, 0.6857],\n",
            "         [0.8866, 0.6583, 0.4079,  ..., 0.6438, 0.5820, 0.9673]]],\n",
            "       grad_fn=<SoftplusBackward0>)\n",
            "Layer: torch.Size([1, 196, 256])\n",
            "Conv1d: tensor([[[0.5216, 0.7930, 0.7316,  ..., 0.9610, 1.0016, 0.6694],\n",
            "         [0.8130, 1.0523, 0.8674,  ..., 0.7102, 0.7778, 0.9399],\n",
            "         [0.8858, 0.7587, 0.5196,  ..., 0.6687, 0.6806, 0.6659],\n",
            "         ...,\n",
            "         [0.5953, 0.6146, 0.8537,  ..., 0.6492, 0.6881, 0.6535],\n",
            "         [0.9149, 0.5856, 0.9503,  ..., 0.6462, 0.4715, 0.6970],\n",
            "         [0.6776, 0.6745, 0.7155,  ..., 0.8397, 0.9230, 0.6248]]],\n",
            "       grad_fn=<SoftplusBackward0>)\n",
            "Conv1d: tensor([[[0.7627, 0.5932, 0.6639,  ..., 0.6608, 0.6158, 0.5633],\n",
            "         [0.7044, 0.5575, 0.6408,  ..., 0.5756, 0.7639, 0.6193],\n",
            "         [0.6409, 0.9378, 0.5659,  ..., 0.9251, 0.8117, 0.5764],\n",
            "         ...,\n",
            "         [0.8509, 0.6253, 0.8739,  ..., 0.5209, 0.7771, 0.6226],\n",
            "         [0.8983, 0.9206, 0.6649,  ..., 0.9308, 0.6117, 0.9346],\n",
            "         [0.8455, 0.5112, 0.9303,  ..., 0.5536, 0.4708, 0.5972]]],\n",
            "       grad_fn=<SoftplusBackward0>)\n",
            "Layer: torch.Size([1, 196, 256])\n",
            "Conv1d: tensor([[[0.4930, 0.7618, 0.5910,  ..., 0.6214, 0.7642, 0.5662],\n",
            "         [0.7241, 0.8089, 0.5769,  ..., 1.1830, 0.7517, 0.8612],\n",
            "         [0.4701, 0.5687, 0.9460,  ..., 0.6627, 1.0028, 0.6898],\n",
            "         ...,\n",
            "         [0.7293, 0.3839, 0.6756,  ..., 0.9250, 0.6023, 0.6844],\n",
            "         [0.7627, 0.6278, 0.7709,  ..., 0.9885, 0.7035, 0.5113],\n",
            "         [0.6958, 0.9702, 0.6731,  ..., 0.8242, 0.8394, 0.6808]]],\n",
            "       grad_fn=<SoftplusBackward0>)\n",
            "Conv1d: tensor([[[0.5007, 0.7041, 0.9015,  ..., 0.7130, 0.6935, 0.8221],\n",
            "         [0.6530, 0.4565, 0.5272,  ..., 0.8994, 0.6908, 0.7255],\n",
            "         [0.5597, 0.8863, 0.8068,  ..., 0.5677, 0.4842, 0.6989],\n",
            "         ...,\n",
            "         [0.6449, 0.7778, 1.2028,  ..., 0.9511, 0.6934, 0.5369],\n",
            "         [0.7772, 0.8879, 0.7092,  ..., 0.4285, 0.4449, 0.7200],\n",
            "         [0.5420, 0.6616, 0.7515,  ..., 0.5408, 0.5021, 0.8342]]],\n",
            "       grad_fn=<SoftplusBackward0>)\n",
            "Layer: torch.Size([1, 196, 256])\n",
            "Conv1d: tensor([[[0.7801, 0.5308, 0.4632,  ..., 0.4859, 0.4972, 0.8138],\n",
            "         [0.7993, 0.6137, 0.7211,  ..., 0.6259, 0.6733, 0.7722],\n",
            "         [0.7806, 0.7094, 0.7045,  ..., 0.7373, 0.7688, 0.7614],\n",
            "         ...,\n",
            "         [0.4809, 0.4276, 0.5213,  ..., 0.5337, 0.7968, 0.4952],\n",
            "         [0.5357, 0.6444, 0.7341,  ..., 0.6818, 0.7921, 0.6337],\n",
            "         [0.6695, 0.7094, 0.7890,  ..., 0.7903, 0.3739, 0.9284]]],\n",
            "       grad_fn=<SoftplusBackward0>)\n",
            "Conv1d: tensor([[[0.4554, 0.7953, 0.5812,  ..., 1.0938, 0.8754, 0.9638],\n",
            "         [0.5083, 0.5525, 0.8152,  ..., 0.7819, 0.9299, 0.5207],\n",
            "         [0.8642, 0.5707, 0.9232,  ..., 0.5190, 0.8234, 0.8518],\n",
            "         ...,\n",
            "         [0.6629, 0.7701, 0.6186,  ..., 0.4504, 0.7443, 0.7214],\n",
            "         [0.7735, 0.5860, 0.8513,  ..., 0.7082, 0.5254, 0.6158],\n",
            "         [0.5162, 0.7246, 0.7618,  ..., 0.8101, 0.8053, 0.5351]]],\n",
            "       grad_fn=<SoftplusBackward0>)\n",
            "Layer: torch.Size([1, 196, 256])\n",
            "torch.Size([1, 1000])\n",
            "tensor([[-6.0560e-01,  5.8428e-01,  4.1890e-01, -2.8302e-02,  2.3222e-01,\n",
            "          4.5955e-01,  3.0124e-01, -4.3803e-01,  1.5968e-01,  2.9284e-01,\n",
            "          7.6269e-01,  5.4520e-01,  5.5013e-01,  2.2466e-01, -6.8922e-03,\n",
            "         -3.2182e-02,  3.4757e-01, -2.1799e-01, -1.5796e-01, -1.9969e-01,\n",
            "          9.5291e-01,  8.2024e-01, -1.2440e-01, -1.0896e+00, -2.1823e-01,\n",
            "          3.3619e-01, -7.9274e-01,  1.8393e-01, -1.1954e-01,  1.2230e+00,\n",
            "         -5.1528e-01, -8.7491e-02, -5.0503e-02,  1.8455e-01,  6.3582e-01,\n",
            "          1.2054e+00, -3.0100e-01, -1.5224e-01,  7.4511e-01,  6.4203e-01,\n",
            "         -1.9904e-01, -5.2611e-01, -5.2003e-01,  3.0634e-02,  3.6431e-01,\n",
            "         -1.1202e-01, -8.1010e-01, -2.0736e-01, -3.7414e-01, -4.9052e-01,\n",
            "         -4.3630e-01,  7.1544e-01, -6.7362e-01, -1.4589e-01,  6.4991e-01,\n",
            "          3.6347e-01, -4.9310e-01,  2.3940e-01, -2.7525e-01, -2.7079e-01,\n",
            "         -5.5611e-01,  8.1184e-01, -1.1806e-02, -1.6201e-01,  1.6241e-01,\n",
            "         -6.1500e-01,  3.1960e-01, -6.2069e-02, -6.7559e-01, -1.9858e-01,\n",
            "          2.0172e-01,  1.0983e+00, -1.3199e-01, -2.6347e-01,  1.3372e-01,\n",
            "          9.1872e-01,  3.7498e-01, -7.4523e-01, -5.6265e-01, -1.5175e-01,\n",
            "         -7.0207e-01, -4.9509e-02, -6.5996e-01,  1.9041e-01, -3.3046e-01,\n",
            "         -5.4533e-01, -5.5655e-01, -2.5076e-01,  2.1376e-01,  5.1329e-01,\n",
            "         -1.1557e+00, -5.2028e-01, -4.4916e-01,  2.2323e-01,  3.9950e-01,\n",
            "         -5.5177e-01,  3.5472e-01,  1.1974e+00, -3.5411e-01,  2.3203e-01,\n",
            "          7.3294e-01, -1.2680e+00,  9.7449e-01,  2.4905e-02, -3.1356e-01,\n",
            "         -7.3935e-02, -7.3944e-04, -9.1749e-01,  6.9598e-01, -4.5839e-01,\n",
            "          3.5824e-01, -2.5628e-01, -1.2683e-02,  7.9377e-02, -6.3423e-01,\n",
            "         -4.0262e-01,  1.9088e-01,  5.4804e-01, -4.1806e-01, -9.4486e-02,\n",
            "         -6.9904e-01, -5.7125e-01, -9.0193e-01, -3.2565e-02, -9.2701e-01,\n",
            "          1.7128e-01, -3.4603e-01, -7.4297e-01,  8.8662e-02, -4.6168e-01,\n",
            "         -3.8868e-01,  5.2983e-02,  4.2153e-01,  2.6677e-01,  1.1615e-02,\n",
            "         -2.5005e-01,  1.6360e-01,  5.4498e-01,  1.6384e-01, -3.9649e-01,\n",
            "         -9.6376e-01,  5.7975e-01, -8.4262e-01, -6.2923e-01,  2.5913e-01,\n",
            "          2.1696e-01, -5.7587e-01, -1.0702e-01,  1.5194e+00, -1.0223e+00,\n",
            "          7.1677e-01,  5.7556e-02, -4.2587e-01, -3.3000e-01, -2.0564e-01,\n",
            "          3.5933e-01,  3.7682e-01, -2.3139e-01, -2.1599e-01,  1.6417e-01,\n",
            "         -4.9647e-01,  4.3946e-01, -2.2711e-02, -1.0043e+00,  4.6419e-01,\n",
            "          4.1976e-01, -2.2013e+00,  8.1496e-03, -2.3908e-01,  6.4981e-01,\n",
            "          9.1655e-01,  9.7080e-02, -1.4983e-01, -1.8319e-01, -5.7509e-02,\n",
            "         -9.7415e-02, -2.0191e-01, -7.0088e-01, -1.1024e+00,  3.8823e-01,\n",
            "          1.0748e+00, -5.8124e-01,  1.4456e+00,  3.7451e-01, -7.1538e-02,\n",
            "          4.0358e-01,  4.9973e-01, -4.5952e-01,  5.4606e-02,  1.0520e-01,\n",
            "          5.8795e-02, -7.4687e-02, -4.9948e-01, -2.4234e-01,  3.0739e-01,\n",
            "          1.0653e+00, -4.5559e-02,  1.2615e-01, -6.3884e-01,  2.4057e-01,\n",
            "          8.4587e-01,  1.3758e+00,  2.1423e-02,  1.0318e+00,  2.4566e-01,\n",
            "          6.3638e-01, -2.1047e-01, -1.0438e+00, -1.5628e-01,  2.7668e-01,\n",
            "          7.9056e-01,  2.0196e-01, -1.4868e-03, -1.2945e+00,  1.2826e-01,\n",
            "         -2.2058e-01,  5.1726e-01,  2.2628e-01,  8.2751e-01, -2.6458e-02,\n",
            "         -4.9420e-01,  2.5442e-01, -9.4687e-01,  5.5381e-01, -5.4862e-02,\n",
            "          7.4946e-01,  3.9220e-01,  9.1529e-01, -3.5218e-01, -6.2644e-02,\n",
            "         -1.3707e+00,  4.6975e-01,  1.6444e-01,  8.6254e-01,  3.1895e-01,\n",
            "         -1.6072e-01, -2.7713e-01, -8.5636e-01, -1.2031e+00,  5.5145e-01,\n",
            "          2.1234e-01,  1.7964e-01,  4.0043e-01,  1.9026e-01,  8.1743e-01,\n",
            "          6.1989e-01, -3.1954e-01, -4.1969e-01,  3.4626e-01,  1.0422e+00,\n",
            "         -9.0829e-02, -2.6711e-01, -3.0421e-01, -2.3272e-01,  1.0636e+00,\n",
            "         -2.8298e-01,  9.6929e-01, -3.0631e-01,  4.9665e-02, -7.9876e-01,\n",
            "         -3.7132e-01, -2.7357e-01,  3.5180e-02,  3.9543e-02, -6.6666e-01,\n",
            "         -3.2075e-01, -3.5840e-01,  1.9049e+00,  8.8761e-01, -8.6832e-01,\n",
            "         -3.8047e-01,  1.1870e-01, -1.4677e-01,  9.0708e-01, -6.7040e-01,\n",
            "          4.0360e-01,  1.0531e+00, -2.1410e-01,  8.6161e-01, -4.8049e-01,\n",
            "          6.5266e-02, -1.8942e-01,  1.4928e-01, -1.2524e+00, -7.1974e-01,\n",
            "         -1.7524e-01, -1.0569e+00, -7.6793e-02,  1.1917e+00,  5.5578e-01,\n",
            "          2.5945e-01,  1.5187e-01, -2.0103e-02, -8.3638e-02, -2.6806e-01,\n",
            "          6.2699e-01,  8.2475e-01,  6.8633e-02, -1.7528e-01,  4.9513e-01,\n",
            "          1.4904e-01, -4.6362e-01, -2.4947e-01,  5.0939e-01,  1.0298e-01,\n",
            "          5.7010e-02,  1.5326e+00,  3.2042e-01,  1.3665e-01,  1.1811e-01,\n",
            "         -1.0985e-01,  1.5216e+00,  4.2689e-01,  1.1939e+00,  2.1650e-01,\n",
            "         -1.5650e-01, -5.8160e-01,  5.8892e-01,  1.1866e-02, -2.4860e-01,\n",
            "         -5.4616e-01,  2.4096e-01, -4.5993e-01, -1.1327e+00,  7.8703e-01,\n",
            "         -3.2645e-01,  6.5077e-01, -4.8777e-01, -2.7952e-01, -3.1935e-01,\n",
            "          6.5181e-01, -9.2308e-01, -4.6060e-01, -5.1468e-01,  2.7439e-01,\n",
            "          6.3698e-01,  8.0661e-01, -1.0746e-01,  9.4415e-01, -3.4867e-01,\n",
            "         -3.3432e-01, -7.5804e-01, -5.5959e-01, -6.7499e-02, -3.4741e-01,\n",
            "         -8.6207e-01, -1.6688e-01,  7.2530e-01,  7.8124e-01, -1.8898e-02,\n",
            "          6.0727e-01, -8.7489e-01,  5.1257e-01, -1.0006e+00,  9.0776e-01,\n",
            "         -5.2288e-01, -4.8068e-02,  1.6272e-01, -1.1861e+00, -6.2139e-01,\n",
            "          3.4304e-01,  7.6745e-01, -1.3016e-01,  7.5094e-02,  3.0839e-01,\n",
            "          1.5444e+00, -1.5374e-02, -2.3902e-01, -1.7275e-01, -1.1160e-01,\n",
            "          6.2343e-01,  5.2683e-01,  1.4586e+00,  1.0203e-01,  3.3501e-01,\n",
            "         -6.6412e-01,  5.5239e-01, -2.1398e-01,  5.6378e-01, -7.4031e-01,\n",
            "         -1.6200e+00, -3.0436e-02,  5.8683e-01,  3.7074e-01, -2.7489e-01,\n",
            "         -1.4445e-01,  9.1746e-02,  5.1032e-01,  1.2702e+00, -2.7913e-01,\n",
            "          1.2510e-01, -3.4433e-01, -1.0264e-01, -5.2285e-01,  2.7387e-01,\n",
            "          4.2116e-03, -2.4663e-01, -1.2563e+00,  1.3555e+00,  6.2330e-01,\n",
            "          4.9544e-01, -9.8887e-01, -2.0922e-01,  1.2143e+00, -2.8237e-01,\n",
            "         -9.8101e-01,  3.2773e-01,  5.0714e-02, -5.2616e-01,  6.5741e-01,\n",
            "         -3.4633e-01,  9.8918e-01,  5.9513e-01, -1.1343e-03, -3.1545e-01,\n",
            "          4.7741e-01,  8.1680e-02, -2.2863e-01, -8.8623e-01,  2.2655e-01,\n",
            "         -5.3608e-01, -1.5385e-02,  4.3019e-01,  1.6911e-01,  6.1700e-01,\n",
            "          3.8307e-01, -7.4487e-01,  1.7699e-01, -3.3224e-01, -3.5079e-01,\n",
            "         -7.2737e-02, -8.3358e-01,  1.0588e+00, -5.6562e-01, -6.4460e-01,\n",
            "         -1.7348e-01, -1.8320e-01,  5.8629e-02, -6.6854e-01,  4.1560e-01,\n",
            "         -5.6841e-01,  1.1651e-01,  4.3478e-01, -1.0713e-02,  1.2216e-01,\n",
            "          4.7328e-01,  9.0354e-03,  1.1487e-01, -6.2264e-01,  1.2467e-02,\n",
            "          8.5428e-02, -7.3675e-01, -6.2887e-01, -5.1769e-01,  2.7624e-01,\n",
            "         -8.5789e-01, -4.7242e-01, -1.0246e+00, -5.3457e-02,  4.7808e-01,\n",
            "         -3.5885e-02, -2.8492e-01, -6.1490e-02, -4.2582e-03,  4.6653e-01,\n",
            "          7.4447e-01,  5.6223e-01,  2.4135e-01, -5.1484e-01,  3.6153e-01,\n",
            "         -7.4708e-01, -2.6212e-01,  4.3867e-01,  4.6588e-01,  3.4266e-01,\n",
            "          9.4688e-02,  8.8247e-01,  6.1281e-01,  4.5990e-01, -2.5768e-01,\n",
            "          4.2247e-01, -3.5585e-01,  4.1067e-02, -4.9529e-01, -2.3473e-01,\n",
            "         -8.9820e-02, -3.6867e-01,  1.0290e+00,  5.1347e-01, -1.0885e+00,\n",
            "          6.8382e-01,  5.5227e-01,  1.0684e+00,  3.2093e-01,  8.8923e-01,\n",
            "          7.8015e-02,  3.6466e-01, -1.8045e+00,  1.4999e+00, -2.8949e-01,\n",
            "         -2.0319e-01,  9.3078e-01,  7.3038e-01,  7.6754e-01,  2.7205e-01,\n",
            "         -2.0116e-02,  2.4361e-01, -5.8286e-02,  2.8175e-01, -2.9140e-03,\n",
            "         -4.2144e-01, -6.0571e-01,  8.8977e-02, -7.6251e-01,  6.4111e-01,\n",
            "          3.9355e-02, -2.3847e-02,  2.4564e-01,  3.1119e-02,  9.4037e-02,\n",
            "          8.2246e-01,  3.4487e-01, -8.8854e-01,  2.4019e-01,  3.2129e-01,\n",
            "          2.7866e-01, -1.1555e-01,  4.5949e-01, -2.3770e-01, -2.3047e-01,\n",
            "         -1.1412e+00,  5.2005e-01, -5.9631e-02, -6.1969e-01,  1.6773e+00,\n",
            "         -4.4731e-01, -1.7870e-02, -2.6903e-01,  2.6737e-01, -2.5054e-01,\n",
            "         -4.5254e-01, -7.8446e-02,  6.6282e-01,  6.6722e-02,  2.8365e-01,\n",
            "         -1.4208e-01,  1.9136e-02, -3.5255e-01, -1.2120e-01,  3.2062e-01,\n",
            "         -2.7656e-01,  4.6813e-01,  4.6637e-01, -5.2407e-01,  6.3220e-01,\n",
            "         -7.4973e-01,  1.1233e+00, -4.8055e-01,  7.5339e-02, -9.3772e-01,\n",
            "         -1.0309e-01, -7.0039e-01, -2.3251e-01,  4.4430e-01, -4.4776e-01,\n",
            "         -3.7455e-01,  5.1173e-01,  7.1218e-01, -3.1013e-01,  1.5363e-01,\n",
            "         -3.2198e-01, -1.3662e-01,  4.5894e-01,  5.1974e-01,  9.8478e-01,\n",
            "         -2.2601e-01,  1.3545e+00, -2.7472e-01, -4.0223e-01,  2.2562e-01,\n",
            "          4.8199e-01,  1.4874e+00, -3.5579e-01, -8.6849e-02,  1.5970e-01,\n",
            "         -8.0799e-02,  1.0894e-01, -8.0770e-02,  4.3085e-01, -3.0998e-01,\n",
            "          4.7620e-01,  6.8442e-01, -6.4390e-01, -5.4010e-01, -1.2380e+00,\n",
            "         -1.0022e+00, -3.9918e-01,  3.9422e-01,  1.9057e-01, -5.9193e-01,\n",
            "         -4.0357e-01,  1.1715e-01,  1.5708e-01, -4.4852e-01, -3.6743e-01,\n",
            "         -1.1226e-01, -8.1920e-01, -9.0750e-01,  3.7973e-01,  7.1358e-02,\n",
            "         -4.0911e-01,  5.4986e-01,  8.9652e-01, -4.1229e-01, -1.3446e-02,\n",
            "         -4.6537e-01, -6.1650e-01,  1.2361e+00,  3.8737e-01,  9.0880e-01,\n",
            "          6.4502e-01,  2.5354e-01,  1.9398e-01,  8.3268e-01,  5.9585e-01,\n",
            "         -1.1385e+00, -5.8995e-01, -3.2647e-02,  1.7469e-01,  6.1603e-01,\n",
            "          1.0900e+00, -4.5445e-01, -1.2291e-02,  7.6231e-01,  7.7174e-02,\n",
            "         -3.6818e-01, -1.2764e+00, -4.6611e-04,  5.4441e-01,  2.2244e-01,\n",
            "         -8.6388e-01, -3.7541e-01,  8.9957e-01,  3.0640e-01, -9.1880e-02,\n",
            "         -4.8338e-02, -1.3593e+00,  1.0282e+00, -9.6034e-01, -6.1575e-01,\n",
            "          8.1770e-01, -3.0863e-01,  2.2519e-01,  8.0948e-02,  1.0712e-02,\n",
            "         -4.8748e-01,  5.8015e-02, -7.8221e-01,  5.7055e-01, -2.3452e-01,\n",
            "         -4.3638e-01,  7.7925e-01, -1.5919e-01, -5.1856e-01, -8.0251e-01,\n",
            "          3.3455e-02,  5.8594e-01, -2.8423e-01,  3.0229e-01, -4.0872e-01,\n",
            "         -5.7706e-01,  1.4846e-01,  1.3922e+00,  6.0887e-01,  1.6830e-01,\n",
            "          3.6818e-01,  9.2507e-02,  1.0779e+00, -1.7888e-01, -5.4006e-01,\n",
            "         -5.9879e-01, -1.2248e-01, -2.8932e-01, -1.4897e+00, -7.8497e-01,\n",
            "          2.5890e-01, -1.8027e-01, -1.9545e-01,  2.0852e-01, -2.5864e-01,\n",
            "          5.4897e-01,  1.0286e+00, -1.0055e+00, -7.6282e-01,  3.8213e-02,\n",
            "          7.4768e-01,  4.1054e-01,  1.6410e-01, -5.6899e-02, -3.6311e-01,\n",
            "          8.4748e-01, -4.7158e-02, -4.4874e-01, -7.7955e-01,  1.8454e-01,\n",
            "          6.2486e-01,  6.2169e-01,  4.3269e-01,  1.4915e-01, -2.3333e-01,\n",
            "         -3.0115e-01,  8.3566e-01,  3.1677e-01,  4.0050e-01, -2.0646e-01,\n",
            "         -7.0751e-02,  9.4726e-01,  7.1175e-01, -7.3535e-01, -1.5688e-01,\n",
            "          2.7373e-01,  4.9350e-01,  4.4127e-01, -3.6898e-01, -2.0092e-01,\n",
            "          2.6750e-02,  2.0701e-01, -7.3916e-01, -3.8203e-01, -4.3951e-01,\n",
            "          3.9982e-01,  3.8014e-01, -3.0812e-01, -5.6724e-01,  5.3868e-01,\n",
            "          7.9785e-01, -8.8996e-03,  4.4100e-01, -2.3545e-01,  1.3684e-02,\n",
            "          4.6951e-01,  3.8988e-01, -1.2394e+00,  5.2804e-01,  6.1694e-01,\n",
            "          7.3898e-01, -3.6184e-01,  1.5786e-01, -2.2015e-01, -5.0882e-01,\n",
            "          4.4181e-02, -1.3050e-01, -6.4178e-02, -8.6900e-02,  1.4790e-01,\n",
            "         -4.3928e-01,  5.4313e-01, -1.1839e-01,  8.0062e-01,  7.1979e-01,\n",
            "          3.3259e-01,  4.4321e-01,  5.1688e-02,  3.1993e-01,  2.2472e-01,\n",
            "          1.9407e-01,  1.1366e+00,  1.2658e+00,  3.8673e-01, -1.2394e+00,\n",
            "         -7.9585e-01, -4.7444e-01,  5.1253e-02, -1.4477e-01, -2.4467e-01,\n",
            "          8.4459e-02,  1.0543e-01,  4.7379e-01, -3.6575e-01,  1.1787e+00,\n",
            "          1.9383e-01, -1.9046e-01,  3.4963e-01,  3.2270e-03, -4.9298e-01,\n",
            "          1.1516e-01,  2.0576e-02, -9.3901e-01, -5.8393e-02,  1.6790e-01,\n",
            "         -9.1502e-02,  1.5631e-01, -5.5637e-01, -1.1752e+00, -3.2214e-01,\n",
            "         -2.6459e-01,  3.8481e-01, -5.2931e-01,  1.7214e-01,  1.3128e-01,\n",
            "          4.5132e-01,  5.4170e-01,  7.3229e-01,  1.8500e+00, -9.4930e-01,\n",
            "         -6.5622e-01,  1.0604e-01, -2.3808e-02,  2.1471e-01,  6.9995e-01,\n",
            "          3.5841e-01,  3.3099e-01, -5.0629e-01, -2.9537e-01,  2.4245e-01,\n",
            "         -1.6848e-01, -1.3459e+00,  3.0650e-01,  1.7945e-01,  4.3826e-01,\n",
            "         -3.0676e-01,  9.3986e-01, -3.8352e-01, -1.0645e+00, -4.2365e-01,\n",
            "          7.8248e-01,  7.2914e-01,  2.9612e-01, -8.6605e-01, -8.6038e-01,\n",
            "         -1.2268e-01, -1.9872e-01,  5.6558e-01,  1.0408e-01,  7.1544e-01,\n",
            "          4.1386e-02,  4.1231e-01, -1.5803e+00,  8.5959e-04,  7.5876e-01,\n",
            "          9.9172e-01, -9.6244e-02, -2.5102e-01,  1.5918e-01, -1.0451e+00,\n",
            "          6.7412e-01, -2.2142e-01, -1.2554e-01,  2.3321e-01, -2.3362e-01,\n",
            "         -2.5845e-02, -1.2356e+00, -1.3935e-01,  6.8354e-01,  1.9909e-01,\n",
            "          7.1177e-01,  1.0313e-01,  4.0351e-01,  3.9751e-01, -8.3531e-01,\n",
            "         -5.9082e-02, -7.9898e-01, -1.3746e+00, -3.7808e-01,  8.9556e-01,\n",
            "          2.9649e-01, -1.0266e-01,  3.2666e-02, -3.8341e-01,  5.6894e-01,\n",
            "          8.3525e-01, -3.8647e-01,  8.7248e-01,  3.2406e-01, -1.7720e-01,\n",
            "          1.7084e-02, -2.6001e-01, -5.7807e-01,  9.0438e-01,  6.0869e-01,\n",
            "         -2.4251e-01,  6.9458e-02, -5.0899e-01,  3.7600e-01,  4.9458e-01,\n",
            "          7.3476e-01, -1.3801e-01, -1.8997e-01, -2.4093e-01,  7.8942e-01,\n",
            "         -4.6747e-01, -1.6157e-01,  7.6482e-02,  4.6321e-01,  1.3448e-01,\n",
            "         -3.5576e-01, -5.3042e-01,  9.7271e-01,  8.0365e-01,  8.3587e-01,\n",
            "         -6.3342e-01,  3.0118e-01, -3.5930e-02, -4.1073e-01, -2.2900e-01,\n",
            "          2.4449e-01, -5.4664e-02, -3.1388e-01,  6.5913e-02,  4.0892e-01,\n",
            "          3.4056e-02, -9.3980e-01, -1.0994e+00,  3.9269e-01,  8.7275e-01,\n",
            "          1.5646e-01, -2.4519e-01, -2.5187e-01, -1.2952e-01, -1.6452e-01,\n",
            "         -7.1306e-01, -9.6839e-01, -9.3451e-01,  4.0715e-01,  5.3814e-01,\n",
            "          3.6124e-01,  1.6014e-01, -1.7063e-01, -4.2047e-01, -1.4882e-01,\n",
            "         -4.2409e-01, -9.1331e-01,  8.1404e-01,  4.3652e-01,  4.0580e-01,\n",
            "          8.8226e-01,  1.9050e-01, -6.0388e-01, -1.2952e-01, -7.9423e-01,\n",
            "         -6.2215e-01,  2.5735e-01,  6.4494e-02, -1.9475e-01,  1.4681e-01,\n",
            "         -9.5049e-01,  1.0146e+00, -3.6949e-01, -7.0582e-01,  5.3380e-01,\n",
            "          2.6139e-01, -4.7154e-01, -8.1205e-01,  4.6470e-01, -5.6212e-02,\n",
            "         -5.2960e-01, -8.0317e-02, -6.2692e-01, -5.6583e-01,  8.2185e-01,\n",
            "          1.1767e-01, -3.4058e-01,  2.3971e-01, -5.1819e-02, -2.7101e-01,\n",
            "          2.0358e-01,  1.9956e-01, -3.4433e-01, -3.5439e-01, -7.9091e-01,\n",
            "         -3.5220e-01, -5.8465e-01,  1.2437e+00, -1.7972e-01, -2.4417e-01,\n",
            "          1.8633e-01,  5.1562e-01, -4.0066e-01,  2.7440e-01,  2.9021e-01,\n",
            "         -2.4580e-01, -6.9433e-01,  6.5266e-01, -7.5263e-01, -2.0142e-01,\n",
            "          3.5820e-01, -1.1505e-01,  6.0372e-01,  1.5642e-01,  3.1472e-01,\n",
            "         -1.7956e-01,  4.2053e-01,  5.6508e-02, -2.0109e-01,  3.6937e-01,\n",
            "         -3.1941e-01, -1.6642e-02, -1.6892e-01,  8.4526e-01, -3.7586e-02]],\n",
            "       grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gbRXSAqzPwrI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}