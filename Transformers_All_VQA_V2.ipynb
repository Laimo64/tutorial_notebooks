{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "df39a0d74b8d4e21bf56f1f1b642e6c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_277ef96bff8e4fcdac7067667c313d57",
              "IPY_MODEL_1b4e85fe1420492daec050aba8c57648",
              "IPY_MODEL_8cca02de6cb14e65a0c291f9aba9fbb5"
            ],
            "layout": "IPY_MODEL_036993c63d7f4dd8ab79e050276de70d"
          }
        },
        "277ef96bff8e4fcdac7067667c313d57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d73a28d5a184e19bc71f4e694bb2b52",
            "placeholder": "​",
            "style": "IPY_MODEL_50f33d5decda45a3b69f9ed7dd9e5404",
            "value": "100%"
          }
        },
        "1b4e85fe1420492daec050aba8c57648": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1bc653cb7c2e45c4a68ed1b1b31aaa77",
            "max": 46830571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b64c819cb9744a8d9cac83beca79fe1e",
            "value": 46830571
          }
        },
        "8cca02de6cb14e65a0c291f9aba9fbb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1997f89c778b455898d3e33e79802ee4",
            "placeholder": "​",
            "style": "IPY_MODEL_3201ee6b44144b59b0080e79a9f5e337",
            "value": " 44.7M/44.7M [00:00&lt;00:00, 72.2MB/s]"
          }
        },
        "036993c63d7f4dd8ab79e050276de70d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d73a28d5a184e19bc71f4e694bb2b52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50f33d5decda45a3b69f9ed7dd9e5404": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1bc653cb7c2e45c4a68ed1b1b31aaa77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b64c819cb9744a8d9cac83beca79fe1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1997f89c778b455898d3e33e79802ee4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3201ee6b44144b59b0080e79a9f5e337": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "df4f153db2874fe1bad6471df4cd0f44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7575197d85514b408add5d2132fcd7d5",
              "IPY_MODEL_accb87559fe04438936a8afd6c576cd8",
              "IPY_MODEL_2eeab72d3e254a92ac18921f7d20f1a7"
            ],
            "layout": "IPY_MODEL_4414714d537840b5a3090a635ac7f19d"
          }
        },
        "7575197d85514b408add5d2132fcd7d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87d94404f58e4241ab3a81a547286238",
            "placeholder": "​",
            "style": "IPY_MODEL_efb5aa0968564031984ed5e588dac15e",
            "value": "Downloading config.json: 100%"
          }
        },
        "accb87559fe04438936a8afd6c576cd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b37b5625078b40b8a72c00eb87343d56",
            "max": 631,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cbf1cf8204ec41689b677eb14da9b057",
            "value": 631
          }
        },
        "2eeab72d3e254a92ac18921f7d20f1a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_590d7b83004c4bd1a9d7755963f4d492",
            "placeholder": "​",
            "style": "IPY_MODEL_0d766871c5b14cabb3ffc833b3ff492a",
            "value": " 631/631 [00:00&lt;00:00, 10.9kB/s]"
          }
        },
        "4414714d537840b5a3090a635ac7f19d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87d94404f58e4241ab3a81a547286238": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "efb5aa0968564031984ed5e588dac15e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b37b5625078b40b8a72c00eb87343d56": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cbf1cf8204ec41689b677eb14da9b057": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "590d7b83004c4bd1a9d7755963f4d492": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d766871c5b14cabb3ffc833b3ff492a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/tutorial_notebooks/blob/main/Transformers_All_VQA_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HnAbtUBvpHZk"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers\n",
        "!pip -q install timm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from transformers import VisualBertModel, VisualBertConfig, BertTokenizerFast\n",
        "from PIL import Image\n",
        "import requests\n",
        "from torchvision.models import resnet18, resnet34, resnet101\n",
        "from torchvision import transforms\n",
        "from timm import create_model\n",
        "\n",
        "img_url = 'https://www.animalfunfacts.net/images/stories/pets/dogs/pembroke_welsh_corgi_l.jpg'\n",
        "img_raw = Image.open(requests.get(img_url, stream=True).raw)\n",
        "mean, std = torch.tensor([0.485, 0.456, 0.406]), torch.tensor([0.229, 0.224, 0.225])\n",
        "transform = transforms.Compose([transforms.Resize((224, 224)), \n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize(mean=mean, std=std)])\n",
        "img = transform(img_raw)[None]\n",
        "\n",
        "test_question = [\"Where is the dog?\"]\n",
        "bert_tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
        "inputs = bert_tokenizer(test_question, return_tensors=\"pt\", padding=\"max_length\",max_length=20,)\n"
      ],
      "metadata": {
        "id": "Fug7RdPopWRC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "VisualBERT (ResNet101)"
      ],
      "metadata": {
        "id": "HfDvrDa47r2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VisualBERT_VQA(nn.Module):\n",
        "    def __init__(self, num_labels=2):\n",
        "        super(VisualBERT_VQA, self).__init__()\n",
        "        self.visualbert = VisualBertModel.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n",
        "        self.cls = nn.Linear(768, num_labels)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        last_hidden_state = self.visualbert(**inputs).last_hidden_state #[1, 56, 768]\n",
        "\n",
        "        # Get the index of the last text token\n",
        "        index_to_gather = inputs['attention_mask'].sum(1) - 2  # as in original code 5\n",
        "        index_to_gather = (\n",
        "            index_to_gather.unsqueeze(-1).unsqueeze(-1).expand(index_to_gather.size(0), 1, last_hidden_state.size(-1))\n",
        "        ) # [b c hw]=[1, 1, 768]\n",
        "\n",
        "        pooled_output = torch.gather(last_hidden_state, 1, index_to_gather) # [1, 1, 768]\n",
        "        logits = self.cls(pooled_output).squeeze(1)\n",
        "        return logits\n",
        "\n",
        "model_visual_feat = resnet101(pretrained=True)\n",
        "model_visual_feat.avgpool = nn.Identity()\n",
        "model_visual_feat.fc = nn.Identity()\n",
        "model_visual_feat.eval()\n",
        "visual_embeds = model_visual_feat(img).view(-1, 49, 2048)\n",
        "visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long)\n",
        "visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float)\n",
        "\n",
        "inputs.update(\n",
        "    {\n",
        "        \"visual_embeds\": visual_embeds,\n",
        "        \"visual_token_type_ids\": visual_token_type_ids,\n",
        "        \"visual_attention_mask\": visual_attention_mask,\n",
        "    }\n",
        ")\n",
        "\n",
        "print('visual_embeds', visual_embeds.shape, 'Text:', inputs['input_ids'].shape)\n",
        "model = VisualBERT_VQA()\n",
        "model.eval()\n",
        "logits = model(inputs)\n",
        "pred_vqa = logits.argmax(-1)\n",
        "print('Logits:',logits, 'Prediction:', pred_vqa)  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofC-BJx421in",
        "outputId": "dbfbacc8-d8c3-4654-8891-0e1f341bde5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "visual_embeds torch.Size([1, 49, 2048]) Text: torch.Size([1, 20])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at uclanlp/visualbert-vqa-coco-pre were not used when initializing VisualBertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing VisualBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing VisualBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits: tensor([[-0.4455,  0.3589]], grad_fn=<SqueezeBackward1>) Prediction: tensor([1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "VisualBERT (ResNet34)"
      ],
      "metadata": {
        "id": "0ZWZfrMd7y9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VisualBERT_VQA(nn.Module):\n",
        "    def __init__(self, num_labels=2):\n",
        "        super(VisualBERT_VQA, self).__init__()\n",
        "        self.config = VisualBertConfig.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n",
        "        self.config.visual_embedding_dim = 512\n",
        "        self.visualbert = VisualBertModel(config=self.config)#.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n",
        "        #self.embeddings = self.visual_bert.embeddings\n",
        "        self.cls = nn.Linear(768, num_labels)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        last_hidden_state = self.visualbert(**inputs).last_hidden_state #[1, 56, 768]\n",
        "\n",
        "        # Get the index of the last text token\n",
        "        index_to_gather = inputs['attention_mask'].sum(1) - 2  # as in original code 5\n",
        "        index_to_gather = (\n",
        "            index_to_gather.unsqueeze(-1).unsqueeze(-1).expand(index_to_gather.size(0), 1, last_hidden_state.size(-1))\n",
        "        ) # [b c hw]=[1, 1, 768]\n",
        "        pooled_output = torch.gather(last_hidden_state, 1, index_to_gather) # [1, 1, 768]\n",
        "        logits = self.cls(pooled_output).squeeze(1)\n",
        "        return logits\n",
        "\n",
        "model_visual_feat = resnet34(pretrained=True)\n",
        "model_visual_feat.avgpool = nn.Identity()\n",
        "model_visual_feat.fc = nn.Identity()\n",
        "model_visual_feat.eval()\n",
        "visual_embeds = model_visual_feat(img).view(-1, 49, 512)\n",
        "visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long)\n",
        "visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float)\n",
        "inputs.update(\n",
        "    {\n",
        "        \"visual_embeds\": visual_embeds,\n",
        "        \"visual_token_type_ids\": visual_token_type_ids,\n",
        "        \"visual_attention_mask\": visual_attention_mask,\n",
        "    }\n",
        ")\n",
        "\n",
        "print('visual_embeds', visual_embeds.shape, 'Text:', inputs['input_ids'].shape)\n",
        "\n",
        "model = VisualBERT_VQA()\n",
        "model.eval()\n",
        "logits = model(inputs)\n",
        "pred_vqa = logits.argmax(-1)\n",
        "print('Logits:',logits, 'Prediction:', pred_vqa)        \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZvDd2GcsaF1",
        "outputId": "fa561909-0011-4cf3-fc0f-c1e2474ac606"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "visual_embeds torch.Size([1, 49, 512]) Text: torch.Size([1, 20])\n",
            "self.visualbert.config.visual_embedding_dim: 512\n",
            "tensor([5]) 1 768\n",
            "torch.Size([1, 1, 768])\n",
            "Logits: tensor([[ 0.5161, -0.5943]], grad_fn=<SqueezeBackward1>) Prediction: tensor([0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ViT_VQA(ResNet18)\n",
        "\n",
        "(BertTokenizerFast = AutoTokenizer)"
      ],
      "metadata": {
        "id": "L_t1Wgkr8CXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from timm import create_model\n",
        "class ViT_VQA(nn.Module):\n",
        "    def __init__(self, num_labels=2):\n",
        "        super(ViT_VQA, self).__init__()\n",
        "        self.config = VisualBertConfig.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n",
        "        self.config.visual_embedding_dim = 512\n",
        "        self.visualbert = VisualBertModel(config=self.config)\n",
        "        self.embeddings = self.visualbert.embeddings\n",
        "\n",
        "        self.vit = create_model(\"vit_base_patch16_224\", pretrained=True)\n",
        "        self.cls = nn.Linear(768, num_labels)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embedding_output = self.embeddings(\n",
        "            input_ids=inputs['input_ids'],\n",
        "            token_type_ids=inputs['token_type_ids'],\n",
        "            position_ids=None,\n",
        "            inputs_embeds=None,\n",
        "            visual_embeds=inputs['visual_embeds'],\n",
        "            visual_token_type_ids=inputs['visual_token_type_ids'],\n",
        "            image_text_alignment=None,\n",
        "        ) #[1, 56, 768]\n",
        "        \n",
        "        x = self.vit.blocks(embedding_output)\n",
        "        x = self.vit.norm(x)\n",
        "        x = x.mean(dim=1)\n",
        "        logits = self.cls(x)\n",
        "        return logits\n",
        "\n",
        "model_visual_feat = resnet18(pretrained=True)\n",
        "model_visual_feat.avgpool = nn.Identity()\n",
        "model_visual_feat.fc = nn.Identity()\n",
        "model_visual_feat.eval()\n",
        "visual_embeds = model_visual_feat(img).view(-1, 49, 512)\n",
        "visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long)\n",
        "visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float)\n",
        "inputs.update(\n",
        "    {\n",
        "        \"visual_embeds\": visual_embeds,\n",
        "        \"visual_token_type_ids\": visual_token_type_ids,\n",
        "        \"visual_attention_mask\": visual_attention_mask,\n",
        "    }\n",
        ")\n",
        "\n",
        "print('visual_embeds', visual_embeds.shape, 'Text:', inputs['input_ids'].shape)\n",
        "\n",
        "model = ViT_VQA(num_labels=2)\n",
        "model.eval()\n",
        "logits = model(inputs)\n",
        "pred_vqa = logits.argmax(-1)\n",
        "print('Logits:',logits, 'Prediction:', pred_vqa) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226,
          "referenced_widgets": [
            "df39a0d74b8d4e21bf56f1f1b642e6c7",
            "277ef96bff8e4fcdac7067667c313d57",
            "1b4e85fe1420492daec050aba8c57648",
            "8cca02de6cb14e65a0c291f9aba9fbb5",
            "036993c63d7f4dd8ab79e050276de70d",
            "8d73a28d5a184e19bc71f4e694bb2b52",
            "50f33d5decda45a3b69f9ed7dd9e5404",
            "1bc653cb7c2e45c4a68ed1b1b31aaa77",
            "b64c819cb9744a8d9cac83beca79fe1e",
            "1997f89c778b455898d3e33e79802ee4",
            "3201ee6b44144b59b0080e79a9f5e337",
            "df4f153db2874fe1bad6471df4cd0f44",
            "7575197d85514b408add5d2132fcd7d5",
            "accb87559fe04438936a8afd6c576cd8",
            "2eeab72d3e254a92ac18921f7d20f1a7",
            "4414714d537840b5a3090a635ac7f19d",
            "87d94404f58e4241ab3a81a547286238",
            "efb5aa0968564031984ed5e588dac15e",
            "b37b5625078b40b8a72c00eb87343d56",
            "cbf1cf8204ec41689b677eb14da9b057",
            "590d7b83004c4bd1a9d7755963f4d492",
            "0d766871c5b14cabb3ffc833b3ff492a"
          ]
        },
        "id": "0z5BDwfv-hy8",
        "outputId": "fe812db9-05cc-4b1b-d681-2260cb0222f8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/44.7M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "df39a0d74b8d4e21bf56f1f1b642e6c7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "visual_embeds torch.Size([1, 49, 512]) Text: torch.Size([1, 20])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading config.json:   0%|          | 0.00/631 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "df4f153db2874fe1bad6471df4cd0f44"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits: tensor([[ 1.0024, -1.4413]], grad_fn=<AddmmBackward0>) Prediction: tensor([0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DeiT_VQA(ResNet18)\n",
        "DeiT: Data-efficient Image Transformers - https://arxiv.org/abs/2012.12877"
      ],
      "metadata": {
        "id": "uFYkgmXZdQoE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from timm import create_model\n",
        "class DeiT_VQA(nn.Module):\n",
        "    def __init__(self, num_labels=2):\n",
        "        super(DeiT_VQA, self).__init__()\n",
        "        self.config = VisualBertConfig.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n",
        "        self.config.visual_embedding_dim = 512\n",
        "        self.visualbert = VisualBertModel(config=self.config)\n",
        "        self.embeddings = self.visualbert.embeddings\n",
        "\n",
        "        self.deit = create_model(\"deit_base_patch16_224\", pretrained=True)\n",
        "        self.cls = nn.Linear(768, num_labels)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embedding_output = self.embeddings(\n",
        "            input_ids=inputs['input_ids'],\n",
        "            token_type_ids=inputs['token_type_ids'],\n",
        "            position_ids=None,\n",
        "            inputs_embeds=None,\n",
        "            visual_embeds=inputs['visual_embeds'],\n",
        "            visual_token_type_ids=inputs['visual_token_type_ids'],\n",
        "            image_text_alignment=None,\n",
        "        ) #[1, 56, 768]\n",
        "        x = self.deit.blocks(embedding_output)\n",
        "        x = self.deit.norm(x)\n",
        "        x = x.mean(dim=1)\n",
        "        logits = self.cls(x)\n",
        "        return logits\n",
        "\n",
        "model_visual_feat = resnet18(pretrained=True)\n",
        "model_visual_feat.avgpool = nn.Identity()\n",
        "model_visual_feat.fc = nn.Identity()\n",
        "model_visual_feat.eval()\n",
        "visual_embeds = model_visual_feat(img).view(-1, 49, 512)\n",
        "visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long)\n",
        "visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float)\n",
        "inputs.update(\n",
        "    {\n",
        "        \"visual_embeds\": visual_embeds,\n",
        "        \"visual_token_type_ids\": visual_token_type_ids,\n",
        "        \"visual_attention_mask\": visual_attention_mask,\n",
        "    }\n",
        ")\n",
        "\n",
        "print('visual_embeds', visual_embeds.shape, 'Text:', inputs['input_ids'].shape)\n",
        "\n",
        "model = DeiT_VQA(num_labels=2)\n",
        "model.eval()\n",
        "logits = model(inputs)\n",
        "pred_vqa = logits.argmax(-1)\n",
        "print('Logits:',logits, 'Prediction:', pred_vqa) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euLURYsrdQAt",
        "outputId": "13e9c4ae-c94b-47d3-ba8f-9e3039009592"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "visual_embeds torch.Size([1, 49, 512]) Text: torch.Size([1, 20])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth\" to /root/.cache/torch/hub/checkpoints/deit_base_patch16_224-b5f2ef4d.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits: tensor([[-0.4111, -0.2708]], grad_fn=<AddmmBackward0>) Prediction: tensor([1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CaiT_VQA(ResNet18)\n",
        "CaiT: Class-Attention in Image Transformers (https://arxiv.org/abs/2103.17239)"
      ],
      "metadata": {
        "id": "AHvBNEF3Dbyy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from timm import create_model\n",
        "class CaiT_VQA(nn.Module):\n",
        "    def __init__(self, num_labels=2):\n",
        "        super(CaiT_VQA, self).__init__()\n",
        "        self.config = VisualBertConfig.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n",
        "        self.config.visual_embedding_dim = 512\n",
        "        self.config.hidden_size = 192\n",
        "        self.visualbert = VisualBertModel(config=self.config)\n",
        "        self.embeddings = self.visualbert.embeddings\n",
        "\n",
        "        self.cait = create_model(\"cait_xxs24_224\", pretrained=True)\n",
        "        self.cls = nn.Linear(192, num_labels)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embedding_output = self.embeddings(\n",
        "            input_ids=inputs['input_ids'],\n",
        "            token_type_ids=inputs['token_type_ids'],\n",
        "            position_ids=None,\n",
        "            inputs_embeds=None,\n",
        "            visual_embeds=inputs['visual_embeds'],\n",
        "            visual_token_type_ids=inputs['visual_token_type_ids'],\n",
        "            image_text_alignment=None,\n",
        "        ) #[1, 69, 768]\n",
        "        x = self.cait.blocks(embedding_output)\n",
        "        cls_tokens = self.cait.cls_token.expand(x.shape[0], -1, -1)\n",
        "        for i, blk in enumerate(self.cait.blocks_token_only):\n",
        "            cls_tokens = blk(x, cls_tokens)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = self.cait.norm(x)\n",
        "        x = x.mean(dim=1)\n",
        "        #x = x[:, 0]\n",
        "        logits = self.cls(x)\n",
        "        return logits\n",
        "\n",
        "model_visual_feat = resnet18(pretrained=True)\n",
        "model_visual_feat.avgpool = nn.Identity()\n",
        "model_visual_feat.fc = nn.Identity()\n",
        "model_visual_feat.eval()\n",
        "visual_embeds = model_visual_feat(img).view(-1, 49, 512)\n",
        "visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long)\n",
        "visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float)\n",
        "inputs.update(\n",
        "    {\n",
        "        \"visual_embeds\": visual_embeds,\n",
        "        \"visual_token_type_ids\": visual_token_type_ids,\n",
        "        \"visual_attention_mask\": visual_attention_mask,\n",
        "    }\n",
        ")\n",
        "\n",
        "print('visual_embeds', visual_embeds.shape, 'Text:', inputs['input_ids'].shape)\n",
        "\n",
        "model = CaiT_VQA(num_labels=2)\n",
        "model.eval()\n",
        "logits = model(inputs)\n",
        "pred_vqa = logits.argmax(-1)\n",
        "print('Logits:',logits, 'Prediction:', pred_vqa) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJTQ7KfeDl6A",
        "outputId": "d3bece68-0f89-4934-96f5-c11444fc0fa1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "visual_embeds torch.Size([1, 49, 512]) Text: torch.Size([1, 20])\n",
            "Logits: tensor([[-0.2232,  1.4499]], grad_fn=<AddmmBackward0>) Prediction: tensor([1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Swin-Transformer_VQA()"
      ],
      "metadata": {
        "id": "WWn9EtjVJ2yd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from transformers import VisualBertModel, VisualBertConfig\n",
        "\n",
        "# Initializing a VisualBERT visualbert-vqa-coco-pre style configuration\n",
        "config = VisualBertConfig.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n",
        "class VisualBertEmbeddings(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n",
        "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
        "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
        "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)))\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        inputs_embeds=None,\n",
        "        visual_embeds=None,\n",
        "        visual_token_type_ids=None,\n",
        "        image_text_alignment=None,\n",
        "    ):\n",
        "\n",
        "        input_shape = input_ids.size()\n",
        "        seq_length = input_shape[1]\n",
        "        if position_ids is None:\n",
        "            position_ids = self.position_ids[:, :seq_length]\n",
        "\n",
        "        if inputs_embeds is None:\n",
        "            inputs_embeds = self.word_embeddings(input_ids)\n",
        "\n",
        "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
        "        embeddings = inputs_embeds + token_type_embeddings\n",
        "\n",
        "        # Absolute Position Embeddings\n",
        "        position_embeddings = self.position_embeddings(position_ids)\n",
        "        embeddings += position_embeddings\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        return embeddings\n",
        "\n",
        "class SwinTranformer_VQA(nn.Module):\n",
        "    def __init__(self, num_labels=2):\n",
        "        super(SwinTranformer_VQA, self).__init__()\n",
        "        self.config = VisualBertConfig.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n",
        "        #self.config.visual_embedding_dim = 128\n",
        "        self.config.hidden_size = 1024 \n",
        "        self.embeddings = VisualBertEmbeddings(config=self.config)\n",
        "\n",
        "        self.swintran = create_model(\"swin_base_patch4_window7_224\", pretrained=True)\n",
        "        self.cls = nn.Linear(1024, num_labels)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embedding_output = self.embeddings(\n",
        "            input_ids=inputs['input_ids'],\n",
        "            token_type_ids=inputs['token_type_ids'],\n",
        "            position_ids=None,\n",
        "            inputs_embeds=None,\n",
        "            visual_embeds=inputs['visual_embeds'],\n",
        "            visual_token_type_ids=None,\n",
        "            image_text_alignment=None,\n",
        "        ) #[1, 69, 768]\n",
        "        x = self.swintran.patch_embed(inputs['visual_embeds'])\n",
        "        x = self.swintran.layers(x)\n",
        "        x = torch.cat((x, embedding_output), dim=1)\n",
        "        x = self.swintran.norm(x)\n",
        "        x = x.mean(dim=1)\n",
        "        logits = self.cls(x)\n",
        "        return logits\n",
        "\n",
        "inputs.update(\n",
        "    {\n",
        "        \"visual_embeds\": img,\n",
        "        \"visual_token_type_ids\": None,\n",
        "        \"visual_attention_mask\": None,\n",
        "    }\n",
        ")\n",
        "\n",
        "print( 'Text Embedding:', inputs['input_ids'].shape)\n",
        "\n",
        "model = SwinTranformer_VQA(num_labels=2)\n",
        "model.eval()\n",
        "logits = model(inputs)\n",
        "pred_vqa = logits.argmax(-1)\n",
        "print('Logits:',logits, 'Prediction:', pred_vqa) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_XpabovKIHM",
        "outputId": "e220f523-dc72-4b1a-b269-18f7cbbce14d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "visual_embeds torch.Size([1, 3136, 128]) Text: torch.Size([1, 20])\n",
            "Logits: tensor([[0.0392, 0.5809]], grad_fn=<AddmmBackward0>) Prediction: tensor([1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ablation on All Transformers for Classification"
      ],
      "metadata": {
        "id": "MHBes80ODNTH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Swin-Transformer"
      ],
      "metadata": {
        "id": "onY4fSyzXP2m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "from timm import create_model\n",
        "\n",
        "\n",
        "class SwinTranformer_Features(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SwinTranformer_Features, self).__init__()\n",
        "        self.swintran = create_model(\"swin_base_patch4_window7_224\", pretrained=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(x.shape)\n",
        "        x = self.swintran.patch_embed(x) # [1, 3136, 128] # vit: [1, 196, 768]\n",
        "        print(x.shape)\n",
        "        x = self.swintran.layers(x)#[1, 49, 1024]\n",
        "        print(x.shape)\n",
        "        x = self.swintran.norm(x)#[1, 49, 1024] # [1, 197, 768]\n",
        "        print(x.shape)\n",
        "        x = x.mean(dim=1)#[1, 1024]\n",
        "        print(x.shape)\n",
        "        logits = self.swintran.head(x)#\n",
        "        return logits\n",
        "\n",
        "model = SwinTranformer_Features()\n",
        "model.eval()\n",
        "logits = model(img)\n",
        "pred = logits.argmax(dim=1).item()\n",
        "print('prediction:', int(torch.argmax(logits)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DEK1j9ZKYT4",
        "outputId": "a7f311eb-82e5-4038-e6c2-1ac9d4064bec"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3136, 128])\n",
            "torch.Size([1, 49, 1024])\n",
            "torch.Size([1, 49, 1024])\n",
            "torch.Size([1, 1024])\n",
            "prediction: 263\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ViT"
      ],
      "metadata": {
        "id": "AAwn_Mj-XMIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "from timm import create_model\n",
        "\n",
        "\n",
        "class ViT_Features(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ViT_Features, self).__init__()\n",
        "        model_name = \"vit_base_patch16_224\"\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.vit = create_model(model_name, pretrained=True).to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        patches = self.vit.patch_embed(x) # [1, 196, 768]\n",
        "        pos_embed = self.vit.pos_embed # [1, 197, 768]\n",
        "        print(patches.shape, pos_embed.shape)\n",
        "        print(torch.cat((self.vit.cls_token, patches), dim=1).shape)\n",
        "        x = torch.cat((self.vit.cls_token, patches), dim=1) + pos_embed #[1, 197, 768]\n",
        "        x = self.vit.blocks(x)\n",
        "        print('self.vit.blocks(x):', x.shape)\n",
        "        # for i, blk in enumerate(self.vit.blocks):\n",
        "        #     x = blk(x)\n",
        "        x = self.vit.norm(x)\n",
        "        x = x.mean(dim=1)\n",
        "        logits = self.vit.head(x)\n",
        "        return logits\n",
        "\n",
        "model = ViT_Features()\n",
        "model.eval()\n",
        "logits = model(img)\n",
        "pred = logits.argmax(dim=1).item()\n",
        "print('prediction:', int(torch.argmax(logits)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pP1ww_-p8_0Z",
        "outputId": "d5c56740-a836-44c3-97f8-4451374548e3"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 196, 768]) torch.Size([1, 197, 768])\n",
            "torch.Size([1, 197, 768])\n",
            "self.vit.blocks(x): torch.Size([1, 197, 768])\n",
            "prediction: 263\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DeiT: Data-efficient Image Transformers - https://arxiv.org/abs/2012.12877"
      ],
      "metadata": {
        "id": "gYbvH5p6kH1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "from timm import create_model\n",
        "\n",
        "\n",
        "class DeiT_Features(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeiT_Features, self).__init__()\n",
        "        self.deit = create_model(\"deit_base_patch16_224\", pretrained=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        patches = self.deit.patch_embed(x) # [1, 196, 768]\n",
        "        pos_embed = self.deit.pos_embed # [1, 197, 768]\n",
        "        x = torch.cat((self.deit.cls_token, patches), dim=1) + pos_embed #[1, 197, 768]\n",
        "        x = self.deit.blocks(x)\n",
        "        x = self.deit.norm(x)\n",
        "        x = x.mean(dim=1)\n",
        "        logits = self.deit.head(x)\n",
        "        return logits\n",
        "\n",
        "model = DeiT_Features()\n",
        "model.eval()\n",
        "logits = model(img)\n",
        "pred = logits.argmax(dim=1).item()\n",
        "print('prediction:', int(torch.argmax(logits)))"
      ],
      "metadata": {
        "id": "JOYHhMJv9O_l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41f239a6-31db-41eb-ea48-b34a6143651b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth\" to /root/.cache/torch/hub/checkpoints/deit_base_patch16_224-b5f2ef4d.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction: 263\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CaiT: Class-Attention in Image Transformers (https://arxiv.org/abs/2103.17239)"
      ],
      "metadata": {
        "id": "KCAAOcprkx9p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "from timm import create_model\n",
        "\n",
        "\n",
        "class CaiT_Features(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CaiT_Features, self).__init__()\n",
        "        self.cait = create_model(\"cait_xxs24_224\", pretrained=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        patches = self.cait.patch_embed(x) # [1, 196, 768]\n",
        "        pos_embed = self.cait.pos_embed # [1, 197, 768]\n",
        "        x = patches + pos_embed #[1, 196, 192]\n",
        "        print(x.shape)\n",
        "        x = self.cait.blocks(x)\n",
        "        cls_tokens = self.cait.cls_token.expand(x.shape[0], -1, -1)\n",
        "        for i, blk in enumerate(self.cait.blocks_token_only):\n",
        "            cls_tokens = blk(x, cls_tokens)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = self.cait.norm(x)\n",
        "        #x = x.mean(dim=1)\n",
        "        x = x[:, 0]\n",
        "        logits = self.cait.head(x)\n",
        "        return logits\n",
        "\n",
        "model = CaiT_Features()\n",
        "model.eval()\n",
        "logits = model(img)\n",
        "pred = logits.argmax(dim=1).item()\n",
        "print('prediction:', int(torch.argmax(logits)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4m8QJpjmkZnd",
        "outputId": "129ed6d0-baf4-49c4-c07c-27bfd3143531"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 196, 192])\n",
            "prediction: 263\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BeiT: BERT Pre-Training of Image Transformers (https://arxiv.org/abs/2106.08254)"
      ],
      "metadata": {
        "id": "S_NRgIA5kPcl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "from timm import create_model\n",
        "\n",
        "\n",
        "class BeiT_Features(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BeiT_Features, self).__init__()\n",
        "        self.beit = create_model(\"beit_base_patch16_224\", pretrained=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.beit.patch_embed(x) # [1, 196, 768]\n",
        "        x = torch.cat((self.beit.cls_token.expand(x.shape[0], -1, -1), x), dim=1)\n",
        "        rel_pos_bias = self.beit.rel_pos_bias() if self.beit.rel_pos_bias is not None else None\n",
        "        for blk in self.beit.blocks:\n",
        "            x = blk(x, shared_rel_pos_bias=rel_pos_bias)\n",
        "        x = self.beit.norm(x)\n",
        "        x = x[:, 1:].mean(dim=1)\n",
        "        x = self.beit.fc_norm(x)\n",
        "        logits = self.beit.head(x)\n",
        "        return logits\n",
        "\n",
        "model = BeiT_Features()\n",
        "model.eval()\n",
        "logits = model(img)\n",
        "pred = logits.argmax(dim=1).item()\n",
        "print('prediction:', int(torch.argmax(logits)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRSJSKhbju6L",
        "outputId": "9db6f11c-5248-42f3-8af4-95bc957314ab"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction: 129\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CoaT: Co-Scale Conv-Attentional Image Transformers - https://arxiv.org/abs/2104.06399"
      ],
      "metadata": {
        "id": "oTcgfDtFT7oC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "from timm import create_model\n",
        "\n",
        "\n",
        "class CoaT_Features(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CoaT_Features, self).__init__()\n",
        "        self.beit = create_model(\"coat_mini\", pretrained=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_feat = self.beit.forward_features(x)\n",
        "        x = self.beit.forward_head(x_feat)\n",
        "        return x\n",
        "\n",
        "model = CoaT_Features()\n",
        "model.eval()\n",
        "logits = model(img)\n",
        "pred = logits.argmax(dim=1).item()\n",
        "print('prediction:', int(torch.argmax(logits)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RyUiM2RUAt6",
        "outputId": "2c272c33-21e9-404e-8828-f6326c61c8fd"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction: 263\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification (et al. ICCV 2021)"
      ],
      "metadata": {
        "id": "h-Os31CuVEWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "from timm import create_model\n",
        "\n",
        "\n",
        "class CrossViT_Features(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CrossViT_Features, self).__init__()\n",
        "        self.crossvit = create_model(\"coat_mini\", pretrained=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_feat = self.crossvit.forward_features(x)\n",
        "        x = self.crossvit.forward_head(x_feat)\n",
        "        return x\n",
        "\n",
        "model = CrossViT_Features()\n",
        "model.eval()\n",
        "logits = model(img)\n",
        "pred = logits.argmax(dim=1).item()\n",
        "print('prediction:', int(torch.argmax(logits)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r84dLXNZVMlI",
        "outputId": "d5d586e3-0158-4dcc-dec0-6d89f9226caa"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction: 263\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ConvMixer: Patches Are All You Need? (https://arxiv.org/pdf/2201.09792.pdf)"
      ],
      "metadata": {
        "id": "k_o8_SDwQy9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "from timm import create_model\n",
        "\n",
        "\n",
        "class BeiT_Features(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BeiT_Features, self).__init__()\n",
        "        self.beit = create_model(\"convmixer_768_32\", pretrained=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.beit.stem(x)\n",
        "        x = self.beit.blocks(x)\n",
        "        x = self.beit.pooling(x)\n",
        "        logits = self.beit.head(x)\n",
        "        return logits\n",
        "\n",
        "model = BeiT_Features()\n",
        "model.eval()\n",
        "logits = model(img)\n",
        "pred = logits.argmax(dim=1).item()\n",
        "print('prediction:', int(torch.argmax(logits)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LP6Rtq8R1Px",
        "outputId": "ae918d37-9d90-46bf-8964-a7555ae1178d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction: 263\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ConvNeXt: A ConvNet for the 2020s - https://arxiv.org/pdf/2201.03545.pdf"
      ],
      "metadata": {
        "id": "UPAO0pN1SXt-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "from timm import create_model\n",
        "\n",
        "\n",
        "class ConvNeXt_Features(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvNeXt_Features, self).__init__()\n",
        "        self.convnext = create_model(\"convnext_base\", pretrained=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.convnext.stem(x)\n",
        "        x = self.convnext.stages(x)\n",
        "        x = self.convnext.norm_pre(x)\n",
        "        x = self.convnext.head.global_pool(x)\n",
        "        x = self.convnext.head.norm(x)\n",
        "        x = self.convnext.head.flatten(x)\n",
        "        x = self.convnext.head.drop(x)\n",
        "        logits = self.convnext.head.fc(x)\n",
        "        return logits\n",
        "\n",
        "model = ConvNeXt_Features()\n",
        "model.eval()\n",
        "logits = model(img)\n",
        "pred = logits.argmax(dim=1).item()\n",
        "print('prediction:', int(torch.argmax(logits)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1Ox5LApSnQO",
        "outputId": "821be17f-b117-4e3e-bf1a-a18f73d15f76"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction: 263\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model(\"crossvit_base_240\", pretrained=True)\n",
        "model.eval()\n",
        "logits = model(img)\n",
        "pred = logits.argmax(dim=1).item()\n",
        "print(pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HffKfNLDPrgE",
        "outputId": "a58fdbc2-6739-4439-f2a8-7876d1ff49ef"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/IBM/CrossViT/releases/download/weights-0.1/crossvit_base_224.pth\" to /root/.cache/torch/hub/checkpoints/crossvit_base_224.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "263\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(name, param.shape)"
      ],
      "metadata": {
        "id": "bHST_tUsRVs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding from Scratch"
      ],
      "metadata": {
        "id": "3VTfSbh53TFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from transformers import VisualBertModel, VisualBertConfig\n",
        "\n",
        "# Initializing a VisualBERT visualbert-vqa-coco-pre style configuration\n",
        "config = VisualBertConfig.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n",
        "class VisualBertEmbeddings(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n",
        "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
        "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
        "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)))\n",
        "\n",
        "        # For Visual Features\n",
        "        # Token type and position embedding for image features\n",
        "        self.visual_token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
        "        self.visual_position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
        "\n",
        "        if config.special_visual_initialize:\n",
        "            self.visual_token_type_embeddings.weight.data = nn.Parameter(\n",
        "                self.token_type_embeddings.weight.data.clone(), requires_grad=True\n",
        "            )\n",
        "            self.visual_position_embeddings.weight.data = nn.Parameter(\n",
        "                self.position_embeddings.weight.data.clone(), requires_grad=True\n",
        "            )\n",
        "\n",
        "        self.visual_projection = nn.Linear(config.visual_embedding_dim, config.hidden_size)\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        inputs_embeds=None,\n",
        "        visual_embeds=None,\n",
        "        visual_token_type_ids=None,\n",
        "        image_text_alignment=None,\n",
        "    ):\n",
        "\n",
        "        input_shape = input_ids.size()\n",
        "        seq_length = input_shape[1]\n",
        "        if position_ids is None:\n",
        "            position_ids = self.position_ids[:, :seq_length]\n",
        "\n",
        "        print('bef', input_ids.shape)\n",
        "        if inputs_embeds is None:\n",
        "            inputs_embeds = self.word_embeddings(input_ids)\n",
        "        \n",
        "        print('af', inputs_embeds.shape)\n",
        "\n",
        "        print('token_type_ids', token_type_ids.shape)\n",
        "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
        "        print('token_type_embeddings', token_type_embeddings.shape)\n",
        "        embeddings = inputs_embeds + token_type_embeddings\n",
        "\n",
        "        # Absolute Position Embeddings\n",
        "        position_embeddings = self.position_embeddings(position_ids)\n",
        "        embeddings += position_embeddings\n",
        "\n",
        "        print('before:',visual_embeds.shape)\n",
        "        visual_embeds = self.visual_projection(visual_embeds)\n",
        "        print('after:',visual_embeds.shape)\n",
        "        print('bef', visual_token_type_ids.shape)\n",
        "        visual_token_type_embeddings = self.visual_token_type_embeddings(visual_token_type_ids)\n",
        "        print('af', visual_token_type_embeddings.shape)\n",
        "        visual_position_ids = torch.zeros(\n",
        "            *visual_embeds.size()[:-1], dtype=torch.long, device=visual_embeds.device\n",
        "        )\n",
        "        print('bef',visual_position_ids.shape)\n",
        "        visual_position_embeddings = self.visual_position_embeddings(visual_position_ids)\n",
        "        print('bef',visual_position_embeddings.shape)\n",
        "        visual_embeddings = visual_embeds + visual_position_embeddings + visual_token_type_embeddings\n",
        "        print('visual_embeddings', visual_embeddings.shape)\n",
        "        embeddings = torch.cat((embeddings, visual_embeddings), dim=1)\n",
        "\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        return embeddings"
      ],
      "metadata": {
        "id": "hZvR0Ah6RuDM"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5JN2UWNa3gjG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}