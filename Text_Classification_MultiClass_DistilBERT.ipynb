{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Classification_MultiClass_DistilBERT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/tutorial_notebooks/blob/main/Text_Classification_MultiClass_DistilBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Installing libraries"
      ],
      "metadata": {
        "id": "AFSG2jHlCJL0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kv5SOlD_85PL",
        "outputId": "319c3aaa-1b9a-47a2-887a-eae3e41307ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 3.4 MB 18.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 67 kB 3.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 46.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 895 kB 64.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 73.6 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "! pip -q install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dataset\n",
        "We are using the News aggregator dataset available at by [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/News+Aggregator)<br>\n",
        "- There are `422937` rows of data\n",
        "- CATEGORY News category (b = business, t = science and technology, e = entertainment, m = health)\n"
      ],
      "metadata": {
        "id": "_H51edGJ-pTc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download dataset:"
      ],
      "metadata": {
        "id": "ab1m5gMbNDbA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -c https://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip\n",
        "!unzip -q NewsAggregatorDataset.zip -d data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JuENXY_MRx1",
        "outputId": "9fa0ef01-04e6-433e-c0f1-ac22aa0183b9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-19 16:40:05--  https://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Preparation"
      ],
      "metadata": {
        "id": "unPncYZWOkZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the libraries needed\n",
        "import pandas as pd\n",
        "import torch\n",
        "import transformers\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import DistilBertModel, DistilBertTokenizer\n",
        "\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "\n",
        "def update_cat(x):\n",
        "    return my_dict[x]\n",
        "\n",
        "def encode_cat(x):\n",
        "    if x not in encode_dict.keys():\n",
        "        encode_dict[x]=len(encode_dict)\n",
        "    return encode_dict[x]\n",
        "\n",
        "df = pd.read_csv('./data/newsCorpora.csv', sep='\\t', names=['ID','TITLE', 'URL', 'PUBLISHER', 'CATEGORY', 'STORY', 'HOSTNAME', 'TIMESTAMP'])\n",
        "df = df[['TITLE','CATEGORY']] # Removing unwanted columns\n",
        "my_dict = {\n",
        "    'e':'Entertainment',\n",
        "    'b':'Business',\n",
        "    't':'Science',\n",
        "    'm':'Health'\n",
        "}\n",
        "\n",
        "df['CATEGORY'] = df['CATEGORY'].apply(lambda x: update_cat(x))\n",
        "\n",
        "encode_dict = {}\n",
        "df['ENCODE_CAT'] = df['CATEGORY'].apply(lambda x: encode_cat(x))"
      ],
      "metadata": {
        "id": "zivr94sbLNxM"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataloader"
      ],
      "metadata": {
        "id": "Nne0VKX7OoDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertModel, DistilBertTokenizer\n",
        "class Triage(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.len = len(dataframe)\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        title = str(self.data.TITLE[index])\n",
        "        title = \" \".join(title.split())\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            title,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'targets': torch.tensor(self.data.ENCODE_CAT[index], dtype=torch.long)\n",
        "        } \n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "MAX_LEN = 512\n",
        "TRAIN_BATCH_SIZE = 32\n",
        "VALID_BATCH_SIZE = 32\n",
        "EPOCHS = 1\n",
        "LEARNING_RATE = 1e-05\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased')\n",
        "\n",
        "train_size = 0.8\n",
        "train_dataset=df.sample(frac=train_size,random_state=200)\n",
        "test_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n",
        "train_dataset = train_dataset.reset_index(drop=True)\n",
        "\n",
        "\n",
        "print(\"FULL Dataset: {}\".format(df.shape))\n",
        "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
        "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
        "\n",
        "training_set = Triage(train_dataset, tokenizer, MAX_LEN)\n",
        "testing_set = Triage(test_dataset, tokenizer, MAX_LEN)\n",
        "\n",
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "testing_loader = DataLoader(testing_set, **test_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzGncRAoOqEq",
        "outputId": "b95d3022-db31-410f-a3c1-7a258c01058b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FULL Dataset: (422419, 3)\n",
            "TRAIN Dataset: (337935, 3)\n",
            "TEST Dataset: (84484, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Architecture of DistilBERT:"
      ],
      "metadata": {
        "id": "zg0wMYcIPcX1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertModel, DistilBertTokenizer\n",
        "class DistilBERTClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DistilBERTClass, self).__init__()\n",
        "        self.l1 = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "        self.classifier = torch.nn.Linear(768, 4)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        hidden_state = output_1[0]\n",
        "        pooler = hidden_state[:, 0]\n",
        "        pooler = self.pre_classifier(pooler)\n",
        "        pooler = torch.nn.ReLU()(pooler)\n",
        "        pooler = self.dropout(pooler)\n",
        "        output = self.classifier(pooler)\n",
        "        return output\n",
        "\n",
        "model = DistilBERTClass()\n",
        "model.to(device);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxB0V4scPbtY",
        "outputId": "54c372c8-93d8-4692-ba08-9f204b102dd3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Script"
      ],
      "metadata": {
        "id": "Kj7a9x_-Pv3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calcuate_accu(big_idx, targets):\n",
        "    n_correct = (big_idx==targets).sum().item()\n",
        "    return n_correct\n",
        "\n",
        "\n",
        "def train(epoch, model, training_loader, optimizer, loss_function):\n",
        "    tr_loss = 0\n",
        "    n_correct = 0\n",
        "    nb_tr_steps = 0\n",
        "    nb_tr_examples = 0\n",
        "    model.train()\n",
        "    for _,data in enumerate(training_loader, 0):\n",
        "        ids = data['ids'].to(device, dtype = torch.long)\n",
        "        mask = data['mask'].to(device, dtype = torch.long)\n",
        "        targets = data['targets'].to(device, dtype = torch.long)\n",
        "\n",
        "        outputs = model(ids, mask)\n",
        "        loss = loss_function(outputs, targets)\n",
        "        tr_loss += loss.item()\n",
        "        big_val, big_idx = torch.max(outputs.data, dim=1)\n",
        "        n_correct += calcuate_accu(big_idx, targets)\n",
        "\n",
        "        nb_tr_steps += 1\n",
        "        nb_tr_examples+=targets.size(0)\n",
        "        \n",
        "        if _%50==0:\n",
        "            loss_step = tr_loss/nb_tr_steps\n",
        "            accu_step = (n_correct*100)/nb_tr_examples \n",
        "            print(f\"Training Loss per 50\\{len(training_loader)} steps: {loss_step}\")\n",
        "            print(f\"Training Accuracy per 50\\{len(training_loader)} steps: {accu_step}\")\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        # # When using GPU\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
        "    epoch_loss = tr_loss/nb_tr_steps\n",
        "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
        "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
        "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
        "\n",
        "    return \n",
        "\n",
        "def valid(model, testing_loader):\n",
        "    model.eval()\n",
        "    n_correct = 0; n_wrong = 0; total = 0; val_loss = 0; nb_val_steps = 0\n",
        "    nb_val_examples = 0\n",
        "    with torch.no_grad():\n",
        "        for _, data in enumerate(testing_loader, 0):\n",
        "            ids = data['ids'].to(device, dtype = torch.long)\n",
        "            mask = data['mask'].to(device, dtype = torch.long)\n",
        "            targets = data['targets'].to(device, dtype = torch.long)\n",
        "            outputs = model(ids, mask).squeeze()\n",
        "            loss = loss_function(outputs, targets)\n",
        "            val_loss += loss.item()\n",
        "            big_val, big_idx = torch.max(outputs.data, dim=1)\n",
        "            n_correct += calcuate_accu(big_idx, targets)\n",
        "\n",
        "            nb_val_steps += 1\n",
        "            nb_val_examples+=targets.size(0)\n",
        "            \n",
        "            if _%50==0:\n",
        "                loss_step = val_loss/nb_val_steps\n",
        "                accu_step = (n_correct*100)/nb_val_examples\n",
        "                print(f\"Validation Loss per 10\\{len(testing_loader)} steps: {loss_step}\")\n",
        "                print(f\"Validation Accuracy per 10\\{len(testing_loader)} steps: {accu_step}\")\n",
        "\n",
        "    epoch_loss = val_loss/nb_val_steps\n",
        "    epoch_accu = (n_correct*100)/nb_val_examples\n",
        "    print(f\"Validation Loss Epoch: {epoch_loss}\")\n",
        "    print(f\"Validation Accuracy Epoch: {epoch_accu}\")\n",
        "    \n",
        "    return epoch_accu\n"
      ],
      "metadata": {
        "id": "Bqz-L2odP5pw"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "HbUyqDZhQDwv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "best_epoch, best_acc = 0.0, 0\n",
        "for epoch in range(EPOCHS):\n",
        "    train(epoch, model, training_loader, optimizer, loss_function)\n",
        "    acc = valid(model, testing_loader)\n",
        "    if acc > best_acc:\n",
        "        best_acc = acc\n",
        "        best_epoch = epoch\n",
        "        torch.save(model.state_dict(), 'best_model_cifar10h.pth.tar')\n",
        "    print('epoch: {}  acc: {:.4f}  best epoch: {}  best acc: {:.4f}'.format(\n",
        "            epoch, acc, best_epoch, best_acc, optimizer.param_groups[0]['lr']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i97QE_CCFX6N",
        "outputId": "331d02ce-54d9-4f91-99dc-72a41e1b228d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss per 50\\10561 steps: 0.9380739331245422\n",
            "Training Accuracy per 50\\10561 steps: 59.375\n",
            "Training Loss per 50\\10561 steps: 1.028627407317068\n",
            "Training Accuracy per 50\\10561 steps: 57.96568627450981\n",
            "Training Loss per 50\\10561 steps: 1.021858262543631\n",
            "Training Accuracy per 50\\10561 steps: 57.88985148514851\n",
            "The Total Accuracy for Epoch 0: 57.88985148514851\n",
            "Training Loss Epoch: 1.021858262543631\n",
            "Training Accuracy Epoch: 57.88985148514851\n",
            "Validation Loss per 10\\2641 steps: 0.9856563806533813\n",
            "Validation Accuracy per 10\\2641 steps: 62.5\n",
            "Validation Loss per 10\\2641 steps: 0.9940446241229188\n",
            "Validation Accuracy per 10\\2641 steps: 58.76225490196079\n",
            "Validation Loss per 10\\2641 steps: 0.9879287977029781\n",
            "Validation Accuracy per 10\\2641 steps: 59.71534653465346\n",
            "Validation Loss per 10\\2641 steps: 0.9869846906883037\n",
            "Validation Accuracy per 10\\2641 steps: 59.41639072847682\n",
            "Validation Loss per 10\\2641 steps: 0.9823407859944585\n",
            "Validation Accuracy per 10\\2641 steps: 59.483830845771145\n",
            "Validation Loss Epoch: 0.9823407859944585\n",
            "Validation Accuracy Epoch: 59.483830845771145\n",
            "epoch: 0  acc: 59.4838  best epoch: 0  best acc: 59.4838\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dGdVnSMRvpW_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}