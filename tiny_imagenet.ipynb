{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tiny_imagenet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/tutorial_notebooks/blob/main/tiny_imagenet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tiny-ImageNet\n",
        "It contains 200 classes from full imagenet dataset.<br>\n",
        "However, file structures are not for train and validation images. Train images are separated with a class-wise folder name where all validation images are a single folder. We need an additional propcessing to rearrange validation images into class-wise folders."
      ],
      "metadata": {
        "id": "gxGDnKghKvNI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Donwload & unzip"
      ],
      "metadata": {
        "id": "pl6Makz8K5T_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ViOjKNfKkPW",
        "outputId": "0640b638-1947-4141-c426-7cf660236ff9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-29 17:28:43--  http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
            "Resolving cs231n.stanford.edu (cs231n.stanford.edu)... 171.64.68.10\n",
            "Connecting to cs231n.stanford.edu (cs231n.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 248100043 (237M) [application/zip]\n",
            "Saving to: ‘tiny-imagenet-200.zip’\n",
            "\n",
            "tiny-imagenet-200.z 100%[===================>] 236.61M  50.9MB/s    in 5.7s    \n",
            "\n",
            "2022-04-29 17:28:49 (41.9 MB/s) - ‘tiny-imagenet-200.zip’ saved [248100043/248100043]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "! wget http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
        "! unzip -q tiny-imagenet-200.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training CNN model"
      ],
      "metadata": {
        "id": "TfQmThl_RS-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision import models\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "import argparse\n",
        "import copy\n",
        "import random\n",
        "import numpy as np\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "parser = argparse.ArgumentParser(description='CIFAR-10H Training')\n",
        "parser.add_argument('--lr', default=0.1, type=float, help='learning rate')\n",
        "parser.add_argument('--lr_schedule', default=0, type=int, help='lr scheduler')\n",
        "parser.add_argument('--train_batch', default=512, type=int, help='batch size')\n",
        "parser.add_argument('--valid_batch', default=1024, type=int, help='batch size')\n",
        "parser.add_argument('--num_epoch', default=100, type=int, help='epoch number')\n",
        "parser.add_argument('--num_classes', type=int, default=200, help='number classes')\n",
        "args = parser.parse_args(args=[])\n",
        "\n",
        "def train(model, trainloader, criterion, optimizer):\n",
        "    model.train()\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "def test(model, testloader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "    return correct / total"
      ],
      "metadata": {
        "id": "UTn-SmzfNkk4"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "Q6CZ-swoSWsw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.io import read_image, ImageReadMode\n",
        "from torch.utils.data import Dataset\n",
        "import glob\n",
        "from PIL import Image\n",
        "\n",
        "seed_everything()\n",
        "VALID_DIR = 'tiny-imagenet-200/val'\n",
        "TRAIN_DIR = 'tiny-imagenet-200/train'\n",
        "\n",
        "class TinyImageNetDataset(Dataset):\n",
        "    def __init__(self, root, id, transform=None, train=False):\n",
        "        self.transform = transform\n",
        "        self.id_dict = id\n",
        "        self.train = train\n",
        "        if self.train:\n",
        "            self.filenames = glob.glob(os.path.join(root, \"train/*/*/*.JPEG\")) \n",
        "        else:\n",
        "            self.filenames = glob.glob(os.path.join(root,\"val/images/*.JPEG\"))\n",
        "            self.cls_dic = {}\n",
        "            for i, line in enumerate(open(os.path.join(root,'val/val_annotations.txt'), 'r')):\n",
        "                a = line.split('\\t')\n",
        "                img, cls_id = a[0],a[1]\n",
        "                self.cls_dic[img] = id[cls_id]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.filenames[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.train:\n",
        "            label = self.id_dict[img_path.split('/')[4]]\n",
        "        else:\n",
        "            label = self.cls_dic[img_path.split('/')[-1]]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "transform_train = transforms.Compose([transforms.Resize(64), transforms.RandomHorizontalFlip(), \n",
        "        transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406,), (0.229, 0.224, 0.225,))]\n",
        "        )\n",
        "transform_test = transforms.Compose([transforms.Resize(64), transforms.ToTensor(),\n",
        "        transforms.Normalize((0.485, 0.456, 0.406,), (0.229, 0.224, 0.225,))]\n",
        "        )\n",
        "\n",
        "id_dict = {}\n",
        "for i, line in enumerate(open('/content/tiny-imagenet-200/wnids.txt', 'r')):\n",
        "  id_dict[line.replace('\\n', '')] = i\n",
        "\n",
        "root = '/content/tiny-imagenet-200/'\n",
        "train_dataset = TinyImageNetDataset(root=root, id=id_dict, transform=transform_train, train=True)\n",
        "test_dataset = TinyImageNetDataset(root=root, id=id_dict, transform=transform_test, train=False)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=args.train_batch, shuffle=True,\n",
        "                                          num_workers=2, pin_memory=True)\n",
        "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=args.valid_batch, shuffle=False, \n",
        "                                         num_workers=2, pin_memory=True)\n",
        "print('sample size- Train:%d, Validation:%d'%(len(train_dataset), len(test_dataset)))\n",
        "\n",
        "\n",
        "model = models.resnet50(pretrained=True).to(device)\n",
        "model.fc = torch.nn.Linear(model.fc.in_features, args.num_classes)\n",
        "model.to(device)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=0.9, weight_decay=5e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "best_epoch, best_acc = 0, 0\n",
        "for epoch in range(args.num_epoch):\n",
        "    for epoch in range(args.num_epoch):\n",
        "        if epoch != 0 and epoch < 100 and epoch % 30 == 0:\n",
        "            for param in optimizer.param_groups:\n",
        "                param['lr'] = param['lr'] / 10 \n",
        "        train(model, trainloader, criterion, optimizer)\n",
        "        accuracy = test(model, testloader)\n",
        "        if accuracy > best_acc:\n",
        "            patience = 0\n",
        "            best_acc = accuracy\n",
        "            best_epoch = epoch\n",
        "            best_model = copy.deepcopy(model)\n",
        "            torch.save(best_model.state_dict(), 'best_model_tiny_imagenet.pth.tar')\n",
        "        print('epoch: {}  acc: {:.4f}  best epoch: {}  best acc: {:.4f}'.format(\n",
        "                epoch, accuracy, best_epoch, best_acc, optimizer.param_groups[0]['lr']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5IYHVb5Rukr",
        "outputId": "c76fc58f-5b53-4b37-f51c-d8c81aea3706"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample size- Train:100000, Validation:10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "HC7i9JeupQeo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}