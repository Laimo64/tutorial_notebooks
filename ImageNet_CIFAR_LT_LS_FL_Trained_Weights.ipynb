{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZ3qL5dG7vBj1tVwHwe9lL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/tutorial_notebooks/blob/main/ImageNet_CIFAR_LT_LS_FL_Trained_Weights.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ImageNet (ILSVRC2012)\n",
        "\n",
        "It contains 1000 classes, 1.28 million training images, and 50 thousand validation images. There are 1,281,167 images and 732-1300 per class in the ILSVRC2012 training set. This dataset spans 1000 object classes and contains 1,281,167 training images, 50,000 validation images and 100,000 test images. It requires more than 150GB of storage, and training a resnet50 on it will take around 215 hours using a T4 GPU on Google Colab. Folder name to actual class mapping: https://www.image-net.org/challenges/LSVRC/2012/browse-synsets.php <br>\n",
        "Sample size is not equal in ImageNet. For example top 10 classes:<br>\n",
        "n02094433:    3047 (Yorkshire terrier)<br>\n",
        "n02086240:    2563 (Shih-Tzu)<br>\n",
        "n01882714:    2469 (koala bear, kangaroo bear, native bear, )<br>\n",
        "n02087394:    2449 (Rhodesian ridgeback)<br>\n",
        "n02100735:    2426 (English setter)<br>\n",
        "n00483313:    2410 (singles)<br>\n",
        "n02279972:    2386 (monarch butterfly, Danaus plexippus)<br>\n",
        "n09428293:    2382 (seashore)<br>\n",
        "n02138441:    2341 (meerkat)<br>\n",
        "n02100583:    2334 (vizsla, Hungarian pointer)<br>\n",
        "\n",
        "\n",
        "Task-1. Image classification (2010-2014): Algorithms produce a list of object categories present in the image.<br>\n",
        "Task-2. Single-object localization (2011-2014): Algorithms\n",
        "produce a list of object categories present in the image, along with an axis-aligned bounding box indicating the position and scale of one instance of each object category.<br>\n",
        "Task-3. Object detection (2013-2014): Algorithms produce\n",
        "a list of object categories present in the image along\n",
        "with an axis-aligned bounding box indicating the\n",
        "position and scale of every instance of each object\n",
        "category.<br>\n",
        "\n",
        "#Download Links:\n",
        "\n",
        "Training Images (taskl&2): https://image-net.org/data/ILSVRC/2012/ILSVRC2012_img_train.tar <br>\n",
        "Training Annotations (taskl&2): https://image-net.org/data/ILSVRC/2012/ILSVRC2012_bbox_train_v2.tar.gz <br>\n",
        "\n",
        "Validation Images (all tasks): https://image-net.org/data/ILSVRC/2012/ILSVRC2012_img_val.tar\n",
        "\n",
        "Validation Annotations (all tasks): https://image-net.org/data/ILSVRC/2012/ILSVRC2012_bbox_val_v3.tgz\n"
      ],
      "metadata": {
        "id": "KHwLWpzQGmJA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing Train Images into Folders (Not using in this tutorial)\n",
        "src: https://github.com/pytorch/examples/blob/main/imagenet/extract_ILSVRC.sh"
      ],
      "metadata": {
        "id": "qtbN1219glYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create train directory; move .tar file; change directory\n",
        "!mkdir imagenet/train && mv ILSVRC2012_img_train.tar imagenet/train/ && cd imagenet/train\n",
        "# Extract training set; remove compressed file\n",
        "!tar -xvf ILSVRC2012_img_train.tar && rm -f ILSVRC2012_img_train.tar\n",
        "#\n",
        "# At this stage imagenet/train will contain 1000 compressed .tar files, one for each category\n",
        "#\n",
        "# For each .tar file: \n",
        "#   1. create directory with same name as .tar file\n",
        "#   2. extract and copy contents of .tar file into directory\n",
        "#   3. remove .tar file\n",
        "!find . -name \"*.tar\" | while read NAME ; do mkdir -p \"${NAME%.tar}\"; tar -xvf \"${NAME}\" -C \"${NAME%.tar}\"; rm -f \"${NAME}\"; done"
      ],
      "metadata": {
        "id": "piDwRwOIgkQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download only Validation Set"
      ],
      "metadata": {
        "id": "2SlLraVFT635"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://image-net.org/data/ILSVRC/2012/ILSVRC2012_img_val.tar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YlcVUkDc1on",
        "outputId": "ad1042cf-f79a-4c87-e973-ea34dd0e1411"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-30 22:45:45--  https://image-net.org/data/ILSVRC/2012/ILSVRC2012_img_val.tar\n",
            "Resolving image-net.org (image-net.org)... 171.64.68.16\n",
            "Connecting to image-net.org (image-net.org)|171.64.68.16|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6744924160 (6.3G) [application/x-tar]\n",
            "Saving to: ‘ILSVRC2012_img_val.tar’\n",
            "\n",
            "ILSVRC2012_img_val. 100%[===================>]   6.28G  16.2MB/s    in 8m 35s  \n",
            "\n",
            "2022-10-30 22:54:21 (12.5 MB/s) - ‘ILSVRC2012_img_val.tar’ saved [6744924160/6744924160]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing Valid Images into Folders"
      ],
      "metadata": {
        "id": "wbsK_wL6g6RA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir imagenet\n",
        "!mkdir imagenet/val\n",
        "!tar -xvf ILSVRC2012_img_val.tar --directory imagenet/val\n",
        "%cd imagenet/val\n",
        "!wget -qO- https://raw.githubusercontent.com/soumith/imagenetloader.torch/master/valprep.sh | bash\n",
        "%cd ../.."
      ],
      "metadata": {
        "id": "4_XdkO8Tc-13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "import shutil\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.distributed as dist\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "import torch.utils.data.distributed\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.models as models\n",
        "\n",
        "def test(model, testloader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "    return correct / total\n",
        "\n",
        "valdir = os.path.join('imagenet', 'val')\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                    std=[0.229, 0.224, 0.225])\n",
        "\n",
        "\n",
        "val_dataset = datasets.ImageFolder(valdir, transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "    ]))\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=512, shuffle=False,\n",
        "    num_workers=2, pin_memory=True)\n",
        "\n",
        "print('Sample size:', len(val_dataset))\n",
        "for i, (input, target) in enumerate(val_loader):\n",
        "    print('First batch:',input.shape, target)\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9hFROHsZRxQ",
        "outputId": "9817e764-d56b-4974-ad06-36fdd0332ac5"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample size: 50000\n",
            "First batch: torch.Size([512, 3, 224, 224]) tensor([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  1,  1,\n",
            "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
            "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
            "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,\n",
            "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
            "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
            "         2,  2,  2,  2,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
            "         3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
            "         3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
            "         3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
            "         4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
            "         4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  5,  5,\n",
            "         5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
            "         5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
            "         5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,\n",
            "         6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
            "         6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
            "         6,  6,  6,  6,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,\n",
            "         7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,\n",
            "         7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,\n",
            "         7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
            "         8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
            "         8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
            "         9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,\n",
            "         9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,\n",
            "         9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10, 10, 10,\n",
            "        10, 10, 10, 10, 10, 10, 10, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Vision Transformer and Variants\n",
        "Basic: https://github.com/mobarakol/tutorial_notebooks/blob/main/ViT_Module_Visualization.ipynb<br>\n",
        "Installation:<br>\n",
        "github: https://github.com/rwightman/pytorch-image-models/tree/master/timm/models"
      ],
      "metadata": {
        "id": "J6wu1OXrmyBe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install timm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KilVWig1mzRk",
        "outputId": "20a4ff4e-9246-45e1-a47e-0d83566b5d84"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 548 kB 24.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 163 kB 61.8 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ViT: AN IMAGE IS WORTH 16X16 WORDS:\n",
        "TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE - https://arxiv.org/pdf/2010.11929.pdf"
      ],
      "metadata": {
        "id": "Z-LU-njAmGdz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from timm import create_model\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "vit = create_model(\"vit_large_patch16_224\", pretrained=True).to(device)#vit_base_patch16_224\n",
        "accuracy = test(vit, val_loader)\n",
        "print('accuracy:',accuracy)"
      ],
      "metadata": {
        "id": "cDj0Q7bjf08m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62f70acd-8107-4bdf-c58e-671119da54df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.84374\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Swin-Transformer: Hierarchical Vision Transformer using Shifted Windows -https://arxiv.org/pdf/2103.14030.pdf"
      ],
      "metadata": {
        "id": "tACc7uMhnAf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "swintran = create_model(\"swin_base_patch4_window7_224\", pretrained=True).to(device)\n",
        "accuracy = test(swintran, val_loader)\n",
        "print('accuracy:',accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ih04ZKrm8UW",
        "outputId": "826d102a-5ba8-4498-da25-73d0d094c83c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "Downloading: \"https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_base_patch4_window7_224_22kto1k.pth\" to /root/.cache/torch/hub/checkpoints/swin_base_patch4_window7_224_22kto1k.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.84714\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DeiT: Data-efficient Image Transformers - https://arxiv.org/abs/2012.12877"
      ],
      "metadata": {
        "id": "FD6by7W1nF90"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "deit = create_model(\"deit_base_patch16_224\", pretrained=True).to(device)\n",
        "accuracy = test(deit, val_loader)\n",
        "print('accuracy:',accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aw7Hb46mnINX",
        "outputId": "26e38478-b7d4-4313-a75f-49ca6e0fa987"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth\" to /root/.cache/torch/hub/checkpoints/deit_base_patch16_224-b5f2ef4d.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.81742\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CaiT: Class-Attention in Image Transformers (https://arxiv.org/abs/2103.17239)"
      ],
      "metadata": {
        "id": "1tSEzCnonKmb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cait = create_model(\"cait_s24_224\", pretrained=True).to(device)\n",
        "accuracy = test(cait, val_loader)\n",
        "print('accuracy:',accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cozK6pPtnNJF",
        "outputId": "50ba3b38-a2a0-49dd-c071-854cb35024fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://dl.fbaipublicfiles.com/deit/S24_224.pth\" to /root/.cache/torch/hub/checkpoints/S24_224.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.83302\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BeiT: BERT Pre-Training of Image Transformers (https://arxiv.org/abs/2106.08254)"
      ],
      "metadata": {
        "id": "FO4IesNynRsF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from timm import create_model\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "beit = create_model(\"beitv2_base_patch16_224\", pretrained=True).to(device)\n",
        "accuracy = test(beit, val_loader)\n",
        "print('accuracy:',accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahpQ8WYInOq2",
        "outputId": "b8bfce77-8bbb-4d0a-8697-9171533b609c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://conversationhub.blob.core.windows.net/beit-share-public/beitv2/beitv2_base_patch16_224_pt1k_ft21kto1k.pth\" to /root/.cache/torch/hub/checkpoints/beitv2_base_patch16_224_pt1k_ft21kto1k.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.86092\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CoaT: Co-Scale Conv-Attentional Image Transformers - https://arxiv.org/abs/2104.06399"
      ],
      "metadata": {
        "id": "qB-NX353nUK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "coat = create_model(\"coat_mini\", pretrained=True).to(device)\n",
        "accuracy = test(coat, val_loader)\n",
        "print('accuracy:',accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8I8yBrITnW1w",
        "outputId": "71765cf9-c5b9-4414-ab24-e109a5c5ef4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-coat-weights/coat_mini-2c6baf49.pth\" to /root/.cache/torch/hub/checkpoints/coat_mini-2c6baf49.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.80912\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification (et al. ICCV 2021)"
      ],
      "metadata": {
        "id": "NM9Jq8_co4Zj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "crossvit = create_model(\"crossvit_base_240\", pretrained=True).to(device)\n",
        "accuracy = test(crossvit, val_loader)\n",
        "print('accuracy:',accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgEfi640o4wr",
        "outputId": "9fbd0359-9e1f-411d-8897-43b5d2155f70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/IBM/CrossViT/releases/download/weights-0.1/crossvit_base_224.pth\" to /root/.cache/torch/hub/checkpoints/crossvit_base_224.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.82092\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ConvMixer: Patches Are All You Need? (https://arxiv.org/pdf/2201.09792.pdf)"
      ],
      "metadata": {
        "id": "n18k-EuwpUYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "convmixer = create_model(\"convmixer_768_32\", pretrained=True).to(device)\n",
        "accuracy = test(convmixer, val_loader)\n",
        "print('accuracy:',accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRAjH7LIpU_7",
        "outputId": "44144e76-50d7-4fa5-f218-02a56438400f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/tmp-iclr/convmixer/releases/download/timm-v1.0/convmixer_768_32_ks7_p7_relu.pth.tar\" to /root/.cache/torch/hub/checkpoints/convmixer_768_32_ks7_p7_relu.pth.tar\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.8008\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ConvNeXt: A ConvNet for the 2020s - https://arxiv.org/pdf/2201.03545.pdf"
      ],
      "metadata": {
        "id": "kdvR55PRpqjd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "convnext = create_model(\"convnext_base\", pretrained=True).to(device)\n",
        "accuracy = test(convnext, val_loader)\n",
        "print('accuracy:',accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icakQq5QprDo",
        "outputId": "cd2aec94-187e-452a-eff4-97c0cf5c0d5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://dl.fbaipublicfiles.com/convnext/convnext_base_1k_224_ema.pth\" to /root/.cache/torch/hub/checkpoints/convnext_base_1k_224_ema.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.83746\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ViT_relpos: Rethinking and Improving Relative Position Encoding for Vision Transformer -https://arxiv.org/pdf/2107.14222.pdf"
      ],
      "metadata": {
        "id": "C8xo5NevQH6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vit_relpos = create_model(\"vit_relpos_base_patch16_cls_224\", pretrained=True).to(device) #vit_relpos_base_patch16_224\n",
        "accuracy = test(vit_relpos, val_loader)\n",
        "print('accuracy:',accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4Z9MueprSC1",
        "outputId": "60170b18-b2d1-489f-ab7a-7c3738d0906e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:timm.models.helpers:No pretrained weights exist or were found for this model. Using random initialization.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ViTs from https://github.com/jeonsworld/ViT-pytorch"
      ],
      "metadata": {
        "id": "wJu0NJPUv1Qk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install ml_collections\n",
        "! git clone https://github.com/jeonsworld/ViT-pytorch\n",
        "%cd ViT-pytorch\n",
        "! wget https://storage.googleapis.com/vit_models/imagenet21k%2Bimagenet2012/R50%2BViT-B_16.npz\n",
        "! touch models/__init__.py"
      ],
      "metadata": {
        "id": "ygrgDKh5Rl3x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4bbf098-e8b1-4f8c-b42c-b7f6950f57bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |████▏                           | 10 kB 30.8 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 20 kB 37.4 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 30 kB 45.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 40 kB 26.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 51 kB 29.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 61 kB 33.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 71 kB 30.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 77 kB 5.9 MB/s \n",
            "\u001b[?25h  Building wheel for ml-collections (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Cloning into 'ViT-pytorch'...\n",
            "remote: Enumerating objects: 170, done.\u001b[K\n",
            "remote: Total 170 (delta 0), reused 0 (delta 0), pack-reused 170\u001b[K\n",
            "Receiving objects: 100% (170/170), 21.20 MiB | 36.25 MiB/s, done.\n",
            "Resolving deltas: 100% (83/83), done.\n",
            "/content/ViT-pytorch\n",
            "--2022-10-21 23:03:41--  https://storage.googleapis.com/vit_models/imagenet21k%2Bimagenet2012/R50%2BViT-B_16.npz\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.163.128, 172.217.15.80, 172.253.62.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.163.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 395916008 (378M) [application/octet-stream]\n",
            "Saving to: ‘R50+ViT-B_16.npz’\n",
            "\n",
            "R50+ViT-B_16.npz    100%[===================>] 377.57M  68.8MB/s    in 5.3s    \n",
            "\n",
            "2022-10-21 23:03:47 (71.8 MB/s) - ‘R50+ViT-B_16.npz’ saved [395916008/395916008]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! wget https://storage.googleapis.com/vit_models/imagenet21k%2Bimagenet2012/ViT-B_16-224.npz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FW2fbSzI0F3j",
        "outputId": "6166d79f-3eed-4442-e46c-3428b1884df8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-21 23:11:20--  https://storage.googleapis.com/vit_models/imagenet21k%2Bimagenet2012/ViT-B_16-224.npz\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.16.128, 142.251.33.208, 142.250.188.48, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.16.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 346335542 (330M) [application/octet-stream]\n",
            "Saving to: ‘ViT-B_16-224.npz’\n",
            "\n",
            "ViT-B_16-224.npz    100%[===================>] 330.29M  87.6MB/s    in 4.0s    \n",
            "\n",
            "2022-10-21 23:11:24 (83.1 MB/s) - ‘ViT-B_16-224.npz’ saved [346335542/346335542]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ViT-pytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oF36sC8g0m_w",
        "outputId": "762f3746-310f-4934-d021-450251b49f41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ViT-pytorch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "import shutil\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.distributed as dist\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "import torch.utils.data.distributed\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.models as models\n",
        "\n",
        "from models.modeling import VisionTransformer, CONFIGS\n",
        "#config = CONFIGS['R50-ViT-B_16']\n",
        "config = CONFIGS['ViT-B_16']\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def test(model, testloader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)[0]\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "    return correct / total\n",
        "\n",
        "valdir = os.path.join('../imagenet', 'val')\n",
        "normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "\n",
        "\n",
        "val_dataset = datasets.ImageFolder(valdir, transforms.Compose([\n",
        "        transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "    ]))\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=300, shuffle=False,\n",
        "    num_workers=2, pin_memory=True)\n",
        "\n",
        "hvit = VisionTransformer(config, num_classes=1000, zero_head=False, img_size=224, vis=True)\n",
        "hvit.load_from(np.load(\"ViT-B_16-224.npz\"))\n",
        "hvit.to(device)\n",
        "accuracy = test(hvit, val_loader)\n",
        "print('accuracy:',accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WNZmU3HwK2U",
        "outputId": "97a9f90b-9109-4863-bf23-9d250d26ead8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.80314\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CIFAR-LT\n",
        "src: https://github.com/XuZhengzhuo/Prior-LT"
      ],
      "metadata": {
        "id": "OtSoOkdJILAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/XuZhengzhuo/Prior-LT.git\n",
        "%cd Prior-LT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FntNX6cIJN8-",
        "outputId": "0a0e9638-e506-40fb-c2aa-3706cf0e28bf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Prior-LT'...\n",
            "remote: Enumerating objects: 54, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 54 (delta 1), reused 0 (delta 0), pack-reused 48\u001b[K\n",
            "Unpacking objects: 100% (54/54), done.\n",
            "/content/Prior-LT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CIFAR-100-LT-50: https://drive.google.com/file/d/1PKpxeeCO5ZRAq4srleTlcQqTTjQd6JfT/view"
      ],
      "metadata": {
        "id": "Wsr4K1fNMXab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#src: https://github.com/XuZhengzhuo/Prior-LT\n",
        "import gdown\n",
        "url = 'https://drive.google.com/uc?id=1PKpxeeCO5ZRAq4srleTlcQqTTjQd6JfT'\n",
        "gdown.download(url,'model_best_cifar100_lt50.pth.tar',quiet=False) \n",
        "\n",
        "url = 'https://drive.google.com/uc?id=16JUoxnbxuO7nivjw4M0LkUQiJ9AyAJDm'\n",
        "gdown.download(url,'model_best_cifar100_lt200.pth.tar',quiet=False) \n",
        "\n",
        "url = 'https://drive.google.com/uc?id=1tclscVkcXj0lJum7Azy8qHecB7Pomc0c'\n",
        "gdown.download(url,'model_best_cifar10_lt50.pth.tar',quiet=False) \n",
        "\n",
        "url = 'https://drive.google.com/uc?id=1GTf42bpfDmMz5MHTVsX9YkjLSeo9WJ-v'\n",
        "gdown.download(url,'model_best_cifar10_lt200.pth.tar',quiet=False) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "7OK3vJCVIunz",
        "outputId": "e6a95685-4397-4a83-fb76-4ee3ea15fc36"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1PKpxeeCO5ZRAq4srleTlcQqTTjQd6JfT\n",
            "To: /content/Prior-LT/model_best_cifar100_lt50.pth.tar\n",
            "100%|██████████| 3.82M/3.82M [00:00<00:00, 238MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=16JUoxnbxuO7nivjw4M0LkUQiJ9AyAJDm\n",
            "To: /content/Prior-LT/model_best_cifar100_lt200.pth.tar\n",
            "100%|██████████| 3.82M/3.82M [00:00<00:00, 193MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1tclscVkcXj0lJum7Azy8qHecB7Pomc0c\n",
            "To: /content/Prior-LT/model_best_cifar10_lt50.pth.tar\n",
            "100%|██████████| 3.77M/3.77M [00:00<00:00, 110MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1GTf42bpfDmMz5MHTVsX9YkjLSeo9WJ-v\n",
            "To: /content/Prior-LT/model_best_cifar10_lt200.pth.tar\n",
            "100%|██████████| 3.77M/3.77M [00:00<00:00, 192MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'model_best_cifar10_lt200.pth.tar'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import models\n",
        "import torchvision.transforms as transforms\n",
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import copy \n",
        "import os\n",
        "import argparse\n",
        "import sys\n",
        "import random\n",
        "import numpy as np\n",
        "from torchvision import models\n",
        "from models import resnet32\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def test(model, testloader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "    return correct / total\n",
        "def main(num_classes=100, ckpt=None, test_loader=None):\n",
        "    model = resnet32(num_classes=num_classes)\n",
        "    model.load_state_dict(torch.load(ckpt)['state_dict_model'])\n",
        "    model.to(device)\n",
        "    acc = test(model, test_loader)\n",
        "    return acc\n",
        "\n",
        "mean_cifar, std_cifar = (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)\n",
        "transform_test = transforms.Compose([transforms.ToTensor(),\n",
        "    transforms.Normalize(mean_cifar, std_cifar),])\n",
        "test_dataset100 = torchvision.datasets.CIFAR100(root='data', train=False, download=True, transform=transform_test)\n",
        "test_loader100 = torch.utils.data.DataLoader(test_dataset100, batch_size=2048, shuffle=False, num_workers=2)\n",
        "\n",
        "test_dataset10 = torchvision.datasets.CIFAR10(root='data', train=False, download=True, transform=transform_test)\n",
        "test_loader10 = torch.utils.data.DataLoader(test_dataset10, batch_size=2048, shuffle=False, num_workers=2)\n",
        "\n",
        "ckpt_all =['model_best_cifar100_lt50.pth.tar', 'model_best_cifar100_lt200.pth.tar', \n",
        "           'model_best_cifar10_lt50.pth.tar', 'model_best_cifar10_lt200.pth.tar'] \n",
        "test_loader_all = [test_loader100, test_loader100, test_loader10, test_loader10]\n",
        "num_classes_all = [100, 100, 10, 10]\n",
        "for idx, ckpt in enumerate(ckpt_all):\n",
        "    print(ckpt,':', main(num_classes=num_classes_all[idx], ckpt=ckpt, test_loader=test_loader_all[idx]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSSTgLywkQvK",
        "outputId": "3a7df531-b94a-44fa-c8ee-56101ad8bcbb"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "model_best_cifar100_lt50.pth.tar : 0.5144\n",
            "model_best_cifar100_lt200.pth.tar : 0.4347\n",
            "model_best_cifar10_lt50.pth.tar : 0.8493\n",
            "model_best_cifar10_lt200.pth.tar : 0.804\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ImageNet-LT\n",
        "All long-tailed: https://github.com/Vanint/Awesome-LongTailed-Learning<br>\n",
        "https://github.com/zzw-zwzhang/Awesome-of-Long-Tailed-Recognition"
      ],
      "metadata": {
        "id": "wQ6QP_wkQoj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JugdP_TgRWP-",
        "outputId": "117df5e0-9d2d-4f03-bb38-88e0b57e16ef"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/root\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/naver-ai/cmo\n",
        "import gdown\n",
        "url = 'https://drive.google.com/uc?id=1RIHcrFwzZccqvOs8GgSX5CUFUkXlVvWp'\n",
        "gdown.download(url,'ckpt.pth.tar',quiet=False) \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "HGqDAZUaNjgX",
        "outputId": "ab9970bd-35f4-4e92-8f2b-a166f5f0a42f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1RIHcrFwzZccqvOs8GgSX5CUFUkXlVvWp\n",
            "To: /root/ckpt.pth.tar\n",
            "100%|██████████| 205M/205M [00:01<00:00, 160MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ckpt.pth.tar'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import models\n",
        "from torch import nn\n",
        "import torch\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def test(model, testloader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "    return correct / total\n",
        "\n",
        "valdir = os.path.join('imagenet', 'val')\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                    std=[0.229, 0.224, 0.225])\n",
        "\n",
        "\n",
        "val_dataset = datasets.ImageFolder(valdir, transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "    ]))\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=512, shuffle=False,\n",
        "    num_workers=2, pin_memory=True)\n",
        "\n",
        "model = models.resnet50(pretrained=False)\n",
        "model = nn.DataParallel(model)\n",
        "model.load_state_dict(torch.load('ckpt.pth.tar')['state_dict'])\n",
        "model = model.module\n",
        "model.to(device)\n",
        "accuracy = test(model, val_loader)\n",
        "print('accuracy:',accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--uwYAjjRY2i",
        "outputId": "af02932d-d49a-4f8a-9bee-be847d39a5a3"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.49266\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tiny ImageNet (Label Smoothing (LS), MBLS,  Focal Loss(FL) )\n",
        "src: https://github.com/by-liu/MbLS/blob/main/docs/TEST.md<br>\n",
        "MBLS paper: https://arxiv.org/pdf/2111.15430.pdf"
      ],
      "metadata": {
        "id": "fDiK29CHVrR5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! wget http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
        "! unzip -q tiny-imagenet-200.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIEq9nYlWc9h",
        "outputId": "e78b43e7-4b7a-4248-e393-ae7c0ed6b576"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-30 20:48:00--  http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
            "Resolving cs231n.stanford.edu (cs231n.stanford.edu)... 171.64.68.10\n",
            "Connecting to cs231n.stanford.edu (cs231n.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 248100043 (237M) [application/zip]\n",
            "Saving to: ‘tiny-imagenet-200.zip’\n",
            "\n",
            "tiny-imagenet-200.z 100%[===================>] 236.61M  12.3MB/s    in 21s     \n",
            "\n",
            "2022-10-30 20:48:21 (11.3 MB/s) - ‘tiny-imagenet-200.zip’ saved [248100043/248100043]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "cloning git repo and weights"
      ],
      "metadata": {
        "id": "tTMfk7FRxsTU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! wget --no-check-certificate https://github.com/by-liu/MbLS/releases/download/v0.2/resnet50_tiny-ls-best.pth\n",
        "! wget --no-check-certificate https://github.com/by-liu/MbLS/releases/download/v0.2/resnet50_tiny-fl-best.pth\n",
        "! wget --no-check-certificate https://github.com/by-liu/MbLS/releases/download/v0.2/resnet50_tiny-ce-best.pth\n",
        "! wget --no-check-certificate https://github.com/by-liu/MbLS/releases/download/v0.2/resnet50_tiny-mbls-best.pth\n",
        "!git clone https://github.com/by-liu/MbLS.git"
      ],
      "metadata": {
        "id": "ERDNCpWExl_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! wget --no-check-certificate https://github.com/by-liu/MbLS/releases/download/v0.2/resnet50_tiny-fl-best.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSux7hCh9tId",
        "outputId": "893d8ebd-44ec-4bd4-e2ba-5664e08c3b36"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-30 22:15:31--  https://github.com/by-liu/MbLS/releases/download/v0.2/resnet50_tiny-fl-best.pth\n",
            "Resolving github.com (github.com)... 140.82.121.3\n",
            "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/431784914/a507f25f-10b6-43b5-8c97-9d55144ea334?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20221030%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20221030T221531Z&X-Amz-Expires=300&X-Amz-Signature=b64cdc8c412895065eb90881353a531c46c1f705fb5db41184c0cb5318ec7cf6&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=431784914&response-content-disposition=attachment%3B%20filename%3Dresnet50_tiny-fl-best.pth&response-content-type=application%2Foctet-stream [following]\n",
            "--2022-10-30 22:15:31--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/431784914/a507f25f-10b6-43b5-8c97-9d55144ea334?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20221030%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20221030T221531Z&X-Amz-Expires=300&X-Amz-Signature=b64cdc8c412895065eb90881353a531c46c1f705fb5db41184c0cb5318ec7cf6&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=431784914&response-content-disposition=attachment%3B%20filename%3Dresnet50_tiny-fl-best.pth&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 201483194 (192M) [application/octet-stream]\n",
            "Saving to: ‘resnet50_tiny-fl-best.pth’\n",
            "\n",
            "resnet50_tiny-fl-be 100%[===================>] 192.15M  7.07MB/s    in 13s     \n",
            "\n",
            "2022-10-30 22:15:45 (14.5 MB/s) - ‘resnet50_tiny-fl-best.pth’ saved [201483194/201483194]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "preparing dataloader"
      ],
      "metadata": {
        "id": "L1nvn6jH8lbp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import pandas as pd\n",
        "import os\n",
        "import shutil\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.distributed as dist\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "import torch.utils.data.distributed\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.models as models\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "def test(model, testloader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "    return correct / total\n",
        "\n",
        "VALID_DIR = 'tiny-imagenet-200/val'\n",
        "val_data = pd.read_csv(f'{VALID_DIR}/val_annotations.txt', sep='\\t', \n",
        "                            header=None, names=['File', 'Class', 'X', 'Y', 'H', 'W'])\n",
        "\n",
        "\n",
        "val_img_dir = os.path.join(VALID_DIR, 'images')\n",
        "fp = open(os.path.join(VALID_DIR, 'val_annotations.txt'), 'r')\n",
        "data = fp.readlines()\n",
        "\n",
        "# Mapping image file name with label name\n",
        "val_img_dict = {}\n",
        "for line in data:\n",
        "    words = line.split('\\t')\n",
        "    val_img_dict[words[0]] = words[1]\n",
        "fp.close()\n",
        "\n",
        "# moving images into corresponding class folders\n",
        "for img, folder in val_img_dict.items():\n",
        "    newpath = (os.path.join(val_img_dir, folder))\n",
        "    if not os.path.exists(newpath):\n",
        "        os.makedirs(newpath)\n",
        "    if os.path.exists(os.path.join(val_img_dir, img)):\n",
        "        os.rename(os.path.join(val_img_dir, img), os.path.join(newpath, img))\n",
        "\n",
        "\n",
        "val_img_dir = os.path.join(VALID_DIR, 'images')\n",
        "\n",
        "transform_test = transforms.Compose([transforms.Resize((64,64)), transforms.ToTensor(),\n",
        "        transforms.Normalize((0.485, 0.456, 0.406,), (0.229, 0.224, 0.225,))]\n",
        "        )\n",
        "\n",
        "test_dataset = datasets.ImageFolder(os.path.join(val_img_dir),\n",
        "    transform=transform_test,)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=256, shuffle=False, \n",
        "                                         num_workers=2, pin_memory=True)\n",
        "print('sample size-  Validation:%d'%(len(test_dataset)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxHhyDPdWhQp",
        "outputId": "e5fa1bc9-d100-482e-be8d-fe959895643f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample size-  Validation:10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('MbLS')\n",
        "from MbLS.calibrate.net.resnet_tiny_imagenet import resnet50\n",
        "def main(method=None):\n",
        "    model = resnet50()\n",
        "    model.load_state_dict(torch.load('resnet50_tiny-{}-best.pth'.format(method))['state_dict'])\n",
        "    model.to(device)\n",
        "    accuracy = test(model, testloader)\n",
        "    return accuracy\n",
        "\n",
        "methods = ['ce', 'ls', 'fl', 'mbls']\n",
        "for method in methods:\n",
        "    accuracy = main(method=method)\n",
        "    print('method:',method, ', accuracy:',accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DexuAwD8aCC",
        "outputId": "82f92849-5592-49cd-cadb-0303a66b953f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "method: ce , accuracy: 0.6495\n",
            "method: ls , accuracy: 0.6565\n",
            "method: fl , accuracy: 0.6319\n",
            "method: mbls , accuracy: 0.6478\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ImageNet (Label Smoothing) \n",
        "code:https://github.com/sutd-visual-computing-group/LS-KD-compatibility<br>\n",
        "paper:https://arxiv.org/pdf/2206.14532.pdf"
      ],
      "metadata": {
        "id": "2N4tiaAYE1Xq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://drive.google.com/drive/folders/1GwqXRVYBpKGolNh2OLEzWUdOHx2XQ6G2\n",
        "import gdown\n",
        "url = 'https://drive.google.com/uc?id=13KVbELef0hWLczrp5suPvzyzH1NtXA2t'\n",
        "gdown.download(url,'teacher_resnet50_ce_best.pth.tar',quiet=False) \n",
        "\n",
        "url = 'https://drive.google.com/uc?id=1MJ-wniJ9dv_-QoSqOHcwfzU-FP3efJp-'\n",
        "gdown.download(url,'teacher_resnet50_ls_best.pth.tar',quiet=False) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "58TKvvBlGotF",
        "outputId": "f7f98f58-0d20-4245-f0bc-20ab77e6f5b8"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=13KVbELef0hWLczrp5suPvzyzH1NtXA2t\n",
            "To: /content/teacher_resnet50_ce_best.pth.tar\n",
            "100%|██████████| 103M/103M [00:01<00:00, 73.2MB/s] \n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1MJ-wniJ9dv_-QoSqOHcwfzU-FP3efJp-\n",
            "To: /content/teacher_resnet50_ls_best.pth.tar\n",
            "100%|██████████| 103M/103M [00:01<00:00, 78.7MB/s] \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'teacher_resnet50_ls_best.pth.tar'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import models\n",
        "from torch import nn\n",
        "import torch\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def test(model, testloader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "    return correct / total\n",
        "\n",
        "valdir = os.path.join('imagenet', 'val')\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                    std=[0.229, 0.224, 0.225])\n",
        "\n",
        "\n",
        "val_dataset = datasets.ImageFolder(valdir, transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "    ]))\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=512, shuffle=False,\n",
        "    num_workers=2, pin_memory=True)\n",
        "\n",
        "def main(method=None):\n",
        "    model = models.resnet50(pretrained=False)\n",
        "    model = nn.DataParallel(model)\n",
        "    model.load_state_dict(torch.load('teacher_resnet50_{}_best.pth.tar'.format(method))['state_dict'])\n",
        "    model = model.module\n",
        "    model.to(device)\n",
        "    accuracy = test(model, val_loader)\n",
        "    return accuracy\n",
        "\n",
        "methods = ['ce','ls']\n",
        "for method in methods:\n",
        "    accuracy = main(method=method)\n",
        "    print('method:', method, ', accuracy:',accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvcUOAVpG8Gw",
        "outputId": "85dad13d-dd1b-4a2b-f3e4-4ce24215f15c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.74548\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "More models can be found here resnet50, 18:\n",
        "https://drive.google.com/drive/folders/1BwxpQELsS09-C-2EJlaGyjvZPS6iBv84<br>\n",
        "NMT: https://drive.google.com/drive/folders/1GwqXRVYBpKGolNh2OLEzWUdOHx2XQ6G2\n",
        "\n"
      ],
      "metadata": {
        "id": "krSZmIpDI5Mg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ImageNet (Focal Loss)\n",
        "src: https://github.com/richardaecn/class-balanced-loss"
      ],
      "metadata": {
        "id": "zjt9a44hJoVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/richardaecn/class-balanced-loss\n",
        "import gdown\n",
        "url = 'https://drive.google.com/uc?id=1SmLv1-D1143Cma4Y5bDxHUfXjOI_0Yvr'\n",
        "gdown.download(url,'imagenet.zip',quiet=False) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "FyFn4KHKH4lx",
        "outputId": "1276e764-4fb4-4956-dab0-553ebaeeb160"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1SmLv1-D1143Cma4Y5bDxHUfXjOI_0Yvr\n",
            "To: /content/imagenet.zip\n",
            "100%|██████████| 1.00G/1.00G [00:08<00:00, 115MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'imagenet.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q imagenet.zip"
      ],
      "metadata": {
        "id": "9V4lRTZDJ_Yi"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.resnet50(pretrained=False)\n",
        "model = nn.DataParallel(model)\n",
        "model.load_state_dict(torch.load('imagenet/model.ckpt-111339.data-00000-of-00001'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "65_DqKxsKE82",
        "outputId": "9bbb56b7-9beb-4cbf-84a7-1f05359b9099"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnpicklingError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-e53fe60b0eb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet50\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'imagenet/model.ckpt-111339.data-00000-of-00001'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    711\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    918\u001b[0m             \"functionality.\")\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m     \u001b[0mmagic_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    921\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmagic_number\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mMAGIC_NUMBER\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid magic number; corrupt file?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnpicklingError\u001b[0m: invalid load key, '\\xf2'."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xo_ptnG4LzaQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}