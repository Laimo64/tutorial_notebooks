{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOx3htV9GrPPUvZPxVM9lAH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/tutorial_notebooks/blob/main/Attention_for_Captioning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initial Imports and loading the utils function. The dataset is used is Flickr 8k from kaggle.\n",
        "Custom dataset and dataloader is implemented in this notebook.\n",
        "\n",
        "SRC: https://www.kaggle.com/code/mdteach/torch-data-loader-flicker-8k\n",
        "\n",
        "Dataset: A new benchmark collection for sentence-based image description and search, consisting of 8,000 images that are each paired with five different captions which provide clear descriptions of the salient entities and events. … The images were chosen from six different Flickr groups, and tend not to contain any well-known people or locations, but were manually selected to depict a variety of scenes and situations\n",
        "\n",
        "SRC: https://www.kaggle.com/datasets/adityajn105/flickr8k"
      ],
      "metadata": {
        "id": "rSxPD6LMd5Kx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -L -s -o archive.zip 'https://storage.googleapis.com/kaggle-data-sets/623289/1111676/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20221206%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20221206T121324Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=90ee374982b4630c450545bfc20816bffdcb9467b06b9b53c4d429044dfd5de8c1f90e11b10d003a43d2ccc37d885cb00e4d3d1d10b3dfdef04d00775e6fbf9119df8a290f7c44f018bd30072e1a7ebb20003aeb561e87546f2a569b68d99b74341d046a8e2aaca6161be27b30c6fb57623d5e7ce8dae0e332cf00ec73d33a4dc7014637fd9d55d3a7165ea2a750f5fa1e1dd7b2fce896c0ac226ff6d01fe27f7a07365d9992400812e5e7eb2187d4df9f6536b0d69cb61f33493d7de287875207351b30dadb3a1d206c2c36817b9ac2b22faa2b99d67ec08c2eb6e2eb8486fa802fc89ed851d2eb1dafd7470303b4c8b1299aad0b1a51bf0ca8d0d66b697480'"
      ],
      "metadata": {
        "id": "bZe1o-8DW-QG"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir flickr8k\n",
        "!unzip -q archive.zip -d flickr8k"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N89PdeNLXRg6",
        "outputId": "8b5caa02-d215-4986-e206-2187e5b2f4b0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘flickr8k’: File exists\n",
            "replace flickr8k/Images/1000268201_693b08cb0e.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset summary"
      ],
      "metadata": {
        "id": "5Oh-1jQeeuV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "#location of the training data \n",
        "data_location =  \"flickr8k\"\n",
        "caption_file = data_location + '/captions.txt'\n",
        "df = pd.read_csv(caption_file)\n",
        "print(\"There are {} image to captions\".format(len(df)))\n",
        "df.head(7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "r_BN7EHzeect",
        "outputId": "89f8bd37-1926-486a-bbbc-67a76592bdec"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 40455 image to captions\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                       image  \\\n",
              "0  1000268201_693b08cb0e.jpg   \n",
              "1  1000268201_693b08cb0e.jpg   \n",
              "2  1000268201_693b08cb0e.jpg   \n",
              "3  1000268201_693b08cb0e.jpg   \n",
              "4  1000268201_693b08cb0e.jpg   \n",
              "5  1001773457_577c3a7d70.jpg   \n",
              "6  1001773457_577c3a7d70.jpg   \n",
              "\n",
              "                                             caption  \n",
              "0  A child in a pink dress is climbing up a set o...  \n",
              "1              A girl going into a wooden building .  \n",
              "2   A little girl climbing into a wooden playhouse .  \n",
              "3  A little girl climbing the stairs to her playh...  \n",
              "4  A little girl in a pink dress going into a woo...  \n",
              "5         A black dog and a spotted dog are fighting  \n",
              "6  A black dog and a tri-colored dog playing with...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-755aa197-acb4-4cfb-a83e-c59a893a71c6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image</th>\n",
              "      <th>caption</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "      <td>A child in a pink dress is climbing up a set o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "      <td>A girl going into a wooden building .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "      <td>A little girl climbing into a wooden playhouse .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "      <td>A little girl climbing the stairs to her playh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "      <td>A little girl in a pink dress going into a woo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1001773457_577c3a7d70.jpg</td>\n",
              "      <td>A black dog and a spotted dog are fighting</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1001773457_577c3a7d70.jpg</td>\n",
              "      <td>A black dog and a tri-colored dog playing with...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-755aa197-acb4-4cfb-a83e-c59a893a71c6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-755aa197-acb4-4cfb-a83e-c59a893a71c6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-755aa197-acb4-4cfb-a83e-c59a893a71c6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#imports \n",
        "import os\n",
        "from collections import Counter\n",
        "import spacy\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image\n",
        "#using spacy for the better text tokenization \n",
        "spacy_eng = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "class Vocabulary:\n",
        "    def __init__(self,freq_threshold):\n",
        "        #setting the pre-reserved tokens int to string tokens\n",
        "        self.itos = {0:\"<PAD>\",1:\"<SOS>\",2:\"<EOS>\",3:\"<UNK>\"}\n",
        "        \n",
        "        #string to int tokens\n",
        "        #its reverse dict self.itos\n",
        "        self.stoi = {v:k for k,v in self.itos.items()}\n",
        "        \n",
        "        self.freq_threshold = freq_threshold\n",
        "        \n",
        "    def __len__(self): return len(self.itos)\n",
        "    \n",
        "    @staticmethod\n",
        "    def tokenize(text):\n",
        "        return [token.text.lower() for token in spacy_eng.tokenizer(text)]\n",
        "    \n",
        "    def build_vocab(self, sentence_list):\n",
        "        frequencies = Counter()\n",
        "        idx = 4\n",
        "        \n",
        "        for sentence in sentence_list:\n",
        "            for word in self.tokenize(sentence):\n",
        "                frequencies[word] += 1\n",
        "                \n",
        "                #add the word to the vocab if it reaches minum frequecy threshold\n",
        "                if frequencies[word] == self.freq_threshold:\n",
        "                    self.stoi[word] = idx\n",
        "                    self.itos[idx] = word\n",
        "                    idx += 1\n",
        "    \n",
        "    def numericalize(self,text):\n",
        "        \"\"\" For each word in the text corresponding index token for that word form the vocab built as list \"\"\"\n",
        "        tokenized_text = self.tokenize(text)\n",
        "        return [ self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"] for token in tokenized_text ] \n",
        "\n",
        "class FlickrDataset(Dataset):\n",
        "    \"\"\"\n",
        "    FlickrDataset\n",
        "    \"\"\"\n",
        "    def __init__(self, root_dir=None, caption_file=None, transform=None, freq_threshold=5):\n",
        "        self.root_dir = root_dir\n",
        "        self.df = pd.read_csv(caption_file)\n",
        "        self.transform = transform\n",
        "        \n",
        "        #Get image and caption colum from the dataframe\n",
        "        self.imgs = self.df[\"image\"]\n",
        "        self.captions = self.df[\"caption\"]\n",
        "        \n",
        "        #Initialize vocabulary and build vocab\n",
        "        self.vocab = Vocabulary(freq_threshold)\n",
        "        self.vocab.build_vocab(self.captions.tolist())\n",
        "        \n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    def __getitem__(self,idx):\n",
        "        caption = self.captions[idx]\n",
        "        img_name = self.imgs[idx]\n",
        "        img_location = os.path.join(self.root_dir,img_name)\n",
        "        img = Image.open(img_location).convert(\"RGB\")\n",
        "        \n",
        "        #apply the transfromation to the image\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "        \n",
        "        #numericalize the caption text\n",
        "        caption_vec = []\n",
        "        caption_vec += [self.vocab.stoi[\"<SOS>\"]]\n",
        "        caption_vec += self.vocab.numericalize(caption)\n",
        "        caption_vec += [self.vocab.stoi[\"<EOS>\"]]\n",
        "        \n",
        "        return img, torch.tensor(caption_vec)"
      ],
      "metadata": {
        "id": "iwuPecueY1ii"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "import torchvision.transforms as T\n",
        "\n",
        "#show the tensor image\n",
        "import matplotlib.pyplot as plt\n",
        "def show_image(img, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    \n",
        "    #unnormalize \n",
        "    img[0] = img[0] * 0.229\n",
        "    img[1] = img[1] * 0.224 \n",
        "    img[2] = img[2] * 0.225 \n",
        "    img[0] += 0.485 \n",
        "    img[1] += 0.456 \n",
        "    img[2] += 0.406\n",
        "    \n",
        "    img = img.numpy().transpose((1, 2, 0))\n",
        "    \n",
        "    \n",
        "    plt.imshow(img)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "\n",
        "\n",
        "class CapsCollate:\n",
        "    \"\"\"\n",
        "    Collate to apply the padding to the captions with dataloader\n",
        "    \"\"\"\n",
        "    def __init__(self,pad_idx,batch_first=False):\n",
        "        self.pad_idx = pad_idx\n",
        "        self.batch_first = batch_first\n",
        "    \n",
        "    def __call__(self,batch):\n",
        "        imgs = [item[0].unsqueeze(0) for item in batch]\n",
        "        imgs = torch.cat(imgs,dim=0)\n",
        "        \n",
        "        targets = [item[1] for item in batch]\n",
        "        targets = pad_sequence(targets, batch_first=self.batch_first, padding_value=self.pad_idx)\n",
        "        return imgs,targets\n",
        "\n",
        "#Initiate the Dataset and Dataloader\n",
        "\n",
        "#setting the constants\n",
        "data_location =  \"flickr8k\"\n",
        "BATCH_SIZE = 256\n",
        "# BATCH_SIZE = 6\n",
        "NUM_WORKER = 4\n",
        "\n",
        "#defining the transform to be applied\n",
        "transforms = T.Compose([\n",
        "    T.Resize(226),                     \n",
        "    T.RandomCrop(224),                 \n",
        "    T.ToTensor(),                               \n",
        "    T.Normalize((0.485, 0.456, 0.406),(0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "\n",
        "#testing the dataset class\n",
        "dataset =  FlickrDataset(\n",
        "    root_dir = data_location+\"/Images\", caption_file = data_location+\"/captions.txt\", transform=transforms\n",
        ")\n",
        "\n",
        "#writing the dataloader\n",
        "pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "    dataset=dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=NUM_WORKER,\n",
        "    shuffle=True,\n",
        "    collate_fn=CapsCollate(pad_idx=pad_idx, batch_first=True)\n",
        ")\n",
        "\n",
        "#vocab_size\n",
        "vocab_size = len(dataset.vocab)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERJnB9EhYecl",
        "outputId": "fa0da08d-91dc-47ae-fd08-36b2421f3754"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Architecture\n",
        "\n",
        "Model is seq2seq model. In the encoder pretrained ResNet model is used to extract the features. Decoder, is the implementation of the Bahdanau Attention Decoder. In the decoder model LSTM cell."
      ],
      "metadata": {
        "id": "r3Iove06bXgX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "import torchvision.transforms as T\n",
        "\n",
        "class EncoderCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EncoderCNN, self).__init__()\n",
        "        resnet = models.resnet50(pretrained=True)\n",
        "        for param in resnet.parameters():\n",
        "            param.requires_grad_(False)\n",
        "        \n",
        "        modules = list(resnet.children())[:-2]\n",
        "        self.resnet = nn.Sequential(*modules)\n",
        "        \n",
        "\n",
        "    def forward(self, images):\n",
        "        features = self.resnet(images)                                    #(batch_size,2048,7,7)\n",
        "        features = features.permute(0, 2, 3, 1)                           #(batch_size,7,7,2048)\n",
        "        features = features.view(features.size(0), -1, features.size(-1)) #(batch_size,49,2048)\n",
        "        return features\n",
        "\n",
        "#Bahdanau Attention\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, encoder_dim,decoder_dim,attention_dim):\n",
        "        super(Attention, self).__init__()\n",
        "        \n",
        "        self.attention_dim = attention_dim\n",
        "        \n",
        "        self.W = nn.Linear(decoder_dim,attention_dim)\n",
        "        self.U = nn.Linear(encoder_dim,attention_dim)\n",
        "        \n",
        "        self.A = nn.Linear(attention_dim,1)\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "    def forward(self, features, hidden_state):\n",
        "        u_hs = self.U(features)     #(batch_size,num_layers,attention_dim)\n",
        "        w_ah = self.W(hidden_state) #(batch_size,attention_dim)\n",
        "        \n",
        "        combined_states = torch.tanh(u_hs + w_ah.unsqueeze(1)) #(batch_size,num_layers,attemtion_dim)\n",
        "        \n",
        "        attention_scores = self.A(combined_states)         #(batch_size,num_layers,1)\n",
        "        attention_scores = attention_scores.squeeze(2)     #(batch_size,num_layers)\n",
        "        \n",
        "        \n",
        "        alpha = F.softmax(attention_scores,dim=1)          #(batch_size,num_layers)\n",
        "        \n",
        "        attention_weights = features * alpha.unsqueeze(2)  #(batch_size,num_layers,features_dim)\n",
        "        attention_weights = attention_weights.sum(dim=1)   #(batch_size,num_layers)\n",
        "        \n",
        "        return alpha,attention_weights\n",
        "\n",
        "#Attention Decoder\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self,embed_size, vocab_size, attention_dim,encoder_dim,decoder_dim,drop_prob=0.3):\n",
        "        super().__init__()\n",
        "        \n",
        "        #save the model param\n",
        "        self.vocab_size = vocab_size\n",
        "        self.attention_dim = attention_dim\n",
        "        self.decoder_dim = decoder_dim\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size,embed_size)\n",
        "        self.attention = Attention(encoder_dim,decoder_dim,attention_dim)\n",
        "        \n",
        "        \n",
        "        self.init_h = nn.Linear(encoder_dim, decoder_dim)  \n",
        "        self.init_c = nn.Linear(encoder_dim, decoder_dim)  \n",
        "        self.lstm_cell = nn.LSTMCell(embed_size+encoder_dim,decoder_dim,bias=True)\n",
        "        self.f_beta = nn.Linear(decoder_dim, encoder_dim)\n",
        "        \n",
        "        \n",
        "        self.fcn = nn.Linear(decoder_dim,vocab_size)\n",
        "        self.drop = nn.Dropout(drop_prob)\n",
        "        \n",
        "        \n",
        "    \n",
        "    def forward(self, features, captions):\n",
        "        \n",
        "        #vectorize the caption\n",
        "        embeds = self.embedding(captions)\n",
        "        \n",
        "        # Initialize LSTM state\n",
        "        h, c = self.init_hidden_state(features)  # (batch_size, decoder_dim)\n",
        "        \n",
        "        #get the seq length to iterate\n",
        "        seq_length = len(captions[0])-1 #Exclude the last one\n",
        "        batch_size = captions.size(0)\n",
        "        num_features = features.size(1)\n",
        "        \n",
        "        preds = torch.zeros(batch_size, seq_length, self.vocab_size).to(device)\n",
        "        alphas = torch.zeros(batch_size, seq_length,num_features).to(device)\n",
        "                \n",
        "        for s in range(seq_length):\n",
        "            alpha,context = self.attention(features, h)\n",
        "            lstm_input = torch.cat((embeds[:, s], context), dim=1)\n",
        "            h, c = self.lstm_cell(lstm_input, (h, c))\n",
        "                    \n",
        "            output = self.fcn(self.drop(h))\n",
        "            \n",
        "            preds[:,s] = output\n",
        "            alphas[:,s] = alpha  \n",
        "        \n",
        "        \n",
        "        return preds, alphas\n",
        "    \n",
        "    def generate_caption(self,features,max_len=20,vocab=None):\n",
        "        # Inference part\n",
        "        # Given the image features generate the captions\n",
        "        \n",
        "        batch_size = features.size(0)\n",
        "        h, c = self.init_hidden_state(features)  # (batch_size, decoder_dim)\n",
        "        \n",
        "        alphas = []\n",
        "        \n",
        "        #starting input\n",
        "        word = torch.tensor(vocab.stoi['<SOS>']).view(1,-1).to(device)\n",
        "        embeds = self.embedding(word)\n",
        "\n",
        "        \n",
        "        captions = []\n",
        "        \n",
        "        for i in range(max_len):\n",
        "            alpha,context = self.attention(features, h)\n",
        "            \n",
        "            \n",
        "            #store the apla score\n",
        "            alphas.append(alpha.cpu().detach().numpy())\n",
        "            \n",
        "            lstm_input = torch.cat((embeds[:, 0], context), dim=1)\n",
        "            h, c = self.lstm_cell(lstm_input, (h, c))\n",
        "            output = self.fcn(self.drop(h))\n",
        "            output = output.view(batch_size,-1)\n",
        "        \n",
        "            \n",
        "            #select the word with most val\n",
        "            predicted_word_idx = output.argmax(dim=1)\n",
        "            \n",
        "            #save the generated word\n",
        "            captions.append(predicted_word_idx.item())\n",
        "            \n",
        "            #end if <EOS detected>\n",
        "            if vocab.itos[predicted_word_idx.item()] == \"<EOS>\":\n",
        "                break\n",
        "            \n",
        "            #send generated word as the next caption\n",
        "            embeds = self.embedding(predicted_word_idx.unsqueeze(0))\n",
        "        \n",
        "        #covert the vocab idx to words and return sentence\n",
        "        return [vocab.itos[idx] for idx in captions],alphas\n",
        "    \n",
        "    \n",
        "    def init_hidden_state(self, encoder_out):\n",
        "        mean_encoder_out = encoder_out.mean(dim=1)\n",
        "        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n",
        "        c = self.init_c(mean_encoder_out)\n",
        "        return h, c\n",
        "\n",
        "class EncoderDecoder(nn.Module):\n",
        "    def __init__(self,embed_size, vocab_size, attention_dim,encoder_dim,decoder_dim,drop_prob=0.3):\n",
        "        super().__init__()\n",
        "        self.encoder = EncoderCNN()\n",
        "        self.decoder = DecoderRNN(\n",
        "            embed_size=embed_size,\n",
        "            vocab_size = len(dataset.vocab),\n",
        "            attention_dim=attention_dim,\n",
        "            encoder_dim=encoder_dim,\n",
        "            decoder_dim=decoder_dim\n",
        "        )\n",
        "        \n",
        "    def forward(self, images, captions):\n",
        "        features = self.encoder(images)\n",
        "        outputs = self.decoder(features, captions)\n",
        "        return outputs\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZRhTf7xKYfCo"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training"
      ],
      "metadata": {
        "id": "jicpyh6mcLvM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparams\n",
        "embed_size=300\n",
        "vocab_size = len(dataset.vocab)\n",
        "attention_dim=256\n",
        "encoder_dim=2048\n",
        "decoder_dim=512\n",
        "learning_rate = 3e-4\n",
        "\n",
        "#init model\n",
        "model = EncoderDecoder(\n",
        "    embed_size=300,\n",
        "    vocab_size = len(dataset.vocab),\n",
        "    attention_dim=256,\n",
        "    encoder_dim=2048,\n",
        "    decoder_dim=512\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#helper function to save the model\n",
        "def save_model(model,num_epochs):\n",
        "    model_state = {\n",
        "        'num_epochs':num_epochs,\n",
        "        'embed_size':embed_size,\n",
        "        'vocab_size':len(dataset.vocab),\n",
        "        'attention_dim':attention_dim,\n",
        "        'encoder_dim':encoder_dim,\n",
        "        'decoder_dim':decoder_dim,\n",
        "        'state_dict':model.state_dict()\n",
        "    }\n",
        "\n",
        "    torch.save(model_state,'attention_model_state.pth')\n",
        "\n",
        "\n",
        "num_epochs = 5\n",
        "print_every = 100\n",
        "\n",
        "for epoch in range(1,num_epochs+1):   \n",
        "    for idx, (image, captions) in enumerate(iter(data_loader)):\n",
        "        image,captions = image.to(device),captions.to(device)\n",
        "\n",
        "        # Zero the gradients.\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Feed forward\n",
        "        outputs,attentions = model(image, captions)\n",
        "\n",
        "        # Calculate the batch loss.\n",
        "        targets = captions[:,1:]\n",
        "        loss = criterion(outputs.view(-1, vocab_size), targets.reshape(-1))\n",
        "        \n",
        "        # Backward pass.\n",
        "        loss.backward()\n",
        "\n",
        "        # Update the parameters in the optimizer.\n",
        "        optimizer.step()\n",
        "\n",
        "        if (idx+1)%print_every == 0:\n",
        "            print(\"Epoch: {} loss: {:.5f}\".format(epoch,loss.item()))\n",
        "            \n",
        "            \n",
        "            #generate the caption\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                dataiter = iter(data_loader)\n",
        "                img,_ = next(dataiter)\n",
        "                features = model.encoder(img[0:1].to(device))\n",
        "                caps,alphas = model.decoder.generate_caption(features,vocab=dataset.vocab)\n",
        "                caption = ' '.join(caps)\n",
        "                show_image(img[0],title=caption)\n",
        "                \n",
        "            model.train()\n",
        "        \n",
        "    #save the latest model\n",
        "    save_model(model,epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gnmqRmocJ6U",
        "outputId": "dde79921-dad3-4b3a-a45c-c07e67934a22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Visualizing the attentions\n",
        "Defining helper functions\n",
        "\n",
        "Given the image generate captions and attention scores\n",
        "Plot the attention scores in the image"
      ],
      "metadata": {
        "id": "OhbDaskQdj-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#generate caption\n",
        "def get_caps_from(features_tensors):\n",
        "    #generate the caption\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        features = model.encoder(features_tensors.to(device))\n",
        "        caps,alphas = model.decoder.generate_caption(features,vocab=dataset.vocab)\n",
        "        caption = ' '.join(caps)\n",
        "        show_image(features_tensors[0],title=caption)\n",
        "    \n",
        "    return caps,alphas\n",
        "\n",
        "#Show attention\n",
        "def plot_attention(img, result, attention_plot):\n",
        "    #untransform\n",
        "    img[0] = img[0] * 0.229\n",
        "    img[1] = img[1] * 0.224 \n",
        "    img[2] = img[2] * 0.225 \n",
        "    img[0] += 0.485 \n",
        "    img[1] += 0.456 \n",
        "    img[2] += 0.406\n",
        "    \n",
        "    img = img.numpy().transpose((1, 2, 0))\n",
        "    temp_image = img\n",
        "\n",
        "    fig = plt.figure(figsize=(15, 15))\n",
        "\n",
        "    len_result = len(result)\n",
        "    for l in range(len_result):\n",
        "        temp_att = attention_plot[l].reshape(7,7)\n",
        "        \n",
        "        ax = fig.add_subplot(len_result//2,len_result//2, l+1)\n",
        "        ax.set_title(result[l])\n",
        "        img = ax.imshow(temp_image)\n",
        "        ax.imshow(temp_att, cmap='gray', alpha=0.7, extent=img.get_extent())\n",
        "        \n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "foQUQrJfcYha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#show any 1\n",
        "dataiter = iter(data_loader)\n",
        "images,_ = next(dataiter)\n",
        "\n",
        "img = images[0].detach().clone()\n",
        "img1 = images[0].detach().clone()\n",
        "caps,alphas = get_caps_from(img.unsqueeze(0))\n",
        "\n",
        "plot_attention(img1, caps, alphas)"
      ],
      "metadata": {
        "id": "lqnjPZY8dpGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#show any 1\n",
        "dataiter = iter(data_loader)\n",
        "images,_ = next(dataiter)\n",
        "\n",
        "img = images[0].detach().clone()\n",
        "img1 = images[0].detach().clone()\n",
        "caps,alphas = get_caps_from(img.unsqueeze(0))\n",
        "\n",
        "plot_attention(img1, caps, alphas)"
      ],
      "metadata": {
        "id": "wJFLGav5dthE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#show any 1\n",
        "dataiter = iter(data_loader)\n",
        "images,_ = next(dataiter)\n",
        "\n",
        "img = images[0].detach().clone()\n",
        "img1 = images[0].detach().clone()\n",
        "caps,alphas = get_caps_from(img.unsqueeze(0))\n",
        "\n",
        "plot_attention(img1, caps, alphas)"
      ],
      "metadata": {
        "id": "KzjbmSOvduJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EzrjJz_2gCx-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}