{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMGNRxoyxWBKVgg4xG4c/D8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/tutorial_notebooks/blob/main/ImageNet_Dataloader_Transformer_All.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ImageNet (ILSVRC2012)\n",
        "\n",
        "It contains 1000 classes, 1.28 million training images, and 50 thousand validation images. There are 1,281,167 images and 732-1300 per class in the ILSVRC2012 training set. This dataset spans 1000 object classes and contains 1,281,167 training images, 50,000 validation images and 100,000 test images. It requires more than 150GB of storage, and training a resnet50 on it will take around 215 hours using a T4 GPU on Google Colab. Folder name to actual class mapping: https://www.image-net.org/challenges/LSVRC/2012/browse-synsets.php <br>\n",
        "Sample size is not equal in ImageNet. For example top 10 classes:<br>\n",
        "n02094433:    3047 (Yorkshire terrier)<br>\n",
        "n02086240:    2563 (Shih-Tzu)<br>\n",
        "n01882714:    2469 (koala bear, kangaroo bear, native bear, )<br>\n",
        "n02087394:    2449 (Rhodesian ridgeback)<br>\n",
        "n02100735:    2426 (English setter)<br>\n",
        "n00483313:    2410 (singles)<br>\n",
        "n02279972:    2386 (monarch butterfly, Danaus plexippus)<br>\n",
        "n09428293:    2382 (seashore)<br>\n",
        "n02138441:    2341 (meerkat)<br>\n",
        "n02100583:    2334 (vizsla, Hungarian pointer)<br>\n",
        "\n",
        "\n",
        "Task-1. Image classification (2010-2014): Algorithms produce a list of object categories present in the image.<br>\n",
        "Task-2. Single-object localization (2011-2014): Algorithms\n",
        "produce a list of object categories present in the image, along with an axis-aligned bounding box indicating the position and scale of one instance of each object category.<br>\n",
        "Task-3. Object detection (2013-2014): Algorithms produce\n",
        "a list of object categories present in the image along\n",
        "with an axis-aligned bounding box indicating the\n",
        "position and scale of every instance of each object\n",
        "category.<br>\n",
        "\n",
        "#Download Links:\n",
        "\n",
        "Training Images (taskl&2): https://image-net.org/data/ILSVRC/2012/ILSVRC2012_img_train.tar <br>\n",
        "Training Annotations (taskl&2): https://image-net.org/data/ILSVRC/2012/ILSVRC2012_bbox_train_v2.tar.gz <br>\n",
        "\n",
        "Validation Images (all tasks): https://image-net.org/data/ILSVRC/2012/ILSVRC2012_img_val.tar\n",
        "\n",
        "Validation Annotations (all tasks): https://image-net.org/data/ILSVRC/2012/ILSVRC2012_bbox_val_v3.tgz\n"
      ],
      "metadata": {
        "id": "KHwLWpzQGmJA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing Train Images into Folders (Not using in this tutorial)\n",
        "src: https://github.com/pytorch/examples/blob/main/imagenet/extract_ILSVRC.sh"
      ],
      "metadata": {
        "id": "qtbN1219glYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create train directory; move .tar file; change directory\n",
        "!mkdir imagenet/train && mv ILSVRC2012_img_train.tar imagenet/train/ && cd imagenet/train\n",
        "# Extract training set; remove compressed file\n",
        "!tar -xvf ILSVRC2012_img_train.tar && rm -f ILSVRC2012_img_train.tar\n",
        "#\n",
        "# At this stage imagenet/train will contain 1000 compressed .tar files, one for each category\n",
        "#\n",
        "# For each .tar file: \n",
        "#   1. create directory with same name as .tar file\n",
        "#   2. extract and copy contents of .tar file into directory\n",
        "#   3. remove .tar file\n",
        "!find . -name \"*.tar\" | while read NAME ; do mkdir -p \"${NAME%.tar}\"; tar -xvf \"${NAME}\" -C \"${NAME%.tar}\"; rm -f \"${NAME}\"; done"
      ],
      "metadata": {
        "id": "piDwRwOIgkQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download only Validation Set"
      ],
      "metadata": {
        "id": "2SlLraVFT635"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://image-net.org/data/ILSVRC/2012/ILSVRC2012_img_val.tar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YlcVUkDc1on",
        "outputId": "00ff1774-713f-4dc5-9857-ac116ac7d88a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-21 22:51:11--  https://image-net.org/data/ILSVRC/2012/ILSVRC2012_img_val.tar\n",
            "Resolving image-net.org (image-net.org)... 171.64.68.16\n",
            "Connecting to image-net.org (image-net.org)|171.64.68.16|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6744924160 (6.3G) [application/x-tar]\n",
            "Saving to: ‘ILSVRC2012_img_val.tar’\n",
            "\n",
            "ILSVRC2012_img_val. 100%[===================>]   6.28G  27.7MB/s    in 8m 35s  \n",
            "\n",
            "2022-10-21 22:59:45 (12.5 MB/s) - ‘ILSVRC2012_img_val.tar’ saved [6744924160/6744924160]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing Valid Images into Folders"
      ],
      "metadata": {
        "id": "wbsK_wL6g6RA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir imagenet\n",
        "!mkdir imagenet/val\n",
        "!tar -xvf ILSVRC2012_img_val.tar --directory imagenet/val\n",
        "%cd imagenet/val\n",
        "!wget -qO- https://raw.githubusercontent.com/soumith/imagenetloader.torch/master/valprep.sh | bash\n",
        "%cd ../.."
      ],
      "metadata": {
        "id": "4_XdkO8Tc-13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "import shutil\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.distributed as dist\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "import torch.utils.data.distributed\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.models as models\n",
        "\n",
        "def test(model, testloader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "    return correct / total\n",
        "\n",
        "valdir = os.path.join('imagenet', 'val')\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                    std=[0.229, 0.224, 0.225])\n",
        "\n",
        "\n",
        "val_dataset = datasets.ImageFolder(valdir, transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "    ]))\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=512, shuffle=False,\n",
        "    num_workers=2, pin_memory=True)\n",
        "\n",
        "print('Sample size:', len(val_dataset))\n",
        "for i, (input, target) in enumerate(val_loader):\n",
        "    print('First batch:',input.shape, target)\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9hFROHsZRxQ",
        "outputId": "1f008bdc-8c20-495a-cafe-cbc5bea659fe"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample size: 50000\n",
            "First batch: torch.Size([512, 3, 224, 224]) tensor([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  1,  1,\n",
            "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
            "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
            "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,\n",
            "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
            "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
            "         2,  2,  2,  2,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
            "         3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
            "         3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
            "         3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
            "         4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
            "         4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  5,  5,\n",
            "         5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
            "         5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
            "         5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,\n",
            "         6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
            "         6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
            "         6,  6,  6,  6,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,\n",
            "         7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,\n",
            "         7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,\n",
            "         7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
            "         8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
            "         8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
            "         9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,\n",
            "         9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,\n",
            "         9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10, 10, 10,\n",
            "        10, 10, 10, 10, 10, 10, 10, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Vision Transformer and Variants\n",
        "Basic: https://github.com/mobarakol/tutorial_notebooks/blob/main/ViT_Module_Visualization.ipynb<br>\n",
        "Installation:<br>\n",
        "github: https://github.com/rwightman/pytorch-image-models/tree/master/timm/models"
      ],
      "metadata": {
        "id": "J6wu1OXrmyBe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install timm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KilVWig1mzRk",
        "outputId": "ccef92bb-7b46-427f-a968-b236f904d0db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 548 kB 9.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 163 kB 61.3 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ViT: AN IMAGE IS WORTH 16X16 WORDS:\n",
        "TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE - https://arxiv.org/pdf/2010.11929.pdf"
      ],
      "metadata": {
        "id": "Z-LU-njAmGdz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from timm import create_model\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "vit = create_model(\"vit_large_patch16_224\", pretrained=True).to(device)#vit_base_patch16_224\n",
        "accuracy = test(vit, val_loader)\n",
        "print('accuracy:',accuracy)"
      ],
      "metadata": {
        "id": "cDj0Q7bjf08m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62f70acd-8107-4bdf-c58e-671119da54df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.84374\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Swin-Transformer: Hierarchical Vision Transformer using Shifted Windows -https://arxiv.org/pdf/2103.14030.pdf"
      ],
      "metadata": {
        "id": "tACc7uMhnAf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "swintran = create_model(\"swin_base_patch4_window7_224\", pretrained=True).to(device)\n",
        "accuracy = test(swintran, val_loader)\n",
        "print('accuracy:',accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ih04ZKrm8UW",
        "outputId": "826d102a-5ba8-4498-da25-73d0d094c83c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "Downloading: \"https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_base_patch4_window7_224_22kto1k.pth\" to /root/.cache/torch/hub/checkpoints/swin_base_patch4_window7_224_22kto1k.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.84714\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DeiT: Data-efficient Image Transformers - https://arxiv.org/abs/2012.12877"
      ],
      "metadata": {
        "id": "FD6by7W1nF90"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "deit = create_model(\"deit_base_patch16_224\", pretrained=True).to(device)\n",
        "accuracy = test(deit, val_loader)\n",
        "print('accuracy:',accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aw7Hb46mnINX",
        "outputId": "26e38478-b7d4-4313-a75f-49ca6e0fa987"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth\" to /root/.cache/torch/hub/checkpoints/deit_base_patch16_224-b5f2ef4d.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.81742\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CaiT: Class-Attention in Image Transformers (https://arxiv.org/abs/2103.17239)"
      ],
      "metadata": {
        "id": "1tSEzCnonKmb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cait = create_model(\"cait_s24_224\", pretrained=True).to(device)\n",
        "accuracy = test(cait, val_loader)\n",
        "print('accuracy:',accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cozK6pPtnNJF",
        "outputId": "50ba3b38-a2a0-49dd-c071-854cb35024fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://dl.fbaipublicfiles.com/deit/S24_224.pth\" to /root/.cache/torch/hub/checkpoints/S24_224.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.83302\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BeiT: BERT Pre-Training of Image Transformers (https://arxiv.org/abs/2106.08254)"
      ],
      "metadata": {
        "id": "FO4IesNynRsF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from timm import create_model\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "beit = create_model(\"beitv2_base_patch16_224\", pretrained=True).to(device)\n",
        "accuracy = test(beit, val_loader)\n",
        "print('accuracy:',accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahpQ8WYInOq2",
        "outputId": "b8bfce77-8bbb-4d0a-8697-9171533b609c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://conversationhub.blob.core.windows.net/beit-share-public/beitv2/beitv2_base_patch16_224_pt1k_ft21kto1k.pth\" to /root/.cache/torch/hub/checkpoints/beitv2_base_patch16_224_pt1k_ft21kto1k.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.86092\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CoaT: Co-Scale Conv-Attentional Image Transformers - https://arxiv.org/abs/2104.06399"
      ],
      "metadata": {
        "id": "qB-NX353nUK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "coat = create_model(\"coat_mini\", pretrained=True).to(device)\n",
        "accuracy = test(coat, val_loader)\n",
        "print('accuracy:',accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8I8yBrITnW1w",
        "outputId": "71765cf9-c5b9-4414-ab24-e109a5c5ef4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-coat-weights/coat_mini-2c6baf49.pth\" to /root/.cache/torch/hub/checkpoints/coat_mini-2c6baf49.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.80912\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification (et al. ICCV 2021)"
      ],
      "metadata": {
        "id": "NM9Jq8_co4Zj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "crossvit = create_model(\"crossvit_base_240\", pretrained=True).to(device)\n",
        "accuracy = test(crossvit, val_loader)\n",
        "print('accuracy:',accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgEfi640o4wr",
        "outputId": "9fbd0359-9e1f-411d-8897-43b5d2155f70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/IBM/CrossViT/releases/download/weights-0.1/crossvit_base_224.pth\" to /root/.cache/torch/hub/checkpoints/crossvit_base_224.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.82092\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ConvMixer: Patches Are All You Need? (https://arxiv.org/pdf/2201.09792.pdf)"
      ],
      "metadata": {
        "id": "n18k-EuwpUYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "convmixer = create_model(\"convmixer_768_32\", pretrained=True).to(device)\n",
        "accuracy = test(convmixer, val_loader)\n",
        "print('accuracy:',accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRAjH7LIpU_7",
        "outputId": "44144e76-50d7-4fa5-f218-02a56438400f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/tmp-iclr/convmixer/releases/download/timm-v1.0/convmixer_768_32_ks7_p7_relu.pth.tar\" to /root/.cache/torch/hub/checkpoints/convmixer_768_32_ks7_p7_relu.pth.tar\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.8008\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ConvNeXt: A ConvNet for the 2020s - https://arxiv.org/pdf/2201.03545.pdf"
      ],
      "metadata": {
        "id": "kdvR55PRpqjd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "convnext = create_model(\"convnext_base\", pretrained=True).to(device)\n",
        "accuracy = test(convnext, val_loader)\n",
        "print('accuracy:',accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icakQq5QprDo",
        "outputId": "cd2aec94-187e-452a-eff4-97c0cf5c0d5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://dl.fbaipublicfiles.com/convnext/convnext_base_1k_224_ema.pth\" to /root/.cache/torch/hub/checkpoints/convnext_base_1k_224_ema.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.83746\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ViT_relpos: Rethinking and Improving Relative Position Encoding for Vision Transformer -https://arxiv.org/pdf/2107.14222.pdf"
      ],
      "metadata": {
        "id": "C8xo5NevQH6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vit_relpos = create_model(\"vit_relpos_base_patch16_cls_224\", pretrained=True).to(device) #vit_relpos_base_patch16_224\n",
        "accuracy = test(vit_relpos, val_loader)\n",
        "print('accuracy:',accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4Z9MueprSC1",
        "outputId": "60170b18-b2d1-489f-ab7a-7c3738d0906e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:timm.models.helpers:No pretrained weights exist or were found for this model. Using random initialization.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ViTs from https://github.com/jeonsworld/ViT-pytorch"
      ],
      "metadata": {
        "id": "wJu0NJPUv1Qk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install ml_collections\n",
        "! git clone https://github.com/jeonsworld/ViT-pytorch\n",
        "%cd ViT-pytorch\n",
        "! wget https://storage.googleapis.com/vit_models/imagenet21k%2Bimagenet2012/R50%2BViT-B_16.npz\n",
        "! touch models/__init__.py"
      ],
      "metadata": {
        "id": "ygrgDKh5Rl3x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4bbf098-e8b1-4f8c-b42c-b7f6950f57bb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |████▏                           | 10 kB 30.8 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 20 kB 37.4 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 30 kB 45.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 40 kB 26.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 51 kB 29.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 61 kB 33.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 71 kB 30.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 77 kB 5.9 MB/s \n",
            "\u001b[?25h  Building wheel for ml-collections (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Cloning into 'ViT-pytorch'...\n",
            "remote: Enumerating objects: 170, done.\u001b[K\n",
            "remote: Total 170 (delta 0), reused 0 (delta 0), pack-reused 170\u001b[K\n",
            "Receiving objects: 100% (170/170), 21.20 MiB | 36.25 MiB/s, done.\n",
            "Resolving deltas: 100% (83/83), done.\n",
            "/content/ViT-pytorch\n",
            "--2022-10-21 23:03:41--  https://storage.googleapis.com/vit_models/imagenet21k%2Bimagenet2012/R50%2BViT-B_16.npz\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.163.128, 172.217.15.80, 172.253.62.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.163.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 395916008 (378M) [application/octet-stream]\n",
            "Saving to: ‘R50+ViT-B_16.npz’\n",
            "\n",
            "R50+ViT-B_16.npz    100%[===================>] 377.57M  68.8MB/s    in 5.3s    \n",
            "\n",
            "2022-10-21 23:03:47 (71.8 MB/s) - ‘R50+ViT-B_16.npz’ saved [395916008/395916008]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! wget https://storage.googleapis.com/vit_models/imagenet21k%2Bimagenet2012/ViT-B_16-224.npz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FW2fbSzI0F3j",
        "outputId": "6166d79f-3eed-4442-e46c-3428b1884df8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-21 23:11:20--  https://storage.googleapis.com/vit_models/imagenet21k%2Bimagenet2012/ViT-B_16-224.npz\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.16.128, 142.251.33.208, 142.250.188.48, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.16.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 346335542 (330M) [application/octet-stream]\n",
            "Saving to: ‘ViT-B_16-224.npz’\n",
            "\n",
            "ViT-B_16-224.npz    100%[===================>] 330.29M  87.6MB/s    in 4.0s    \n",
            "\n",
            "2022-10-21 23:11:24 (83.1 MB/s) - ‘ViT-B_16-224.npz’ saved [346335542/346335542]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ViT-pytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oF36sC8g0m_w",
        "outputId": "762f3746-310f-4934-d021-450251b49f41"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ViT-pytorch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "import shutil\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.distributed as dist\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "import torch.utils.data.distributed\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.models as models\n",
        "\n",
        "from models.modeling import VisionTransformer, CONFIGS\n",
        "#config = CONFIGS['R50-ViT-B_16']\n",
        "config = CONFIGS['ViT-B_16']\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def test(model, testloader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)[0]\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "    return correct / total\n",
        "\n",
        "valdir = os.path.join('../imagenet', 'val')\n",
        "normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "\n",
        "\n",
        "val_dataset = datasets.ImageFolder(valdir, transforms.Compose([\n",
        "        transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "    ]))\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=300, shuffle=False,\n",
        "    num_workers=2, pin_memory=True)\n",
        "\n",
        "hvit = VisionTransformer(config, num_classes=1000, zero_head=False, img_size=224, vis=True)\n",
        "hvit.load_from(np.load(\"ViT-B_16-224.npz\"))\n",
        "hvit.to(device)\n",
        "accuracy = test(hvit, val_loader)\n",
        "print('accuracy:',accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WNZmU3HwK2U",
        "outputId": "97a9f90b-9109-4863-bf23-9d250d26ead8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.80314\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sQU1Ga2s4NWl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}