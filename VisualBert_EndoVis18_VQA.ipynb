{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNVAVQ668gSuLEnC5o3u6Iw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/tutorial_notebooks/blob/main/VisualBert_EndoVis18_VQA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0lkpVfSHTQ8",
        "outputId": "4b6e900e-3b20-4caa-8204-86b5f4f8853b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:138: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1WGdztykX3nW6pi_BKp4rO8nA7ESNRfVN\n",
            "From (redirected): https://drive.google.com/uc?id=1WGdztykX3nW6pi_BKp4rO8nA7ESNRfVN&confirm=t&uuid=8a22b3b8-fa97-4c31-8103-79b66d2e80f5\n",
            "To: /content/EndoVis-18-VQA.zip\n",
            "100% 2.70G/2.70G [01:00<00:00, 44.6MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Downloading the VQA EndoVis18 Dataset https://drive.google.com/file/d/1WGdztykX3nW6pi_BKp4rO8nA7ESNRfVN/view?usp=sharing\n",
        "!gdown --id 1WGdztykX3nW6pi_BKp4rO8nA7ESNRfVN\n",
        "\n",
        "# Unzipping the VQA EndoVis18 Dataset\\\n",
        "!unzip -q EndoVis-18-VQA.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# python3 train_classification.py train.py --lr=0.00001 --checkpoint_dir='checkpoints/vb/m18/vb_' \\\n",
        "#                                          --dataset_type='m18' --patch_size=5 \\\n",
        "#                                          --tokenizer_ver='v2' --model_ver='vb'"
      ],
      "metadata": {
        "id": "q4Emuo7jLLuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "import glob\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "from torch import nn\n",
        "from transformers import ViTFeatureExtractor, AutoFeatureExtractor\n",
        "\n",
        "class EndoVis18VQAGPTClassification(Dataset):\n",
        "    '''\n",
        "    \tseq: train_seq  = [2, 3, 4, 6, 7, 9, 10, 11, 12, 14, 15]\n",
        "    \t     val_seq    = [1, 5, 16]\n",
        "    \tfolder_head     = 'dataset/EndoVis-18-VQA/seq_'\n",
        "    \tfolder_tail     = '/vqa/Classification/*.txt'\n",
        "    '''\n",
        "    def __init__(self, seq, folder_head, folder_tail, img_feat_extractor = None, transform=None):\n",
        "\n",
        "        if img_feat_extractor == \"ViT\":\n",
        "            self.transform = None\n",
        "            self.image_processor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "        elif img_feat_extractor == \"Swin\":\n",
        "            self.transform = None\n",
        "            self.image_processor = AutoFeatureExtractor.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\")\n",
        "        elif img_feat_extractor == \"RN18\":\n",
        "            self.image_processor = models.resnet18(pretrained=True)\n",
        "            self.image_processor.fc = nn.Sequential(*list(self.image_processor.fc.children())[:-1])\n",
        "\n",
        "\n",
        "\n",
        "        # files, question and answers\n",
        "        filenames = []\n",
        "        for curr_seq in seq: filenames = filenames + glob.glob(folder_head + str(curr_seq) + folder_tail)\n",
        "        self.vqas = []\n",
        "        for file in filenames:\n",
        "            file_data = open(file, \"r\")\n",
        "            lines = [line.strip(\"\\n\") for line in file_data if line != \"\\n\"]\n",
        "            file_data.close()\n",
        "            for line in lines: self.vqas.append([file, line])\n",
        "        print('Total files: %d | Total question: %.d' %(len(filenames), len(self.vqas)))\n",
        "\n",
        "        # Labels\n",
        "        self.labels = ['kidney', 'Idle', 'Grasping', 'Retraction', 'Tissue_Manipulation',\n",
        "                        'Tool_Manipulation', 'Cutting', 'Cauterization', 'Suction',\n",
        "                        'Looping', 'Suturing', 'Clipping', 'Staple', 'Ultrasound_Sensing',\n",
        "                        'left-top', 'right-top', 'left-bottom', 'right-bottom']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.vqas)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        loc = self.vqas[idx][0].split('/')\n",
        "\n",
        "        # img\n",
        "        img_loc = os.path.join(loc[0],loc[1],loc[2], 'left_frames',loc[-1].split('_')[0]+'.png')\n",
        "        if self.transform:\n",
        "            img = Image.open(img_loc)\n",
        "            img = self.transform(img)\n",
        "        else:\n",
        "            img = self.image_processor(Image.open(img_loc), return_tensors=\"pt\")\n",
        "\n",
        "        # question and answer\n",
        "        question = self.vqas[idx][1].split('|')[0]\n",
        "        label = self.labels.index(str(self.vqas[idx][1].split('|')[1]))\n",
        "\n",
        "        return os.path.join(loc[0],loc[1],loc[2], 'left_frames',loc[-1].split('_')[0]+'.png'), img, question, label\n",
        "\n",
        "train_seq = [2, 3, 4, 6, 7, 9, 10, 11, 12, 14, 15]\n",
        "val_seq = [1, 5, 16]\n",
        "\n",
        "folder_head = 'EndoVis-18-VQA/seq_'\n",
        "folder_tail = '/vqa/Sentence/*.txt'\n",
        "\n",
        "train_dataset = EndoVis18VQAGPTClassification(train_seq, folder_head, folder_tail, img_feat_extractor='RN18')\n",
        "print(len(train_dataset))\n",
        "_, _, img, question, label = train_dataset[0]\n",
        "# train_dataloader = DataLoader(dataset=train_dataset, batch_size= args.batch_size, shuffle=True, num_workers=8)\n",
        "# val_dataset = EndoVis18VQAGPTClassification(val_seq, folder_head, folder_tail, model_ver=args.model_ver)\n",
        "# val_dataloader = DataLoader(dataset=val_dataset, batch_size= args.batch_size, shuffle=False, num_workers=8)\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "45XdAwYrTDlz",
        "outputId": "001f9083-cd5b-4c94-bf3f-8f9106ad4c81"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total files: 1560 | Total question: 10574\n",
            "10574\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'EndoVis18VQAGPTClassification' object has no attribute 'transform'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-fba903959cf9>\u001b[0m in \u001b[0;36m<cell line: 76>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEndoVis18VQAGPTClassification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder_head\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder_tail\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_feat_extractor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'RN18'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;31m# train_dataloader = DataLoader(dataset=train_dataset, batch_size= args.batch_size, shuffle=True, num_workers=8)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;31m# val_dataset = EndoVis18VQAGPTClassification(val_seq, folder_head, folder_tail, model_ver=args.model_ver)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-fba903959cf9>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m# img\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mimg_loc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'left_frames'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_loc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'EndoVis18VQAGPTClassification' object has no attribute 'transform'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class EndoVis18VQAClassification(Dataset):\n",
        "    def __init__(self, seq, folder_head, folder_tail, patch_size=4):\n",
        "\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        # files, question and answers\n",
        "        filenames = []\n",
        "        for curr_seq in seq: filenames = filenames + glob.glob(folder_head + str(curr_seq) + folder_tail)\n",
        "        self.vqas = []\n",
        "        for file in filenames:\n",
        "            file_data = open(file, \"r\")\n",
        "            lines = [line.strip(\"\\n\") for line in file_data if line != \"\\n\"]\n",
        "            file_data.close()\n",
        "            for line in lines: self.vqas.append([file, line])\n",
        "        print('Total files: %d | Total question: %.d' %(len(filenames), len(self.vqas)))\n",
        "\n",
        "        # Labels\n",
        "        self.labels = ['kidney', 'Idle', 'Grasping', 'Retraction', 'Tissue_Manipulation',\n",
        "                        'Tool_Manipulation', 'Cutting', 'Cauterization', 'Suction',\n",
        "                        'Looping', 'Suturing', 'Clipping', 'Staple', 'Ultrasound_Sensing',\n",
        "                        'left-top', 'right-top', 'left-bottom', 'right-bottom']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.vqas)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        # img\n",
        "        loc = self.vqas[idx][0].split('/')\n",
        "        visual_feature_loc = os.path.join(loc[0],loc[1],loc[2], 'vqa/img_features', (str(self.patch_size)+'x'+str(self.patch_size)),loc[-1].split('_')[0]+'.hdf5')\n",
        "        frame_data = h5py.File(visual_feature_loc, 'r')\n",
        "        visual_features = torch.from_numpy(frame_data['visual_features'][:])\n",
        "\n",
        "        # question and answer\n",
        "        question = self.vqas[idx][1].split('|')[0]\n",
        "        label = self.labels.index(str(self.vqas[idx][1].split('|')[1]))\n",
        "\n",
        "        return loc[-1].split('_')[0], visual_features, question, label"
      ],
      "metadata": {
        "id": "WVbUi1wGQY5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import argparse\n",
        "import pandas as pd\n",
        "from lib2to3.pytree import convert\n",
        "\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.utils.data\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "from transformers import BertTokenizer\n",
        "from torch.utils.data  import DataLoader\n",
        "\n",
        "from utils import *\n",
        "from dataloaders.dataloaderClassification import *\n",
        "from models.VisualBertClassification import VisualBertClassification\n",
        "from models.VisualBertResMLPClassification import VisualBertResMLPClassification\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "\n",
        "'''\n",
        "Seed randoms\n",
        "'''\n",
        "def seed_everything(seed=27):\n",
        "    '''\n",
        "    Set random seed for reproducible experiments\n",
        "    Inputs: seed number\n",
        "    '''\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "def train(args, train_dataloader, model, criterion, optimizer, epoch, tokenizer, device):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    label_true = None\n",
        "    label_pred = None\n",
        "    label_score = None\n",
        "\n",
        "\n",
        "    for i, (_, visual_features, q, labels) in enumerate(train_dataloader,0):\n",
        "\n",
        "        # prepare questions\n",
        "        questions = []\n",
        "        for question in q: questions.append(question)\n",
        "        inputs = tokenizer(questions, return_tensors=\"pt\", padding=\"max_length\", max_length=args.question_len)\n",
        "\n",
        "\n",
        "        # GPU / CPU\n",
        "        visual_features = visual_features.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs, visual_features)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        scores, predicted = torch.max(F.softmax(outputs, dim=1).data, 1)\n",
        "        label_true = labels.data.cpu() if label_true == None else torch.cat((label_true, labels.data.cpu()), 0)\n",
        "        label_pred = predicted.data.cpu() if label_pred == None else torch.cat((label_pred, predicted.data.cpu()), 0)\n",
        "        label_score = scores.data.cpu() if label_score == None else torch.cat((label_score, scores.data.cpu()), 0)\n",
        "\n",
        "    # loss and acc\n",
        "    acc, c_acc = calc_acc(label_true, label_pred), calc_classwise_acc(label_true, label_pred)\n",
        "    precision, recall, fscore = calc_precision_recall_fscore(label_true, label_pred)\n",
        "    print('Train: epoch: %d loss: %.6f | Acc: %.6f | Precision: %.6f | Recall: %.6f | FScore: %.6f' %(epoch, total_loss, acc, precision, recall, fscore))\n",
        "    return acc\n",
        "\n",
        "\n",
        "def validate(args, val_loader, model, criterion, epoch, tokenizer, device, save_output = False):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    label_true = None\n",
        "    label_pred = None\n",
        "    label_score = None\n",
        "    file_names = list()\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (file_name, visual_features, q, labels) in enumerate(val_loader,0):\n",
        "            # prepare questions\n",
        "            questions = []\n",
        "            for question in q: questions.append(question)\n",
        "            inputs = tokenizer(questions, return_tensors=\"pt\", padding=\"max_length\", max_length=args.question_len)\n",
        "\n",
        "            # GPU / CPU\n",
        "            visual_features = visual_features.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs, visual_features)\n",
        "            loss = criterion(outputs,labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            scores, predicted = torch.max(F.softmax(outputs, dim=1).data, 1)\n",
        "            label_true = labels.data.cpu() if label_true == None else torch.cat((label_true, labels.data.cpu()), 0)\n",
        "            label_pred = predicted.data.cpu() if label_pred == None else torch.cat((label_pred, predicted.data.cpu()), 0)\n",
        "            label_score = scores.data.cpu() if label_score == None else torch.cat((label_score, scores.data.cpu()), 0)\n",
        "            for f in file_name: file_names.append(f)\n",
        "\n",
        "    acc = calc_acc(label_true, label_pred)\n",
        "    c_acc = 0.0\n",
        "    # c_acc = calc_classwise_acc(label_true, label_pred)\n",
        "    precision, recall, fscore = calc_precision_recall_fscore(label_true, label_pred)\n",
        "\n",
        "    print('Test: epoch: %d loss: %.6f | Acc: %.6f | Precision: %.6f | Recall: %.6f | FScore: %.6f' %(epoch, total_loss, acc, precision, recall, fscore))\n",
        "\n",
        "    if save_output:\n",
        "        '''\n",
        "            Saving predictions\n",
        "        '''\n",
        "        if os.path.exists(args.checkpoint_dir + 'text_files') == False:\n",
        "            os.mkdir(args.checkpoint_dir + 'text_files' )\n",
        "        file1 = open(args.checkpoint_dir + 'text_files/labels.txt', 'w')\n",
        "        file1.write(str(label_true))\n",
        "        file1.close()\n",
        "\n",
        "        file1 = open(args.checkpoint_dir + 'text_files/predictions.txt', 'w')\n",
        "        file1.write(str(label_pred))\n",
        "        file1.close()\n",
        "\n",
        "        if args.dataset_type == 'med_vqa':\n",
        "            if args.dataset_cat == 'cat1':\n",
        "                convert_arr = ['cta - ct angiography', 'no', 'us - ultrasound', 'xr - plain film', 'noncontrast', 'yes', 't2', 'ct w/contrast (iv)', 'mr - flair', 'mammograph', 'ct with iv contrast',\n",
        "                            'gi and iv', 't1', 'mr - t2 weighted', 'mr - t1w w/gadolinium', 'contrast', 'iv', 'an - angiogram', 'mra - mr angiography/venography', 'nm - nuclear medicine', 'mr - dwi diffusion weighted',\n",
        "                            'ct - gi & iv contrast', 'ct noncontrast', 'mr - other pulse seq.', 'ct with gi and iv contrast', 'flair', 'mr - t1w w/gd (fat suppressed)', 'ugi - upper gi', 'mr - adc map (app diff coeff)',\n",
        "                            'bas - barium swallow', 'pet - positron emission', 'mr - pdw proton density', 'mr - t1w - noncontrast', 'be - barium enema', 'us-d - doppler ultrasound', 'mr - stir', 'mr - flair w/gd',\n",
        "                            'ct with gi contrast', 'venogram', 'mr t2* gradient,gre,mpgr,swan,swi', 'mr - fiesta', 'ct - myelogram', 'gi', 'sbft - small bowel', 'pet-ct fusion']\n",
        "            elif args.dataset_cat == 'cat2':\n",
        "                convert_arr = ['axial', 'longitudinal', 'coronal', 'lateral', 'ap', 'sagittal', 'mammo - mlo', 'pa', 'mammo - cc', 'transverse', 'mammo - mag cc', 'frontal', 'oblique', '3d reconstruction', 'decubitus', 'mammo - xcc']\n",
        "            else:\n",
        "                convert_arr = ['lung, mediastinum, pleura', 'skull and contents', 'genitourinary', 'spine and contents', 'musculoskeletal', 'heart and great vessels', 'vascular and lymphatic', 'gastrointestinal', 'face, sinuses, and neck', 'breast']\n",
        "        elif args.dataset_type == 'c80':\n",
        "            convert_arr = ['no', 'calot triangle dissection', 'yes', '1', '2', 'gallbladder dissection',\n",
        "                            'clipping cutting', 'gallbladder retraction', '0', 'cleaning coagulation',\n",
        "                            'gallbladder packaging', 'preparation', '3']\n",
        "        elif args.dataset_type == 'm18':\n",
        "            convert_arr = ['kidney', 'Idle', 'Grasping', 'Retraction', 'Tissue_Manipulation',\n",
        "                            'Tool_Manipulation', 'Cutting', 'Cauterization', 'Suction',\n",
        "                            'Looping', 'Suturing', 'Clipping', 'Staple', 'Ultrasound_Sensing',\n",
        "                            'left-top', 'right-top', 'left-bottom', 'right-bottom']\n",
        "\n",
        "        df = pd.DataFrame(columns=[\"Img\", \"Ground Truth\", \"Prediction\"])\n",
        "        for i in range(len(label_true)):\n",
        "            df = df.append({'Img': file_names[i], 'Ground Truth': convert_arr[label_true[i]], 'Prediction': convert_arr[label_pred[i]]}, ignore_index=True)\n",
        "\n",
        "        df.to_csv(args.checkpoint_dir + args.checkpoint_dir.split('/')[1] + '_' + args.checkpoint_dir.split('/')[2] + '_eval.csv')\n",
        "\n",
        "    return (acc, c_acc, precision, recall, fscore)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    parser = argparse.ArgumentParser(description='VisualQuestionAnswerClassification')\n",
        "\n",
        "    # Model parameters\n",
        "    parser.add_argument('--emb_dim',        type=int,   default=300,                                help='dimension of word embeddings.')\n",
        "    parser.add_argument('--n_heads',        type=int,   default=8,                                  help='Multi-head attention.')\n",
        "    parser.add_argument('--dropout',        type=float, default=0.1,                                help='dropout')\n",
        "    parser.add_argument('--encoder_layers', type=int,   default=6,                                  help='the number of layers of encoder in Transformer.')\n",
        "\n",
        "    # Training parameters\n",
        "    parser.add_argument('--epochs',         type=int,   default=80,                                 help='number of epochs to train for (if early stopping is not triggered).') #80, 26\n",
        "    parser.add_argument('--batch_size',     type=int,   default=64,                                 help='batch_size')\n",
        "    parser.add_argument('--workers',        type=int,   default=1,                                  help='for data-loading; right now, only 1 works with h5pys.')\n",
        "    parser.add_argument('--print_freq',     type=int,   default=100,                                help='print training/validation stats every __ batches.')\n",
        "\n",
        "    # existing checkpoint\n",
        "    parser.add_argument('--checkpoint',     default=None,                                           help='path to checkpoint, None if none.')\n",
        "\n",
        "    parser.add_argument('--lr',             type=float, default=0.00001,                            help='0.000005, 0.00001, 0.000005')\n",
        "    parser.add_argument('--checkpoint_dir', default= 'checkpoints/clf_v1_2_5x5/m18f3/',    help='med_vqa_c$version$/m18/c80//m18_vid$temporal_size$/c80_vid$temporal_size$') #clf_v1_2_1x1/med_vqa_c3\n",
        "    parser.add_argument('--dataset_type',   default= 'm18',                                     help='med_vqa/m18/c80/m18_vid/c80_vid')\n",
        "    parser.add_argument('--dataset_cat',    default= 'None',                                        help='cat1/cat2/cat3')\n",
        "    parser.add_argument('--transformer_ver',default= 'vbrm',                                        help='vb/vbrm')\n",
        "    parser.add_argument('--tokenizer_ver',  default= 'v2',                                          help='v2/v3')\n",
        "    parser.add_argument('--patch_size',     default= 5,                                             help='1/2/3/4/5')\n",
        "    parser.add_argument('--temporal_size',  default= 3,                                             help='1/2/3/4/5')\n",
        "    parser.add_argument('--question_len',   default= 25,                                            help='25')\n",
        "    parser.add_argument('--num_class',      default= 2,                                             help='25')\n",
        "    parser.add_argument('--validate',       default=False,                                          help='When only validation required False/True')\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # load checkpoint, these parameters can't be modified\n",
        "    final_args = {\"emb_dim\": args.emb_dim, \"n_heads\": args.n_heads, \"dropout\": args.dropout, \"encoder_layers\": args.encoder_layers}\n",
        "\n",
        "    seed_everything()\n",
        "\n",
        "    # GPU or CPU\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # sets device for model and PyTorch tensors\n",
        "    cudnn.benchmark = True  # set to true only if inputs to model are fixed size; otherwise lot of computational overhead\n",
        "    print('device =', device)\n",
        "\n",
        "    # best model initialize\n",
        "    start_epoch = 1\n",
        "    best_epoch = [0]\n",
        "    best_results = [0.0]\n",
        "    epochs_since_improvement = 0\n",
        "\n",
        "    # dataset\n",
        "    if args.dataset_type == 'med_vqa':\n",
        "        '''\n",
        "        Train and test dataloader for MED_VQA\n",
        "        '''\n",
        "        # tokenizer\n",
        "        tokenizer = None\n",
        "        if args.tokenizer_ver == 'v2': tokenizer = BertTokenizer.from_pretrained('./dataset/bertvocab/v2/bert-medvqa/')\n",
        "        elif args.tokenizer_ver == 'v3': tokenizer = BertTokenizer.from_pretrained('./dataset/bertvocab/v3/bert-medvqa/', do_lower_case=True)\n",
        "\n",
        "        # data location\n",
        "        train_folder = 'dataset/VQA-Med/ImageClef-2019-VQA-Med-Training/'\n",
        "        val_folder = 'dataset/VQA-Med/ImageClef-2019-VQA-Med-Validation/'\n",
        "        train_img_folder = 'Train_images/'\n",
        "        val_img_folder = 'Val_images/'\n",
        "\n",
        "        # dataloader\n",
        "        train_dataset = MedVQAClassification(train_folder, train_img_folder, args.dataset_cat, patch_size = args.patch_size, validation=False)\n",
        "        train_dataloader = DataLoader(dataset=train_dataset, batch_size= args.batch_size, shuffle=True)\n",
        "        val_dataset = MedVQAClassification(val_folder, val_img_folder, args.dataset_cat, patch_size = args.patch_size, validation=True)\n",
        "        val_dataloader = DataLoader(dataset=val_dataset, batch_size= args.batch_size, shuffle=False)\n",
        "\n",
        "        # num_classes\n",
        "        if args.dataset_cat == 'cat1': args.num_class = 45\n",
        "        elif args.dataset_cat == 'cat2': args.num_class = 16\n",
        "        elif args.dataset_cat == 'cat3': args.num_class = 10\n",
        "\n",
        "    elif args.dataset_type == 'm18':\n",
        "        '''\n",
        "        Train and test dataloader for EndoVis18\n",
        "        '''\n",
        "        # tokenizer\n",
        "        tokenizer = None\n",
        "        if args.tokenizer_ver == 'v2': tokenizer = BertTokenizer.from_pretrained('./dataset/bertvocab/v2/bert-EndoVis-18-VQA/')\n",
        "        elif args.tokenizer_ver == 'v3': tokenizer = BertTokenizer.from_pretrained('./dataset/bertvocab/v3/bert-EndoVis-18-VQA/', do_lower_case=True)\n",
        "\n",
        "        # data location\n",
        "        train_seq = [2, 3, 4, 6, 7, 9, 10, 11, 12, 14, 15]\n",
        "        val_seq = [1, 5, 16]\n",
        "        # train_seq = [1, 2, 3, 5, 6, 7, 9, 10, 14, 15, 16]\n",
        "        # val_seq = [4, 11, 12]\n",
        "        folder_head = 'dataset/EndoVis-18-VQA/seq_'\n",
        "        folder_tail = '/vqa/Classification/*.txt'\n",
        "\n",
        "        # dataloader\n",
        "        train_dataset = EndoVis18VQAClassification(train_seq, folder_head, folder_tail, patch_size = args.patch_size)\n",
        "        train_dataloader = DataLoader(dataset=train_dataset, batch_size= args.batch_size, shuffle=True)\n",
        "        val_dataset = EndoVis18VQAClassification(val_seq, folder_head, folder_tail, patch_size = args.patch_size)\n",
        "        val_dataloader = DataLoader(dataset=val_dataset, batch_size= args.batch_size, shuffle=False)\n",
        "\n",
        "        # num_classes\n",
        "        args.num_class = 18\n",
        "\n",
        "    elif args.dataset_type == 'm18_vid':\n",
        "        '''\n",
        "        Train and test dataloader for EndoVis18 temporal\n",
        "        '''\n",
        "        # tokenizer\n",
        "        tokenizer = None\n",
        "        if args.tokenizer_ver == 'v2': tokenizer = BertTokenizer.from_pretrained('./dataset/bertvocab/v2/bert-EndoVis-18-VQA/')\n",
        "        elif args.tokenizer_ver == 'v3': tokenizer = BertTokenizer.from_pretrained('./dataset/bertvocab/v3/bert-EndoVis-18-VQA/', do_lower_case=True)\n",
        "\n",
        "        # data location\n",
        "        train_seq = [2, 3, 4, 6, 7, 9, 10, 11, 12, 14, 15]\n",
        "        val_seq = [1, 5, 16]\n",
        "        folder_head = 'dataset/EndoVis-18-VQA/seq_'\n",
        "        folder_tail = '/vqa/Classification/*.txt'\n",
        "\n",
        "        # dataloader\n",
        "        train_dataset = EndoVis18VidVQAClassification(train_seq, folder_head, folder_tail, patch_size = args.patch_size, temporal_size=args.temporal_size)\n",
        "        train_dataloader = DataLoader(dataset=train_dataset, batch_size= args.batch_size, shuffle=True)\n",
        "        val_dataset = EndoVis18VidVQAClassification(val_seq, folder_head, folder_tail, patch_size = args.patch_size, temporal_size=args.temporal_size)\n",
        "        val_dataloader = DataLoader(dataset=val_dataset, batch_size= args.batch_size, shuffle=False)\n",
        "\n",
        "        # num_classes\n",
        "        args.num_class = 18\n",
        "\n",
        "    elif args.dataset_type == 'c80':\n",
        "        '''\n",
        "        Train and test for cholec dataset\n",
        "        '''\n",
        "        # tokenizer\n",
        "        if args.tokenizer_ver == 'v2': tokenizer = BertTokenizer.from_pretrained('./dataset/bertvocab/v2/bert-Cholec80-VQA/')\n",
        "        elif args.tokenizer_ver == 'v3': tokenizer = BertTokenizer.from_pretrained('./dataset/bertvocab/v3/bert-Cholec80-VQA/', do_lower_case=True)\n",
        "\n",
        "        # dataloader\n",
        "        train_seq = [1, 2, 3, 4, 6, 7, 8, 9, 10, 13, 14, 15, 16, 18, 20, 21, 22, 23, 24, 25, 28, 29, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40]\n",
        "        val_seq = [5, 11, 12, 17, 19, 26, 27, 31]\n",
        "        folder_head = 'dataset/Cholec80-VQA/Classification/'\n",
        "        folder_tail = '/*.txt'\n",
        "\n",
        "        # dataloader\n",
        "        train_dataset = Cholec80VQAClassification(train_seq, folder_head, folder_tail, patch_size = args.patch_size)\n",
        "        train_dataloader = DataLoader(dataset=train_dataset, batch_size= args.batch_size, shuffle=True)\n",
        "        val_dataset = Cholec80VQAClassification(val_seq, folder_head, folder_tail, patch_size = args.patch_size)\n",
        "        val_dataloader = DataLoader(dataset=val_dataset, batch_size= args.batch_size, shuffle=False)\n",
        "\n",
        "        # num_classes\n",
        "        args.num_class = 13\n",
        "\n",
        "    elif args.dataset_type == 'c80_vid':\n",
        "        '''\n",
        "        Train and test dataloader for c80 temporal\n",
        "        '''\n",
        "        # tokenizer\n",
        "        tokenizer = None\n",
        "        if args.tokenizer_ver == 'v2': tokenizer = BertTokenizer.from_pretrained('./dataset/bertvocab/v2/bert-Cholec80-VQA/')\n",
        "        elif args.tokenizer_ver == 'v3': tokenizer = BertTokenizer.from_pretrained('./dataset/bertvocab/v3/bert-Cholec80-VQA/', do_lower_case=True)\n",
        "\n",
        "        # data location\n",
        "        train_seq = [1, 2, 3, 4, 6, 7, 8, 9, 10, 13, 14, 15, 16, 18, 20, 21, 22, 23, 24, 25, 28, 29, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40]\n",
        "        val_seq = [5, 11, 12, 17, 19, 26, 27, 31]\n",
        "        folder_head = 'dataset/Cholec80-VQA/Classification/'\n",
        "        folder_tail = '/*.txt'\n",
        "\n",
        "        # dataloader\n",
        "        train_dataset = Cholec80VQAVidClassification(train_seq, folder_head, folder_tail, patch_size = args.patch_size, temporal_size=args.temporal_size)\n",
        "        train_dataloader = DataLoader(dataset=train_dataset, batch_size= args.batch_size, shuffle=True)\n",
        "        val_dataset = Cholec80VQAVidClassification(val_seq, folder_head, folder_tail, patch_size = args.patch_size, temporal_size=args.temporal_size)\n",
        "        val_dataloader = DataLoader(dataset=val_dataset, batch_size= args.batch_size, shuffle=False)\n",
        "\n",
        "        # num_classes\n",
        "        args.num_class = 13\n",
        "\n",
        "\n",
        "    # Initialize / load checkpoint\n",
        "    if args.checkpoint is None:\n",
        "        # model\n",
        "        if args.transformer_ver == 'vb':\n",
        "            model = VisualBertClassification(vocab_size=len(tokenizer), layers=args.encoder_layers, n_heads=args.n_heads, num_class = args.num_class)\n",
        "        elif args.transformer_ver == 'vbrm':\n",
        "            model = VisualBertResMLPClassification(vocab_size=len(tokenizer), layers=args.encoder_layers, n_heads=args.n_heads, num_class = args.num_class, token_size = int(args.question_len+(args.patch_size * args.patch_size)))\n",
        "        # optimizer\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
        "\n",
        "    else:\n",
        "        checkpoint = torch.load(args.checkpoint, map_location=str(device))\n",
        "        start_epoch = checkpoint['epoch']\n",
        "        epochs_since_improvement = checkpoint['epochs_since_improvement']\n",
        "        best_Acc = checkpoint['Acc']\n",
        "        model = checkpoint['model']\n",
        "        optimizer = checkpoint['optimizer']\n",
        "        final_args = checkpoint['final_args']\n",
        "        for key in final_args.keys(): args.__setattr__(key, final_args[key])\n",
        "\n",
        "\n",
        "    # Move to GPU, if available\n",
        "    model = model.to(device)\n",
        "    print(final_args)\n",
        "    pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
        "    print('model params: ', pytorch_total_params)\n",
        "    # print(model)\n",
        "\n",
        "    # Loss function\n",
        "    criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "    # validation\n",
        "    if args.validate:\n",
        "        test_acc, test_c_acc, test_precision, test_recall, test_fscore = validate(args, val_loader=val_dataloader, model = model, criterion=criterion, epoch=(args.epochs-1), tokenizer = tokenizer, device = device)\n",
        "    else:\n",
        "        for epoch in range(start_epoch, args.epochs):\n",
        "\n",
        "            if epochs_since_improvement > 0 and epochs_since_improvement % 5 == 0:\n",
        "                adjust_learning_rate(optimizer, 0.8)\n",
        "\n",
        "            # train\n",
        "            train_acc = train(args, train_dataloader=train_dataloader, model = model, criterion=criterion, optimizer=optimizer, epoch=epoch, tokenizer = tokenizer, device = device)\n",
        "\n",
        "            # validation\n",
        "            test_acc, test_c_acc, test_precision, test_recall, test_fscore = validate(args, val_loader=val_dataloader, model = model, criterion=criterion, epoch=epoch, tokenizer = tokenizer, device = device)\n",
        "\n",
        "            if test_acc >= best_results[0]:\n",
        "                epochs_since_improvement = 0\n",
        "\n",
        "                best_results[0] = test_acc\n",
        "                best_epoch[0] = epoch\n",
        "                # print('Best epoch: %d | Best acc: %.6f' %(best_epoch[0], best_results[0]))\n",
        "                save_clf_checkpoint(args.checkpoint_dir, epoch, epochs_since_improvement, model, optimizer, best_results[0], final_args)\n",
        "\n",
        "            else:\n",
        "                epochs_since_improvement += 1\n",
        "                print(\"\\nEpochs since last improvement: %d\\n\" % (epochs_since_improvement,))\n",
        "\n",
        "            if train_acc >= 1.0: break"
      ],
      "metadata": {
        "id": "STbygXWwQVlf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}