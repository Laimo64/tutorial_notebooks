{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/tutorial_notebooks/blob/main/VisualBert_EndoVis18_VQA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0lkpVfSHTQ8",
        "outputId": "b2bc7041-bec3-4f36-c0de-e576f5cd95bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:138: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1WGdztykX3nW6pi_BKp4rO8nA7ESNRfVN\n",
            "From (redirected): https://drive.google.com/uc?id=1WGdztykX3nW6pi_BKp4rO8nA7ESNRfVN&confirm=t&uuid=e24f2a17-8619-4f98-b084-36aba1c3e431\n",
            "To: /content/EndoVis-18-VQA.zip\n",
            "100% 2.70G/2.70G [00:37<00:00, 72.7MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Downloading the VQA EndoVis18 Dataset https://drive.google.com/file/d/1WGdztykX3nW6pi_BKp4rO8nA7ESNRfVN/view?usp=sharing\n",
        "!gdown --id 1WGdztykX3nW6pi_BKp4rO8nA7ESNRfVN\n",
        "\n",
        "# Unzipping the VQA EndoVis18 Dataset\\\n",
        "!unzip -q EndoVis-18-VQA.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "import glob\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "from torch import nn\n",
        "from transformers import ViTFeatureExtractor, AutoFeatureExtractor\n",
        "\n",
        "class EndoVis18VQAGPTClassification(Dataset):\n",
        "    '''\n",
        "    \tseq: train_seq  = [2, 3, 4, 6, 7, 9, 10, 11, 12, 14, 15]\n",
        "    \t     val_seq    = [1, 5, 16]\n",
        "    \tfolder_head     = 'dataset/EndoVis-18-VQA/seq_'\n",
        "    \tfolder_tail     = '/vqa/Classification/*.txt'\n",
        "    '''\n",
        "    def __init__(self, seq, folder_head, folder_tail, transform=None):\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "                    transforms.Resize((300,256)),\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
        "                    ])\n",
        "\n",
        "\n",
        "\n",
        "        # files, question and answers\n",
        "        filenames = []\n",
        "        for curr_seq in seq: filenames = filenames + glob.glob(folder_head + str(curr_seq) + folder_tail)\n",
        "        self.vqas = []\n",
        "        for file in filenames:\n",
        "            file_data = open(file, \"r\")\n",
        "            lines = [line.strip(\"\\n\") for line in file_data if line != \"\\n\"]\n",
        "            file_data.close()\n",
        "            for line in lines: self.vqas.append([file, line])\n",
        "        print('Total files: %d | Total question: %.d' %(len(filenames), len(self.vqas)))\n",
        "\n",
        "        # Labels\n",
        "        self.labels = ['kidney', 'Idle', 'Grasping', 'Retraction', 'Tissue_Manipulation',\n",
        "                        'Tool_Manipulation', 'Cutting', 'Cauterization', 'Suction',\n",
        "                        'Looping', 'Suturing', 'Clipping', 'Staple', 'Ultrasound_Sensing',\n",
        "                        'left-top', 'right-top', 'left-bottom', 'right-bottom']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.vqas)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        loc = self.vqas[idx][0].split('/')\n",
        "\n",
        "        # img\n",
        "        img_loc = os.path.join(loc[0],loc[1],'left_frames',loc[-1].split('_')[0]+'.png')\n",
        "        if self.transform:\n",
        "            img = Image.open(img_loc)\n",
        "            img = self.transform(img)\n",
        "\n",
        "        # question and answer\n",
        "        question = self.vqas[idx][1].split('|')[0]\n",
        "        label = self.labels.index(str(self.vqas[idx][1].split('|')[1]))\n",
        "\n",
        "        return img, question, label\n",
        "\n",
        "train_seq = [2, 3, 4, 6, 7, 9, 10, 11, 12, 14, 15]\n",
        "val_seq = [1, 5, 16]\n",
        "\n",
        "folder_head = 'EndoVis-18-VQA/seq_'\n",
        "folder_tail = '/vqa/Classification/*.txt'\n",
        "\n",
        "train_dataset = EndoVis18VQAGPTClassification(train_seq, folder_head, folder_tail)\n",
        "print(len(train_dataset))\n",
        "imgs, question, label = train_dataset[0]\n",
        "# train_dataloader = DataLoader(dataset=train_dataset, batch_size= args.batch_size, shuffle=True, num_workers=8)\n",
        "# val_dataset = EndoVis18VQAGPTClassification(val_seq, folder_head, folder_tail, model_ver=args.model_ver)\n",
        "# val_dataloader = DataLoader(dataset=val_dataset, batch_size= args.batch_size, shuffle=False, num_workers=8)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45XdAwYrTDlz",
        "outputId": "819940bd-f480-4d7c-c6a1-b2a874d38dce"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total files: 1560 | Total question: 9014\n",
            "9014\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"\n",
        "    Keeps track of most recent, average, sum, and count of a metric.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "def adjust_learning_rate(optimizer, shrink_factor):\n",
        "    \"\"\"\n",
        "    Shrinks learning rate by a specified factor.\n",
        "\n",
        "    :param optimizer: optimizer whose learning rate must be shrunk.\n",
        "    :param shrink_factor: factor in interval (0, 1) to multiply learning rate with.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\nDECAYING learning rate.\")\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = param_group['lr'] * shrink_factor\n",
        "    print(\"The new learning rate is %f\\n\" % (optimizer.param_groups[0]['lr'],))\n",
        "\n",
        "\n",
        "def save_clf_checkpoint(checkpoint_dir, epoch, epochs_since_improvement, model, optimizer, Acc, final_args):\n",
        "    \"\"\"\n",
        "    Saves model checkpoint.\n",
        "    \"\"\"\n",
        "    state = {'epoch': epoch,\n",
        "             'epochs_since_improvement': epochs_since_improvement,\n",
        "             'Acc': Acc,\n",
        "             'model': model,\n",
        "             'optimizer': optimizer,\n",
        "             'final_args': final_args}\n",
        "    filename = checkpoint_dir + 'Best.pth.tar'\n",
        "    torch.save(state, filename)\n",
        "\n",
        "def calc_acc(y_true, y_pred):\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    return acc\n",
        "\n",
        "def calc_classwise_acc(y_true, y_pred):\n",
        "    matrix = confusion_matrix(y_true, y_pred)\n",
        "    classwise_acc = matrix.diagonal()/matrix.sum(axis=1)\n",
        "    return classwise_acc\n",
        "\n",
        "def calc_map(y_true, y_scores):\n",
        "    mAP = average_precision_score(y_true, y_scores,average=None)\n",
        "    return mAP\n",
        "\n",
        "def calc_precision_recall_fscore(y_true, y_pred):\n",
        "    precision, recall, fscore, _ = precision_recall_fscore_support(y_true, y_pred, average='macro', zero_division = 1)\n",
        "    return(precision, recall, fscore)\n",
        "\n",
        "\n",
        "def seed_everything(seed=27):\n",
        "    '''\n",
        "    Set random seed for reproducible experiments\n",
        "    Inputs: seed number\n",
        "    '''\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "gHVYjRD1Bi7a"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from transformers import VisualBertModel, VisualBertConfig\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "'''\n",
        "VisualBert Classification Model\n",
        "'''\n",
        "class VisualBertClassification(nn.Module):\n",
        "    '''\n",
        "    VisualBert Classification Model\n",
        "    vocab_size    = tokenizer length\n",
        "    encoder_layer = 6\n",
        "    n_heads       = 8\n",
        "    num_class     = number of class in dataset\n",
        "    '''\n",
        "    def __init__(self, vocab_size, layers, n_heads, num_class = 10):\n",
        "        super(VisualBertClassification, self).__init__()\n",
        "        VBconfig = VisualBertConfig(vocab_size= vocab_size, visual_embedding_dim = 512, num_hidden_layers = layers, num_attention_heads = n_heads, hidden_size = 2048)\n",
        "        self.VisualBertEncoder = VisualBertModel(VBconfig)\n",
        "        self.classifier = nn.Linear(VBconfig.hidden_size, num_class)\n",
        "        self.dropout = nn.Dropout(VBconfig.hidden_dropout_prob)\n",
        "        self.num_labels = num_class\n",
        "\n",
        "        ## image processing\n",
        "        self.img_feature_extractor = models.resnet18(weights=True)\n",
        "        new_fc = nn.Sequential(*list(self.img_feature_extractor.fc.children())[:-1])\n",
        "        self.img_feature_extractor.fc = new_fc\n",
        "\n",
        "    def forward(self, inputs, img):\n",
        "        # prepare visual embedding\n",
        "        visual_embeds = self.img_feature_extractor(img)\n",
        "        visual_embeds = torch.unsqueeze(visual_embeds, dim=1)\n",
        "        visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long).to(device)\n",
        "        visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float).to(device)\n",
        "\n",
        "        # append visual features to text\n",
        "        inputs.update({\n",
        "                        \"visual_embeds\": visual_embeds,\n",
        "                        \"visual_token_type_ids\": visual_token_type_ids,\n",
        "                        \"visual_attention_mask\": visual_attention_mask,\n",
        "                        \"output_attentions\": True\n",
        "                        })\n",
        "\n",
        "        inputs['input_ids'] = inputs['input_ids'].to(device)\n",
        "        inputs['token_type_ids'] = inputs['token_type_ids'].to(device)\n",
        "        inputs['attention_mask'] = inputs['attention_mask'].to(device)\n",
        "        inputs['visual_token_type_ids'] = inputs['visual_token_type_ids'].to(device)\n",
        "        inputs['visual_attention_mask'] = inputs['visual_attention_mask'].to(device)\n",
        "\n",
        "        '----------------- VQA -----------------'\n",
        "        index_to_gather = inputs['attention_mask'].sum(1) - 2  # as in original code # 6\n",
        "\n",
        "        outputs = self.VisualBertEncoder(**inputs)\n",
        "        sequence_output = outputs[0] # [1, 33, 2048]\n",
        "\n",
        "        # TO-CHECK: From the original code\n",
        "        index_to_gather = (index_to_gather.unsqueeze(-1).unsqueeze(-1).expand(index_to_gather.size(0), 1, sequence_output.size(-1))) #  [1, 1, 2048]\n",
        "\n",
        "        pooled_output = torch.gather(sequence_output, 1, index_to_gather) # [1, 33, 2048]\n",
        "\n",
        "        pooled_output = self.dropout(pooled_output) # [1, 1, 2048]\n",
        "        logits = self.classifier(pooled_output) # [1, 1, 8]\n",
        "        reshaped_logits = logits.view(-1, self.num_labels) # [1, 8]\n",
        "        return reshaped_logits\n",
        "\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.utils.data\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data  import DataLoader\n",
        "from transformers import BertTokenizer\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "def get_arg():\n",
        "    parser = argparse.ArgumentParser(description='VisualQuestionAnswerClassification')\n",
        "\n",
        "    # VB Model parameters\n",
        "    parser.add_argument('--emb_dim',        type=int,   default=300,                                help='dimension of word embeddings.')\n",
        "    parser.add_argument('--n_heads',        type=int,   default=8,                                  help='Multi-head attention.')\n",
        "    parser.add_argument('--dropout',        type=float, default=0.1,                                help='dropout')\n",
        "    parser.add_argument('--encoder_layers', type=int,   default=6,                                  help='the number of layers of encoder in Transformer.')\n",
        "\n",
        "    # Training parameters\n",
        "    parser.add_argument('--epochs',         type=int,   default=2,                                 help='number of epochs to train for (if early stopping is not triggered).') #80, 26\n",
        "    parser.add_argument('--batch_size',     type=int,   default=32,                                 help='batch_size')\n",
        "    parser.add_argument('--workers',        type=int,   default=1,                                  help='for data-loading; right now, only 1 works with h5pys.')\n",
        "    parser.add_argument('--print_freq',     type=int,   default=100,                                help='print training/validation stats every __ batches.')\n",
        "\n",
        "    # existing checkpoint\n",
        "    parser.add_argument('--checkpoint',     default=None,                                           help='path to checkpoint, None if none.')\n",
        "\n",
        "    parser.add_argument('--lr',             type=float, default=0.00001,                           help='0.000005, 0.00001, 0.000005')\n",
        "    parser.add_argument('--checkpoint_dir', default= 'checkpoints/efvlegpt2Swin/m18_v1_z_qf_',            help='med_vqa_c/m18/c80/m18_vid/c80_vid') #clf_v1_2_1x1/med_vqa_c3\n",
        "    parser.add_argument('--dataset_type',   default= 'm18',                                          help='med_vqa/m18/c80/m18_vid/c80_vid')\n",
        "    parser.add_argument('--dataset_cat',    default= 'cat1',                                        help='cat1/cat2/cat3')\n",
        "    parser.add_argument('--tokenizer_ver',  default= 'gpt2v1',                                      help='btv2/btv3/gpt2v1')\n",
        "    parser.add_argument('--question_len',   default= 25,                                            help='25')\n",
        "    parser.add_argument('--model_ver',      default= 'efvlegpt2Swin',                                          help='vb/vbrm/efvlegpt2rs18/efvlegpt2Swin/\"')  #vrvb/gpt2rs18/gpt2ViT/gpt2Swin/biogpt2rs18/vilgpt2vqa/efgpt2rs18gr/efvlegpt2Swingr\n",
        "    parser.add_argument('--model_subver',   default= 'v1',                                          help='V0,v1/v2/v3/v4')\n",
        "    parser.add_argument('--vis_pos_emb',   default= 'zeroes',                                           help='None, zeroes, pos')\n",
        "    parser.add_argument('--patch_size',     default= 5,                                             help='1/2/3/4/5')\n",
        "\n",
        "    parser.add_argument('--num_class',      default= 2,                                             help='25')\n",
        "    # parser.add_argument('--temporal_size',  default= 1,                                             help='1/2/3/4/5')\n",
        "    parser.add_argument('--validate',       default=False,                                          help='When only validation required False/True')\n",
        "\n",
        "    if 'ipykernel' in sys.modules:\n",
        "        args = parser.parse_args([])\n",
        "    else:\n",
        "        args = parser.parse_args()\n",
        "    return args\n",
        "\n",
        "def train(args, train_dataloader, model, criterion, optimizer, epoch, tokenizer, device):\n",
        "\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    label_true = None\n",
        "    label_pred = None\n",
        "    label_score = None\n",
        "\n",
        "    for i, (imgs, q, labels) in enumerate(train_dataloader,0):\n",
        "        questions = []\n",
        "        for question in q: questions.append(question)\n",
        "        inputs = tokenizer(questions, padding=\"max_length\",max_length= args.question_len, return_tensors=\"pt\")\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        outputs = model(inputs, imgs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        scores, predicted = torch.max(F.softmax(outputs, dim=1).data, 1)\n",
        "        label_true = labels.data.cpu() if label_true == None else torch.cat((label_true, labels.data.cpu()), 0)\n",
        "        label_pred = predicted.data.cpu() if label_pred == None else torch.cat((label_pred, predicted.data.cpu()), 0)\n",
        "        label_score = scores.data.cpu() if label_score == None else torch.cat((label_score, scores.data.cpu()), 0)\n",
        "\n",
        "    # loss and acc\n",
        "    acc, c_acc = calc_acc(label_true, label_pred), calc_classwise_acc(label_true, label_pred)\n",
        "    precision, recall, fscore = calc_precision_recall_fscore(label_true, label_pred)\n",
        "    print('Train: epoch: %d loss: %.6f | Acc: %.6f | Precision: %.6f | Recall: %.6f | FScore: %.6f' %(epoch, total_loss, acc, precision, recall, fscore))\n",
        "    return acc\n",
        "\n",
        "\n",
        "def validate(args, val_loader, model, criterion, epoch, tokenizer, device, save_output = False):\n",
        "\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    label_true = None\n",
        "    label_pred = None\n",
        "    label_score = None\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, ( imgs, q, labels) in enumerate(val_loader,0):\n",
        "            questions = []\n",
        "            for question in q: questions.append(question)\n",
        "            inputs = tokenizer(questions, padding=\"max_length\",max_length=args.question_len, return_tensors=\"pt\")\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "            # model forward pass\n",
        "            outputs = model(inputs, imgs)\n",
        "\n",
        "            # loss\n",
        "            loss = criterion(outputs,labels)\n",
        "            total_loss += loss.item()\n",
        "            scores, predicted = torch.max(F.softmax(outputs, dim=1).data, 1)\n",
        "            label_true = labels.data.cpu() if label_true == None else torch.cat((label_true, labels.data.cpu()), 0)\n",
        "            label_pred = predicted.data.cpu() if label_pred == None else torch.cat((label_pred, predicted.data.cpu()), 0)\n",
        "            label_score = scores.data.cpu() if label_score == None else torch.cat((label_score, scores.data.cpu()), 0)\n",
        "\n",
        "    acc = calc_acc(label_true, label_pred)\n",
        "    c_acc = 0.0\n",
        "    precision, recall, fscore = calc_precision_recall_fscore(label_true, label_pred)\n",
        "    print('Test: epoch: %d loss: %.6f | Acc: %.6f | Precision: %.6f | Recall: %.6f | FScore: %.6f' %(epoch, total_loss, acc, precision, recall, fscore))\n",
        "\n",
        "    return (acc, c_acc, precision, recall, fscore)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    args = get_arg()\n",
        "    os.makedirs('checkpoints/efvlegpt2Swin', exist_ok=True)\n",
        "    seed_everything()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    start_epoch = 1\n",
        "    best_epoch = [0]\n",
        "    best_results = [0.0]\n",
        "    epochs_since_improvement = 0\n",
        "    final_args = {\"emb_dim\": args.emb_dim, \"n_heads\": args.n_heads, \"dropout\": args.dropout, \"encoder_layers\": args.encoder_layers}\n",
        "    train_seq = [2, 3, 4, 6, 7, 9, 10, 11, 12, 14, 15]\n",
        "    val_seq = [1, 5, 16]\n",
        "    args.num_class = 18\n",
        "\n",
        "    folder_head = 'EndoVis-18-VQA/seq_'\n",
        "    folder_tail = '/vqa/Classification/*.txt'\n",
        "\n",
        "    train_dataset = EndoVis18VQAGPTClassification(train_seq, folder_head, folder_tail)\n",
        "    train_dataloader = DataLoader(dataset=train_dataset, batch_size= args.batch_size, shuffle=True, num_workers=8)\n",
        "    val_dataset = EndoVis18VQAGPTClassification(val_seq, folder_head, folder_tail)\n",
        "    val_dataloader = DataLoader(dataset=val_dataset, batch_size= args.batch_size, shuffle=False, num_workers=8)\n",
        "\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    model = VisualBertClassification(vocab_size=len(tokenizer), layers=args.encoder_layers, n_heads=args.n_heads, num_class = args.num_class)\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss().to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
        "\n",
        "    for epoch in range(start_epoch, args.epochs):\n",
        "\n",
        "            if epochs_since_improvement > 0 and epochs_since_improvement % 5 == 0:\n",
        "                adjust_learning_rate(optimizer, 0.8)\n",
        "\n",
        "            # train\n",
        "            train_acc = train(args, train_dataloader=train_dataloader, model = model, criterion=criterion, optimizer=optimizer, epoch=epoch, tokenizer = tokenizer, device = device)\n",
        "\n",
        "            # validation\n",
        "            test_acc, test_c_acc, test_precision, test_recall, test_fscore = validate(args, val_loader=val_dataloader, model = model, criterion=criterion, epoch=epoch, tokenizer = tokenizer, device = device)\n",
        "\n",
        "            if test_acc >= best_results[0]:\n",
        "                print('Best Epoch:', epoch)\n",
        "                epochs_since_improvement = 0\n",
        "                best_results[0] = test_acc\n",
        "                best_epoch[0] = epoch\n",
        "                save_clf_checkpoint(args.checkpoint_dir, epoch, epochs_since_improvement, model, optimizer, best_results[0], final_args)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "s1TQaWXz4D0H",
        "outputId": "a2b52187-bc60-4a4b-85af-ad0ef0adf77b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total files: 1560 | Total question: 9014\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total files: 447 | Total question: 2769\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IHGD3kJH_UlJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}