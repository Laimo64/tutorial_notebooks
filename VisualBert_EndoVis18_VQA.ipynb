{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/tutorial_notebooks/blob/main/VisualBert_EndoVis18_VQA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download Dataset"
      ],
      "metadata": {
        "id": "wevyY60BkqzM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0lkpVfSHTQ8",
        "outputId": "88031049-3485-4999-c73c-bfaa1a06c58e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:138: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1WGdztykX3nW6pi_BKp4rO8nA7ESNRfVN\n",
            "From (redirected): https://drive.google.com/uc?id=1WGdztykX3nW6pi_BKp4rO8nA7ESNRfVN&confirm=t&uuid=ac0f7007-28df-417c-bccd-7a9408408f28\n",
            "To: /content/EndoVis-18-VQA.zip\n",
            "100% 2.70G/2.70G [00:47<00:00, 56.3MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Downloading the VQA EndoVis18 Dataset https://drive.google.com/file/d/1WGdztykX3nW6pi_BKp4rO8nA7ESNRfVN/view?usp=sharing\n",
        "!gdown --id 1WGdztykX3nW6pi_BKp4rO8nA7ESNRfVN\n",
        "\n",
        "# Unzipping the VQA EndoVis18 Dataset\\\n",
        "!unzip -q EndoVis-18-VQA.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#VisualBERT VQA Classification"
      ],
      "metadata": {
        "id": "A1NtTmt0xpqV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Different Versions of VisualBert Adapatation:"
      ],
      "metadata": {
        "id": "uyhruA3SozDM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import VisualBertModel, VisualBertConfig\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "from torch import nn\n",
        "\n",
        "#V1: Naive version without any pretrained weights\n",
        "class VisualBertClassification_v1(nn.Module):\n",
        "    '''\n",
        "    VisualBert Classification Model\n",
        "    vocab_size    = tokenizer length\n",
        "    encoder_layer = 6\n",
        "    n_heads       = 8\n",
        "    num_class     = number of class in dataset\n",
        "    '''\n",
        "    def __init__(self, vocab_size, layers, n_heads, num_class = 10):\n",
        "        super(VisualBertClassification_v1, self).__init__()\n",
        "        VBconfig = VisualBertConfig(vocab_size= vocab_size, visual_embedding_dim = 512, num_hidden_layers = layers, num_attention_heads = n_heads, hidden_size = 2048)\n",
        "        self.VisualBertEncoder = VisualBertModel(VBconfig)\n",
        "        self.classifier = nn.Linear(VBconfig.hidden_size, num_class)\n",
        "        self.dropout = nn.Dropout(VBconfig.hidden_dropout_prob)\n",
        "        self.num_labels = num_class\n",
        "\n",
        "        ## image processing\n",
        "        self.img_feature_extractor = models.resnet18(weights=True)\n",
        "        new_fc = nn.Sequential(*list(self.img_feature_extractor.fc.children())[:-1])\n",
        "        self.img_feature_extractor.fc = new_fc\n",
        "\n",
        "    def forward(self, inputs, img):\n",
        "        # prepare visual embedding\n",
        "        visual_embeds = self.img_feature_extractor(img)\n",
        "        visual_embeds = torch.unsqueeze(visual_embeds, dim=1)\n",
        "        visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long).to(device)\n",
        "        visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float).to(device)\n",
        "\n",
        "        # append visual features to text\n",
        "        inputs.update({\n",
        "                        \"visual_embeds\": visual_embeds,\n",
        "                        \"visual_token_type_ids\": visual_token_type_ids,\n",
        "                        \"visual_attention_mask\": visual_attention_mask,\n",
        "                        \"output_attentions\": True\n",
        "                        })\n",
        "\n",
        "        inputs['input_ids'] = inputs['input_ids'].to(device)\n",
        "        inputs['token_type_ids'] = inputs['token_type_ids'].to(device)\n",
        "        inputs['attention_mask'] = inputs['attention_mask'].to(device)\n",
        "        inputs['visual_token_type_ids'] = inputs['visual_token_type_ids'].to(device)\n",
        "        inputs['visual_attention_mask'] = inputs['visual_attention_mask'].to(device)\n",
        "\n",
        "        '----------------- VQA -----------------'\n",
        "        index_to_gather = inputs['attention_mask'].sum(1) - 2  # as in original code # 6\n",
        "\n",
        "        outputs = self.VisualBertEncoder(**inputs)\n",
        "        sequence_output = outputs[0] # [1, 33, 2048]\n",
        "\n",
        "        # TO-CHECK: From the original code\n",
        "        index_to_gather = (index_to_gather.unsqueeze(-1).unsqueeze(-1).expand(index_to_gather.size(0), 1, sequence_output.size(-1))) #  [1, 1, 2048]\n",
        "\n",
        "        pooled_output = torch.gather(sequence_output, 1, index_to_gather) # [1, 33, 2048]\n",
        "\n",
        "        pooled_output = self.dropout(pooled_output) # [1, 1, 2048]\n",
        "        logits = self.classifier(pooled_output) # [1, 1, 8]\n",
        "        reshaped_logits = logits.view(-1, self.num_labels) # [1, 8]\n",
        "        return reshaped_logits\n",
        "\n",
        "\n",
        "#V2: With pretrained weights\n",
        "class VisualBertClassification_V2(nn.Module):\n",
        "    '''\n",
        "    VisualBert Classification Model\n",
        "    vocab_size    = tokenizer length\n",
        "    encoder_layer = 6\n",
        "    n_heads       = 8\n",
        "    num_class     = number of class in dataset\n",
        "    '''\n",
        "    def __init__(self, vocab_size, layers, n_heads, num_class = 10):\n",
        "        super(VisualBertClassification_V2, self).__init__()\n",
        "        self.num_labels = num_class\n",
        "        self.VisualBertEncoder = VisualBertModel.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n",
        "        #Projection is not required for ResNet50 feature extractor as the output is equal to 2048 as VisualBERT visual embedding\n",
        "        self.visual_projection = nn.Linear(512, self.VisualBertEncoder.embeddings.visual_projection.in_features)\n",
        "        self.classifier = nn.Linear(768, self.num_labels)\n",
        "\n",
        "        ## image processing\n",
        "        self.img_feature_extractor = models.resnet18(weights=True)\n",
        "        self.img_feature_extractor.fc = nn.Sequential(*list(self.img_feature_extractor.fc.children())[:-1])\n",
        "\n",
        "    def forward(self, inputs, img):\n",
        "        # prepare visual embedding\n",
        "        visual_embeds = self.img_feature_extractor(img)\n",
        "        visual_embeds = self.visual_projection(visual_embeds)\n",
        "        visual_embeds = torch.unsqueeze(visual_embeds, dim=1)\n",
        "        visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long).to(device)\n",
        "        visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float).to(device)\n",
        "\n",
        "        # append visual features to text\n",
        "        inputs.update({\n",
        "                        \"visual_embeds\": visual_embeds,\n",
        "                        \"visual_token_type_ids\": visual_token_type_ids,\n",
        "                        \"visual_attention_mask\": visual_attention_mask,\n",
        "                        \"output_attentions\": True\n",
        "                        })\n",
        "\n",
        "        inputs['input_ids'] = inputs['input_ids'].to(device)\n",
        "        inputs['token_type_ids'] = inputs['token_type_ids'].to(device)\n",
        "        inputs['attention_mask'] = inputs['attention_mask'].to(device)\n",
        "        inputs['visual_token_type_ids'] = inputs['visual_token_type_ids'].to(device)\n",
        "        inputs['visual_attention_mask'] = inputs['visual_attention_mask'].to(device)\n",
        "\n",
        "        '----------------- VQA -----------------'\n",
        "        index_to_gather = inputs['attention_mask'].sum(1) - 2  # as in original code # 6\n",
        "        outputs = self.VisualBertEncoder(**inputs)\n",
        "        sequence_output = outputs[0] # [1, 33, 2048]\n",
        "\n",
        "        # TO-CHECK: From the original code\n",
        "        index_to_gather = (index_to_gather.unsqueeze(-1).unsqueeze(-1).expand(index_to_gather.size(0), 1, sequence_output.size(-1))) #  [1, 1, 2048]\n",
        "\n",
        "        pooled_output = torch.gather(sequence_output, 1, index_to_gather) # [1, 33, 2048]\n",
        "\n",
        "        # pooled_output = self.dropout(pooled_output) # [1, 1, 2048]\n",
        "        logits = self.classifier(pooled_output) # [1, 1, 8]\n",
        "        # reshaped_logits = logits.view(-1, self.num_labels) # [1, 8]\n",
        "        return logits\n",
        "\n",
        "\n",
        "#V3: with VQA pretrained weights directly\n",
        "from transformers import AutoTokenizer, VisualBertForQuestionAnswering\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
        "# model = VisualBertForQuestionAnswering.from_pretrained(\"uclanlp/visualbert-vqa\")\n",
        "class VisualBertClassification_V3(nn.Module):\n",
        "    '''\n",
        "    VisualBert Classification Model\n",
        "    vocab_size    = tokenizer length\n",
        "    encoder_layer = 6\n",
        "    n_heads       = 8\n",
        "    num_class     = number of class in dataset\n",
        "    '''\n",
        "    def __init__(self, vocab_size, layers, n_heads, num_class = 10):\n",
        "        super(VisualBertClassification_V3, self).__init__()\n",
        "        self.num_labels = num_class\n",
        "        self.VisualBertEncoder = VisualBertForQuestionAnswering.from_pretrained(\"uclanlp/visualbert-vqa\")\n",
        "        self.VisualBertEncoder.num_labels = self.num_labels\n",
        "        self.VisualBertEncoder.cls = nn.Linear(self.VisualBertEncoder.cls.in_features, self.num_labels)\n",
        "        self.visual_projection = nn.Linear(512, self.VisualBertEncoder.visual_bert.embeddings.visual_projection.in_features)\n",
        "\n",
        "        ## image processing\n",
        "        self.img_feature_extractor = models.resnet18(weights=True)\n",
        "        self.img_feature_extractor.fc = nn.Sequential(*list(self.img_feature_extractor.fc.children())[:-1])\n",
        "\n",
        "    def forward(self, inputs, img):\n",
        "        # prepare visual embedding\n",
        "        visual_embeds = self.img_feature_extractor(img)\n",
        "        visual_embeds = self.visual_projection(visual_embeds)\n",
        "        visual_embeds = torch.unsqueeze(visual_embeds, dim=1)\n",
        "        visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long).to(device)\n",
        "        visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float).to(device)\n",
        "\n",
        "        # append visual features to text\n",
        "        inputs.update({\n",
        "                        \"visual_embeds\": visual_embeds,\n",
        "                        \"visual_token_type_ids\": visual_token_type_ids,\n",
        "                        \"visual_attention_mask\": visual_attention_mask,\n",
        "                        \"output_attentions\": True\n",
        "                        })\n",
        "\n",
        "        inputs['input_ids'] = inputs['input_ids'].to(device)\n",
        "        inputs['token_type_ids'] = inputs['token_type_ids'].to(device)\n",
        "        inputs['attention_mask'] = inputs['attention_mask'].to(device)\n",
        "        inputs['visual_token_type_ids'] = inputs['visual_token_type_ids'].to(device)\n",
        "        inputs['visual_attention_mask'] = inputs['visual_attention_mask'].to(device)\n",
        "\n",
        "        '----------------- VQA -----------------'\n",
        "        index_to_gather = inputs['attention_mask'].sum(1) - 2  # as in original code # 6\n",
        "        logits = self.VisualBertEncoder(**inputs).logits\n",
        "        reshaped_logits = logits.view(-1, self.num_labels) # [1, 8]\n",
        "        return reshaped_logits"
      ],
      "metadata": {
        "id": "Zv8MXL9mo4MQ"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utils"
      ],
      "metadata": {
        "id": "alIq7g24kpkp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"\n",
        "    Keeps track of most recent, average, sum, and count of a metric.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "def adjust_learning_rate(optimizer, shrink_factor):\n",
        "    \"\"\"\n",
        "    Shrinks learning rate by a specified factor.\n",
        "\n",
        "    :param optimizer: optimizer whose learning rate must be shrunk.\n",
        "    :param shrink_factor: factor in interval (0, 1) to multiply learning rate with.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\nDECAYING learning rate.\")\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = param_group['lr'] * shrink_factor\n",
        "    print(\"The new learning rate is %f\\n\" % (optimizer.param_groups[0]['lr'],))\n",
        "\n",
        "\n",
        "def save_clf_checkpoint(checkpoint_dir, epoch, epochs_since_improvement, model, optimizer, Acc, final_args):\n",
        "    \"\"\"\n",
        "    Saves model checkpoint.\n",
        "    \"\"\"\n",
        "    state = {'epoch': epoch,\n",
        "             'epochs_since_improvement': epochs_since_improvement,\n",
        "             'Acc': Acc,\n",
        "             'model': model,\n",
        "             'optimizer': optimizer,\n",
        "             'final_args': final_args}\n",
        "    filename = checkpoint_dir + 'Best.pth.tar'\n",
        "    torch.save(state, filename)\n",
        "\n",
        "def calc_acc(y_true, y_pred):\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    return acc\n",
        "\n",
        "def calc_classwise_acc(y_true, y_pred):\n",
        "    matrix = confusion_matrix(y_true, y_pred)\n",
        "    classwise_acc = matrix.diagonal()/matrix.sum(axis=1)\n",
        "    return classwise_acc\n",
        "\n",
        "def calc_map(y_true, y_scores):\n",
        "    mAP = average_precision_score(y_true, y_scores,average=None)\n",
        "    return mAP\n",
        "\n",
        "def calc_precision_recall_fscore(y_true, y_pred):\n",
        "    precision, recall, fscore, _ = precision_recall_fscore_support(y_true, y_pred, average='macro', zero_division = 1)\n",
        "    return(precision, recall, fscore)\n",
        "\n",
        "\n",
        "def seed_everything(seed=27):\n",
        "    '''\n",
        "    Set random seed for reproducible experiments\n",
        "    Inputs: seed number\n",
        "    '''\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "gHVYjRD1Bi7a"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Model"
      ],
      "metadata": {
        "id": "DUy9LwOxkuhX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#############Dataloader###############\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "import glob\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class EndoVis18VQAGPTClassification(Dataset):\n",
        "    '''\n",
        "    \tseq: train_seq  = [2, 3, 4, 6, 7, 9, 10, 11, 12, 14, 15]\n",
        "    \t     val_seq    = [1, 5, 16]\n",
        "    \tfolder_head     = 'dataset/EndoVis-18-VQA/seq_'\n",
        "    \tfolder_tail     = '/vqa/Classification/*.txt'\n",
        "    '''\n",
        "    def __init__(self, seq, folder_head, folder_tail, transform=None):\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "                    transforms.Resize((224,224)),\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
        "                    ])\n",
        "\n",
        "\n",
        "        # files, question and answers\n",
        "        filenames = []\n",
        "        for curr_seq in seq: filenames = filenames + glob.glob(folder_head + str(curr_seq) + folder_tail)\n",
        "        self.vqas = []\n",
        "        for file in filenames:\n",
        "            file_data = open(file, \"r\")\n",
        "            lines = [line.strip(\"\\n\") for line in file_data if line != \"\\n\"]\n",
        "            file_data.close()\n",
        "            for line in lines: self.vqas.append([file, line])\n",
        "        print('Total files: %d | Total question: %.d' %(len(filenames), len(self.vqas)))\n",
        "\n",
        "        # Labels\n",
        "        self.labels = ['kidney', 'Idle', 'Grasping', 'Retraction', 'Tissue_Manipulation',\n",
        "                        'Tool_Manipulation', 'Cutting', 'Cauterization', 'Suction',\n",
        "                        'Looping', 'Suturing', 'Clipping', 'Staple', 'Ultrasound_Sensing',\n",
        "                        'left-top', 'right-top', 'left-bottom', 'right-bottom']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.vqas)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        loc = self.vqas[idx][0].split('/')\n",
        "\n",
        "        # img\n",
        "        img_loc = os.path.join(loc[0],loc[1],'left_frames',loc[-1].split('_')[0]+'.png')\n",
        "        if self.transform:\n",
        "            img = Image.open(img_loc)\n",
        "            img = self.transform(img)\n",
        "\n",
        "        # question and answer\n",
        "        question = self.vqas[idx][1].split('|')[0]\n",
        "        label = self.labels.index(str(self.vqas[idx][1].split('|')[1]))\n",
        "\n",
        "        return img, question, label\n",
        "\n",
        "\n",
        "#############Model Architecture###############\n",
        "import torch\n",
        "from torch import nn\n",
        "from transformers import VisualBertModel, VisualBertConfig\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "class VisualBertClassification(nn.Module):\n",
        "    '''\n",
        "    VisualBert Classification Model\n",
        "    vocab_size    = tokenizer length\n",
        "    encoder_layer = 6\n",
        "    n_heads       = 8\n",
        "    num_class     = number of class in dataset\n",
        "    '''\n",
        "    def __init__(self, vocab_size, layers, n_heads, num_class = 10):\n",
        "        super(VisualBertClassification, self).__init__()\n",
        "        self.num_labels = num_class\n",
        "        self.VisualBertEncoder = VisualBertModel.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n",
        "        self.visual_projection = nn.Linear(512, self.VisualBertEncoder.embeddings.visual_projection.in_features)\n",
        "        self.classifier = nn.Linear(768, self.num_labels)\n",
        "\n",
        "        ## image processing\n",
        "        self.img_feature_extractor = models.resnet18(weights=True)\n",
        "        self.img_feature_extractor.fc = nn.Sequential(*list(self.img_feature_extractor.fc.children())[:-1])\n",
        "\n",
        "    def forward(self, inputs, img):\n",
        "        # prepare visual embedding\n",
        "        visual_embeds = self.img_feature_extractor(img)\n",
        "        visual_embeds = self.visual_projection(visual_embeds)\n",
        "        visual_embeds = torch.unsqueeze(visual_embeds, dim=1)\n",
        "        visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long).to(device)\n",
        "        visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float).to(device)\n",
        "\n",
        "        # append visual features to text\n",
        "        inputs.update({\n",
        "                        \"visual_embeds\": visual_embeds,\n",
        "                        \"visual_token_type_ids\": visual_token_type_ids,\n",
        "                        \"visual_attention_mask\": visual_attention_mask,\n",
        "                        \"output_attentions\": True\n",
        "                        })\n",
        "\n",
        "        inputs['input_ids'] = inputs['input_ids'].to(device)\n",
        "        inputs['token_type_ids'] = inputs['token_type_ids'].to(device)\n",
        "        inputs['attention_mask'] = inputs['attention_mask'].to(device)\n",
        "        inputs['visual_token_type_ids'] = inputs['visual_token_type_ids'].to(device)\n",
        "        inputs['visual_attention_mask'] = inputs['visual_attention_mask'].to(device)\n",
        "\n",
        "        '----------------- VQA -----------------'\n",
        "        index_to_gather = inputs['attention_mask'].sum(1) - 2  # as in original code # 6\n",
        "        outputs = self.VisualBertEncoder(**inputs)\n",
        "        sequence_output = outputs[0] # [1, 33, 2048]\n",
        "\n",
        "        # TO-CHECK: From the original code\n",
        "        index_to_gather = (index_to_gather.unsqueeze(-1).unsqueeze(-1).expand(index_to_gather.size(0), 1, sequence_output.size(-1))) #  [1, 1, 2048]\n",
        "\n",
        "        pooled_output = torch.gather(sequence_output, 1, index_to_gather) # [1, 33, 2048]\n",
        "\n",
        "        # pooled_output = self.dropout(pooled_output) # [1, 1, 2048]\n",
        "        logits = self.classifier(pooled_output) # [1, 1, 8]\n",
        "        reshaped_logits = logits.view(-1, self.num_labels) # [1, 8]\n",
        "        return reshaped_logits\n",
        "\n",
        "\n",
        "#############Training Script###############\n",
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.utils.data\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data  import DataLoader\n",
        "from transformers import BertTokenizer\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "def get_arg():\n",
        "    parser = argparse.ArgumentParser(description='VisualQuestionAnswerClassification')\n",
        "\n",
        "    # VB Model parameters\n",
        "    parser.add_argument('--n_heads',        type=int,   default=8,                                  help='Multi-head attention.')\n",
        "    parser.add_argument('--encoder_layers', type=int,   default=6,                                  help='the number of layers of encoder in Transformer.')\n",
        "\n",
        "    # Training parameters\n",
        "    parser.add_argument('--epochs',         type=int,   default=2,                                 help='number of epochs to train for (if early stopping is not triggered).') #80, 26\n",
        "    parser.add_argument('--batch_size',     type=int,   default=64,                                 help='batch_size')\n",
        "    parser.add_argument('--workers',        type=int,   default=1,                                  help='for data-loading; right now, only 1 works with h5pys.')\n",
        "    parser.add_argument('--lr',             type=float, default=0.00001,                           help='0.000005, 0.00001, 0.000005')\n",
        "    parser.add_argument('--checkpoint_dir', default= 'checkpoints/VB_RN18',            help='med_vqa_c/m18/c80/m18_vid/c80_vid') #clf_v1_2_1x1/med_vqa_c3\n",
        "    parser.add_argument('--question_len',   default= 25,                                            help='25')\n",
        "    parser.add_argument('--num_class',      default= 2,                                             help='25')\n",
        "    parser.add_argument('--validate',       default=False,                                          help='When only validation required False/True')\n",
        "\n",
        "    if 'ipykernel' in sys.modules:\n",
        "        args = parser.parse_args([])\n",
        "    else:\n",
        "        args = parser.parse_args()\n",
        "    return args\n",
        "\n",
        "def train(args, train_dataloader, model, criterion, optimizer, epoch, tokenizer, device):\n",
        "\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    label_true = None\n",
        "    label_pred = None\n",
        "    label_score = None\n",
        "\n",
        "    for i, (imgs, q, labels) in enumerate(train_dataloader,0):\n",
        "        questions = []\n",
        "        for question in q: questions.append(question)\n",
        "        inputs = tokenizer(questions, padding=\"max_length\",max_length= args.question_len, return_tensors=\"pt\")\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        outputs = model(inputs, imgs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        scores, predicted = torch.max(F.softmax(outputs, dim=1).data, 1)\n",
        "        label_true = labels.data.cpu() if label_true == None else torch.cat((label_true, labels.data.cpu()), 0)\n",
        "        label_pred = predicted.data.cpu() if label_pred == None else torch.cat((label_pred, predicted.data.cpu()), 0)\n",
        "        label_score = scores.data.cpu() if label_score == None else torch.cat((label_score, scores.data.cpu()), 0)\n",
        "\n",
        "    # loss and acc\n",
        "    acc, c_acc = calc_acc(label_true, label_pred), calc_classwise_acc(label_true, label_pred)\n",
        "    precision, recall, fscore = calc_precision_recall_fscore(label_true, label_pred)\n",
        "    print('Train: epoch: %d loss: %.6f | Acc: %.6f | Precision: %.6f | Recall: %.6f | FScore: %.6f' %(epoch, total_loss, acc, precision, recall, fscore))\n",
        "    return acc\n",
        "\n",
        "\n",
        "def validate(args, val_loader, model, criterion, epoch, tokenizer, device, save_output = False):\n",
        "\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    label_true = None\n",
        "    label_pred = None\n",
        "    label_score = None\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, ( imgs, q, labels) in enumerate(val_loader,0):\n",
        "            questions = []\n",
        "            for question in q: questions.append(question)\n",
        "            inputs = tokenizer(questions, padding=\"max_length\",max_length=args.question_len, return_tensors=\"pt\")\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "            # model forward pass\n",
        "            outputs = model(inputs, imgs)\n",
        "\n",
        "            # loss\n",
        "            loss = criterion(outputs,labels)\n",
        "            total_loss += loss.item()\n",
        "            scores, predicted = torch.max(F.softmax(outputs, dim=1).data, 1)\n",
        "            label_true = labels.data.cpu() if label_true == None else torch.cat((label_true, labels.data.cpu()), 0)\n",
        "            label_pred = predicted.data.cpu() if label_pred == None else torch.cat((label_pred, predicted.data.cpu()), 0)\n",
        "            label_score = scores.data.cpu() if label_score == None else torch.cat((label_score, scores.data.cpu()), 0)\n",
        "\n",
        "    acc = calc_acc(label_true, label_pred)\n",
        "    c_acc = 0.0\n",
        "    precision, recall, fscore = calc_precision_recall_fscore(label_true, label_pred)\n",
        "    print('Test: epoch: %d loss: %.6f | Acc: %.6f | Precision: %.6f | Recall: %.6f | FScore: %.6f' %(epoch, total_loss, acc, precision, recall, fscore))\n",
        "\n",
        "    return (acc, c_acc, precision, recall, fscore)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    args = get_arg()\n",
        "    args.checkpoint_dir = 'checkpoints/VB_RN18'\n",
        "    os.makedirs(args.checkpoint_dir, exist_ok=True)\n",
        "    seed_everything()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    start_epoch = 1\n",
        "    best_epoch = [0]\n",
        "    best_results = [0.0]\n",
        "    epochs_since_improvement = 0\n",
        "    final_args = { \"n_heads\": args.n_heads, \"encoder_layers\": args.encoder_layers}\n",
        "    train_seq = [2, 3, 4, 6, 7, 9, 10, 11, 12, 14, 15]\n",
        "    val_seq = [1, 5, 16]\n",
        "    args.num_class = 18\n",
        "\n",
        "    folder_head = 'EndoVis-18-VQA/seq_'\n",
        "    folder_tail = '/vqa/Classification/*.txt'\n",
        "\n",
        "    train_dataset = EndoVis18VQAGPTClassification(train_seq, folder_head, folder_tail)\n",
        "    train_dataloader = DataLoader(dataset=train_dataset, batch_size= args.batch_size, shuffle=True, num_workers=2)\n",
        "    val_dataset = EndoVis18VQAGPTClassification(val_seq, folder_head, folder_tail)\n",
        "    val_dataloader = DataLoader(dataset=val_dataset, batch_size= args.batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    # model = VisualBertClassification(vocab_size=len(tokenizer), layers=args.encoder_layers, n_heads=args.n_heads, num_class = args.num_class)\n",
        "    model = VisualBertClassification_V3(vocab_size=len(tokenizer), layers=args.encoder_layers, n_heads=args.n_heads, num_class = args.num_class)\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss().to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
        "\n",
        "    for epoch in range(start_epoch, args.epochs):\n",
        "\n",
        "            if epochs_since_improvement > 0 and epochs_since_improvement % 5 == 0:\n",
        "                adjust_learning_rate(optimizer, 0.8)\n",
        "\n",
        "            train_acc = train(args, train_dataloader=train_dataloader, model = model, criterion=criterion, optimizer=optimizer, epoch=epoch, tokenizer = tokenizer, device = device)\n",
        "            test_acc, test_c_acc, test_precision, test_recall, test_fscore = validate(args, val_loader=val_dataloader, model = model, criterion=criterion, epoch=epoch, tokenizer = tokenizer, device = device)\n",
        "\n",
        "            if test_acc >= best_results[0]:\n",
        "                print('Best Epoch:', epoch)\n",
        "                epochs_since_improvement = 0\n",
        "                best_results[0] = test_acc\n",
        "                best_epoch[0] = epoch\n",
        "                save_clf_checkpoint(args.checkpoint_dir, epoch, epochs_since_improvement, model, optimizer, best_results[0], final_args)\n",
        "            else:\n",
        "                epochs_since_improvement += 1\n",
        "                print(\"\\nEpochs since last improvement: %d\\n\" % (epochs_since_improvement,))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1TQaWXz4D0H",
        "outputId": "5098ae09-c15b-4893-b19b-5494046cfc76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total files: 1560 | Total question: 9014\n",
            "Total files: 447 | Total question: 2769\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch: 1 loss: 160.448552 | Acc: 0.589860 | Precision: 0.450971 | Recall: 0.248736 | FScore: 0.243505\n"
          ]
        }
      ]
    }
  ]
}