{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "split_cifar10(S-CIFAR10).ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOr6nBFRaE4f3KTzcsYMMoH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/tutorial_notebooks/blob/main/split_cifar10(S_CIFAR10).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "kCjH5DdFQCiI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download cifar10 dataset"
      ],
      "metadata": {
        "id": "PJU-qLEoQER0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oa7DGlLUE2K7",
        "outputId": "97556b9f-f88b-4dae-8632-96521e6bf2a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-28 16:38:54--  https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 170052171 (162M) [application/x-gzip]\n",
            "Saving to: ‘cifar-10-binary.tar.gz’\n",
            "\n",
            "cifar-10-binary.tar 100%[===================>] 162.17M  31.4MB/s    in 5.8s    \n",
            "\n",
            "2022-04-28 16:39:01 (28.0 MB/s) - ‘cifar-10-binary.tar.gz’ saved [170052171/170052171]\n",
            "\n",
            "cifar-10-batches-bin/\n",
            "cifar-10-batches-bin/data_batch_1.bin\n",
            "cifar-10-batches-bin/batches.meta.txt\n",
            "cifar-10-batches-bin/data_batch_3.bin\n",
            "cifar-10-batches-bin/data_batch_4.bin\n",
            "cifar-10-batches-bin/test_batch.bin\n",
            "cifar-10-batches-bin/readme.html\n",
            "cifar-10-batches-bin/data_batch_5.bin\n",
            "cifar-10-batches-bin/data_batch_2.bin\n"
          ]
        }
      ],
      "source": [
        "! wget https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz\n",
        "! tar xvzf cifar-10-binary.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "split class into folders"
      ],
      "metadata": {
        "id": "KpGMC44xQbMJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from skimage import io\n",
        "\n",
        "path = 'cifar-10-batches-bin'\n",
        "\n",
        "# Labels.\n",
        "label_strings = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "collection =[]\n",
        "\n",
        "# Split train dataset.\n",
        "for i in range(1, 6):\n",
        "    fpath = os.path.join(path, 'data_batch_' + str(i) + '.bin')\n",
        "    raw = np.fromfile(fpath, dtype='uint8')\n",
        "    collection.append(np.reshape(raw, (10000, 3073)))\n",
        "records = np.concatenate(collection, axis=0)\n",
        "labels = records[:, 0]\n",
        "images = np.reshape(records[:, 1:], (50000, 3, 32, 32,))\n",
        "# Gather different classes to corresponding files.\n",
        "#os.makedirs('data/train')\n",
        "#os.makedirs('data/train', mode = 0o777, exist_ok = False) \n",
        "os.makedirs('data/train', exist_ok = True)\n",
        "for i in range(10):\n",
        "    index = (labels == i)\n",
        "    class_labels = labels[index]\n",
        "    class_images = images[index]\n",
        "    class_records = np.concatenate(\n",
        "        [np.expand_dims(class_labels, axis=1), np.reshape(class_images, (5000, -1))],\n",
        "        axis=1)\n",
        "    raw_records = class_records.flatten()\n",
        "    class_file_name = 'data/train/' + label_strings[i] + '.bin'\n",
        "    raw_records.tofile(class_file_name)\n",
        "\n",
        "\n",
        "# Split test dataset.\n",
        "fpath = os.path.join(path, 'test_batch.bin')\n",
        "raw = np.fromfile(fpath, dtype='uint8')\n",
        "records = np.reshape(raw, (10000, 3073))\n",
        "labels = records[:, 0]\n",
        "images = np.reshape(records[:, 1:], (10000, 3, 32, 32,))\n",
        "# Gather different classes to corresponding files.\n",
        "os.makedirs('data/test', exist_ok = True)\n",
        "for i in range(10):\n",
        "    index = (labels == i)\n",
        "    class_labels = labels[index]\n",
        "    class_images = images[index]\n",
        "    class_records = np.concatenate(\n",
        "        [np.expand_dims(class_labels, axis=1), np.reshape(class_images, (1000, -1))],\n",
        "        axis=1)\n",
        "    raw_records = class_records.flatten()\n",
        "    class_file_name = 'data/test/' + label_strings[i] + '.bin'\n",
        "    raw_records.tofile(class_file_name)"
      ],
      "metadata": {
        "id": "ftisTKNYQLKw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training script"
      ],
      "metadata": {
        "id": "IrIYidp3FL-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import models\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "import argparse\n",
        "import copy\n",
        "import random\n",
        "import numpy as np\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "def seed_everything(seed=12):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "parser = argparse.ArgumentParser(description='BalancedLSF Training')\n",
        "parser.add_argument('--lr', default=0.1, type=float, help='learning rate')\n",
        "parser.add_argument('--lr_schedule', default=0, type=int, help='lr scheduler')\n",
        "parser.add_argument('--batch_size', default=1024, type=int, help='batch size')\n",
        "parser.add_argument('--test_batch_size', default=2048, type=int, help='batch size')\n",
        "parser.add_argument('--num_epoch', default=50, type=int, help='epoch number')\n",
        "parser.add_argument('--num_classes', type=int, default=10, help='number classes')\n",
        "args = parser.parse_args(args=[])\n",
        "\n",
        "def train(model, trainloader, criterion, optimizer):\n",
        "    model.train()\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "def test(model, testloader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "    return correct / total"
      ],
      "metadata": {
        "id": "xGunE0nuFLi_"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XjokivJ4jxST"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}