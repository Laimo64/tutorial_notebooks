{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "entropy_uncertainty",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/tutorial_notebooks/blob/main/entropy_uncertainty.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entropy in Machine Learning\n",
        "Entropy in physics is a measurement of randomness in an isolated system. It’s quite similar when it comes to machine learning! Here, entropy is also a measure of randomness. However, here, you measure the disorder of the information processed in your ML project.\n",
        "\n",
        "\n",
        "Let’s use a simple example–flipping a coin. There can be two outcomes. However, they are difficult to predict because there is no direct relation between the flipping itself and the outcome. Whatever you do, it’s 50-50. In such a situation, entropy is high–getting conclusions from the information is difficult. \n",
        "\n",
        "\n",
        "https://stats.stackexchange.com/questions/25827/how-does-one-measure-the-non-uniformity-of-a-distribution"
      ],
      "metadata": {
        "id": "4ECqLUVUkSXk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entropy with log base 2:"
      ],
      "metadata": {
        "id": "lMJR61YYjdYY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyRqsZAwkMk7",
        "outputId": "63dcf6a7-0391-406e-8888-3aa02c643379"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10,) (10,) (10,)\n",
            "std            : [5 3 6 0 4 2 7 8 1 9]\n",
            "shannon entropy: [5 3 6 2 0 4 8 7 1 9]\n",
            "kl_div         : [5 3 6 2 0 4 8 7 1 9]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import entropy\n",
        "from scipy.special import kl_div\n",
        "from scipy.special import rel_entr\n",
        "\n",
        "def get_shannon_entropy(A):\n",
        "    pA = A / A.sum()\n",
        "    se = -np.sum(pA*np.log2(pA))\n",
        "    return se\n",
        "\n",
        "def softmax(x):\n",
        "    f_x = np.exp(x) / np.sum(np.exp(x))\n",
        "    return f_x\n",
        "\n",
        "rng = np.random.default_rng(12345)\n",
        "dist = []\n",
        "std = []\n",
        "se = []\n",
        "kl = []\n",
        "for _ in range(10):\n",
        "    rints = softmax(rng.random((10, 1)))\n",
        "    dist.append(rints)\n",
        "    std.append(np.std(rints))\n",
        "    se.append(entropy(rints, base=2))\n",
        "    kl.append(np.sum(kl_div(np.squeeze(rints), np.ones(10)/10)))\n",
        "std = np.array(std)\n",
        "se = 1 - np.array(se).squeeze()\n",
        "kl = np.array(kl)\n",
        "print(std.shape, se.shape, kl.shape)\n",
        "\n",
        "print('std            :',np.argsort(std))\n",
        "print('shannon entropy:',np.argsort(se))\n",
        "print('kl_div         :',np.argsort(kl))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entropy with log base 10 (numpy vs scipy vs tensor)"
      ],
      "metadata": {
        "id": "3MOUSKJEbGOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from numpy\n",
        "from scipy.stats import entropy\n",
        "import numpy as np\n",
        "def get_se_v1(p):\n",
        "    logp = np.log(p)\n",
        "    entropy1 = np.sum(-p*logp)\n",
        "    return entropy1\n",
        "\n",
        "p = np.array([0.1, 0.2, 0.4, 0.3])\n",
        "print('numpy v1:', get_se_v1(p))\n",
        "\n",
        "#scipy\n",
        "print('scipy v2:',entropy(p))\n",
        "\n",
        "#from tensor\n",
        "import torch\n",
        "from torch.distributions import Categorical\n",
        "p_tensor = torch.Tensor(p)\n",
        "entropy2 = Categorical(p_tensor).entropy()\n",
        "print('tensor v3:', entropy2.item())"
      ],
      "metadata": {
        "id": "kRbFpYZjSUn0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8915ed4b-d165-4588-e3cd-78f4b26d0d0a"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "numpy v1: 1.2798542258336676\n",
            "scipy v2: 1.2798542258336676\n",
            "tensor v3: 1.2798542976379395\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entropy range"
      ],
      "metadata": {
        "id": "nvQOwPOcbJBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p0 = torch.tensor([0.2, 0.2, 0.2, 0.2, 0.2])\n",
        "p1 = torch.tensor([0.0, 0.0, 1, 0.0, 0.0])\n",
        "p2 = torch.tensor([0.1, 0.0, 0.9, 0.0, 0.0])\n",
        "p3 = torch.tensor([0.1, 0.2, 0.4, 0.2, 0.1])\n",
        "\n",
        "se0 = Categorical(p0).entropy()\n",
        "se1 = Categorical(p1).entropy()\n",
        "se2 = Categorical(p2).entropy()\n",
        "se3 = Categorical(p3).entropy()\n",
        "print('se0:%0.4f'%se0.item(), '\\nse1:%0.4f'%se1.item(), '\\nse2:%0.4f'%se2.item(), '\\nse3:%0.4f'% se3.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KBXYJYzWNp7",
        "outputId": "48ba3be0-5bc7-4a67-ec19-31618ffe0043"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "se0:1.6094 \n",
            "se1:0.0000 \n",
            "se2:0.3251 \n",
            "se3:1.4708\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Why am I getting information entropy greater than 1?"
      ],
      "metadata": {
        "id": "zF7MOt_hdNrc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entropy can be greater than 1 depends on class category (binary or multiclass) and log base (2 or 10).\n",
        "\n",
        "Entropy ranges from 0-1 for binary classification problems and 0 to log base 2 k, where k is the number of classes you have. Entropy measures the \"information\" or \"uncertainty\" of a random variable. When you are using base 2, it is measured in bits; and there can be more than one bit of information in a variable."
      ],
      "metadata": {
        "id": "dr-0ZyH-dSKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Binary Category: Log base 2 Vs. Log base 10:"
      ],
      "metadata": {
        "id": "wHc521W8f9jM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_se_log10(p):\n",
        "    logp = np.log(p)\n",
        "    entropy1 = np.sum(-p*logp)\n",
        "    return entropy1\n",
        "\n",
        "def get_se_log2(p):\n",
        "    logp = np.log2(p)\n",
        "    entropy1 = np.sum(-p*logp)\n",
        "    return entropy1\n",
        "\n",
        "p = np.array([0.5, 0.5])\n",
        "get_se_log10(p), get_se_log2(p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AD9IwVmpf8zB",
        "outputId": "bea8cc1e-1dc0-4f8d-aa55-b68695dbd9b6"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6931471805599453, 1.0)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiclass Category: Log base 2 Vs. Log base 10:"
      ],
      "metadata": {
        "id": "ExaWNrr_grSH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p = np.array([0.2, 0.2, 0.2, 0.2, 0.2])\n",
        "get_se_log10(p), get_se_log2(p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8l1srYSJgqlm",
        "outputId": "440789ca-52aa-47de-ccfd-faaa4df83113"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.6094379124341005, 2.321928094887362)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Therefore, log base 2 is only ideal for binary clssification only."
      ],
      "metadata": {
        "id": "g_P7O2gojNB6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entropy on multiple tensors at a time (log base 10)"
      ],
      "metadata": {
        "id": "MrW7w-L3bLv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p = torch.stack([p0, p1, p2, p3])\n",
        "se = Categorical(p).entropy()\n",
        "print('se0:%0.4f'%se[0].item(), '\\nse1:%0.4f'%se[1].item(), '\\nse2:%0.4f'%se[2].item(), '\\nse3:%0.4f'% se[3].item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBZxt1j7Wl7O",
        "outputId": "d8c3ea2e-6c93-4726-cf97-82826eb2e67b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "se0:1.6094 \n",
            "se1:0.0000 \n",
            "se2:0.3251 \n",
            "se3:1.4708\n"
          ]
        }
      ]
    }
  ]
}