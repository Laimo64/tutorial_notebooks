{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/tutorial_notebooks/blob/main/Clip_Visual_Language.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers"
      ],
      "metadata": {
        "id": "1bHfHDNkDdxs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CLIP Visual-Language Lib:"
      ],
      "metadata": {
        "id": "ISh1WDrzZAwY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdUXarFcDYbI",
        "outputId": "81aaf4d1-498f-40e2-eb59-0537448e1f61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "probability for the question of a photo of a cat or dog: tensor([[0.9949, 0.0051]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "inputs = processor(text=[\" a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "outputs = model(**inputs)\n",
        "logits_per_image = outputs.logits_per_image # this is the image-text similarity score\n",
        "probs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities\n",
        "print(\"probability for the question of a photo of a cat or dog:\",probs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CLIP Visual-Language Custom:"
      ],
      "metadata": {
        "id": "DTC2VVQ9ZGWP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "class CLIPModel_CUSTOM(nn.Module):\n",
        "    def __init__(self, ):\n",
        "        super(CLIPModel_CUSTOM, self).__init__()\n",
        "        model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "        self.config = model.config\n",
        "        self.text_model = model.text_model\n",
        "        self.vision_model = model.vision_model\n",
        "        self.visual_projection = model.visual_projection\n",
        "        self.text_projection = model.text_projection\n",
        "        self.logit_scale = model.logit_scale\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids = None,\n",
        "        pixel_values = None,\n",
        "        attention_mask = None,\n",
        "        position_ids = None,\n",
        "        return_loss = None,\n",
        "        output_attentions = None,\n",
        "        output_hidden_states = None,\n",
        "        return_dict = None,\n",
        "    ):\n",
        "\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states)\n",
        "\n",
        "        vision_outputs = self.vision_model(\n",
        "            pixel_values=pixel_values,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        text_outputs = self.text_model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            position_ids=position_ids,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        image_embeds = vision_outputs[1]\n",
        "        image_embeds = self.visual_projection(image_embeds)\n",
        "\n",
        "        text_embeds = text_outputs[1]\n",
        "        text_embeds = self.text_projection(text_embeds)\n",
        "\n",
        "        # normalized features\n",
        "        image_embeds = image_embeds / image_embeds.norm(p=2, dim=-1, keepdim=True)\n",
        "        text_embeds = text_embeds / text_embeds.norm(p=2, dim=-1, keepdim=True)\n",
        "\n",
        "        # cosine similarity as logits\n",
        "        logit_scale = self.logit_scale.exp()\n",
        "        logits_per_text = torch.matmul(text_embeds, image_embeds.t()) * logit_scale\n",
        "        logits_per_image = logits_per_text.t()\n",
        "        return logits_per_image, logits_per_text\n",
        "\n",
        "\n",
        "model = CLIPModel_CUSTOM()\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "inputs = processor(text=[\" a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True)\n",
        "logits_per_image, logits_per_text = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], pixel_values=inputs['pixel_values'])\n",
        "probs_per_image = logits_per_image.softmax(dim=1)\n",
        "print(\"logits_per_image:\",logits_per_image, \"logits_per_text:\", logits_per_text)\n",
        "\n",
        "print(\"probability for the question of a photo of a cat or dog:\",probs_per_image)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VTViNF-lvBY",
        "outputId": "48164665-a7f9-41d0-c140-a3e127a183b9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logits_per_image: tensor([[24.5701, 19.3049]], grad_fn=<TBackward0>) logits_per_text: tensor([[24.5701],\n",
            "        [19.3049]], grad_fn=<MulBackward0>)\n",
            "probability for the question of a photo of a cat or dog: tensor([[0.9949, 0.0051]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CLIP Language-Language Custom Scaler Probability:"
      ],
      "metadata": {
        "id": "t0H4jPtZZJpo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "class CLIPModel_Text(nn.Module):\n",
        "    def __init__(self, ):\n",
        "        super(CLIPModel_Text, self).__init__()\n",
        "        model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "        self.config = model.config\n",
        "        self.text_model = model.text_model\n",
        "        self.text_projection = model.text_projection\n",
        "        self.logit_scale = model.logit_scale\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids = None,\n",
        "        attention_mask = None,\n",
        "        position_ids = None,\n",
        "        return_loss = None,\n",
        "        output_attentions = None,\n",
        "        output_hidden_states = None,\n",
        "        return_dict = None,\n",
        "    ):\n",
        "\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states)\n",
        "\n",
        "        text_outputs = self.text_model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            position_ids=position_ids,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        text_embeds = text_outputs[1]\n",
        "        text_embeds = self.text_projection(text_embeds)\n",
        "\n",
        "        # normalized features\n",
        "        text_embeds = text_embeds / text_embeds.norm(p=2, dim=-1, keepdim=True)\n",
        "\n",
        "        # cosine similarity as logits\n",
        "        prob_per_pair = torch.matmul(text_embeds[0], text_embeds[1]) #* logit_scale\n",
        "\n",
        "        return prob_per_pair\n",
        "\n",
        "\n",
        "model = CLIPModel_Text()\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "inputs = processor(text=[\" a photo of a cat\", \"a photo of a dog\"], return_tensors=\"pt\", padding=True)\n",
        "prob_per_pair = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
        "print(\"prob per pair:\",prob_per_pair)\n",
        "\n",
        "inputs = processor(text=[\"hello, how are you?\", \"a photo of a dog\"], return_tensors=\"pt\", padding=True)\n",
        "prob_per_pair = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
        "print(\"prob per pair:\",prob_per_pair)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eukh56ndsMjO",
        "outputId": "54fdc46b-abbe-4a66-8d42-feff889547d5"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prob per pair: tensor(0.9310, grad_fn=<DotBackward0>)\n",
            "prob per pair: tensor(0.8267, grad_fn=<DotBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CLIP Language-Language Custom Vector Probability or Feature Map Single Pair:"
      ],
      "metadata": {
        "id": "ZSxk-ZmLZrA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "class CLIPModel_Text(nn.Module):\n",
        "    def __init__(self, ):\n",
        "        super(CLIPModel_Text, self).__init__()\n",
        "        model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "        self.config = model.config\n",
        "        self.text_model = model.text_model\n",
        "        self.text_projection = model.text_projection\n",
        "        self.logit_scale = model.logit_scale\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids = None,\n",
        "        attention_mask = None,\n",
        "        position_ids = None,\n",
        "        return_loss = None,\n",
        "        output_attentions = None,\n",
        "        output_hidden_states = None,\n",
        "        return_dict = None,\n",
        "    ):\n",
        "\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states)\n",
        "\n",
        "        text_outputs = self.text_model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            position_ids=position_ids,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        text_embeds = text_outputs[1]\n",
        "        text_embeds = self.text_projection(text_embeds)\n",
        "\n",
        "        # normalized features\n",
        "        text_embeds = text_embeds / text_embeds.norm(p=2, dim=-1, keepdim=True)\n",
        "\n",
        "        # cosine similarity as logits\n",
        "        prob_per_pair = text_embeds[0] * text_embeds[1] #torch.mm(text_embeds[0], text_embeds[1]) #* logit_scale\n",
        "\n",
        "        return prob_per_pair\n",
        "\n",
        "\n",
        "model = CLIPModel_Text()\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "inputs = processor(text=[\"hello, how are you?\", \"a photo of a dog\"], return_tensors=\"pt\", padding=True)\n",
        "prob_per_pair = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
        "print(\"prob per pair:\",prob_per_pair.shape, prob_per_pair.max(), prob_per_pair.min(), prob_per_pair.sum())\n",
        "\n",
        "\n",
        "inputs = processor(text=[\" a photo of a cat\", \"a photo of a dog\"], return_tensors=\"pt\", padding=True)\n",
        "prob_per_pair = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
        "print(\"prob per pair:\",prob_per_pair.shape, prob_per_pair.max(), prob_per_pair.min(), prob_per_pair.sum())"
      ],
      "metadata": {
        "id": "cfLvjelZIiZB",
        "outputId": "54a59c44-0f7a-4cf3-d62d-0dccb9e196dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prob per pair: torch.Size([512]) tensor(0.3729, grad_fn=<MaxBackward1>) tensor(-0.0018, grad_fn=<MinBackward1>) tensor(0.8267, grad_fn=<SumBackward0>)\n",
            "prob per pair: torch.Size([512]) tensor(0.3537, grad_fn=<MaxBackward1>) tensor(-0.0006, grad_fn=<MinBackward1>) tensor(0.9310, grad_fn=<SumBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CLIP Language-Language Custom Vector Probability or Feature Map Double Pair:"
      ],
      "metadata": {
        "id": "VKP09tHbZ9dg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "class CLIPModel_Text(nn.Module):\n",
        "    def __init__(self, ):\n",
        "        super(CLIPModel_Text, self).__init__()\n",
        "        model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "        self.config = model.config\n",
        "        self.text_model = model.text_model\n",
        "        self.text_projection = model.text_projection\n",
        "        self.logit_scale = model.logit_scale\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids = None,\n",
        "        attention_mask = None,\n",
        "        position_ids = None,\n",
        "        return_loss = None,\n",
        "        output_attentions = None,\n",
        "        output_hidden_states = None,\n",
        "        return_dict = None,\n",
        "    ):\n",
        "\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states)\n",
        "\n",
        "        text_outputs = self.text_model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            position_ids=position_ids,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        text_embeds = text_outputs[1]\n",
        "        text_embeds = self.text_projection(text_embeds)\n",
        "\n",
        "        # normalized features\n",
        "        text_embeds = text_embeds / text_embeds.norm(p=2, dim=-1, keepdim=True)\n",
        "        print(text_embeds.shape)\n",
        "        # cosine similarity as logits\n",
        "        prob_per_pair1 = text_embeds[0] * text_embeds[1] #torch.mm(text_embeds[0], text_embeds[1]) #* logit_scale\\\n",
        "        prob_per_pair2 = text_embeds[2] * text_embeds[3]\n",
        "\n",
        "        return torch.stack([prob_per_pair1, prob_per_pair2])\n",
        "\n",
        "\n",
        "model = CLIPModel_Text()\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "pairs_all = [\"a photo of a dog\", \"a photo of a dog\"]\n",
        "pair2 = [\"a photo of a cat cow\", \"a photo of a dog\"]\n",
        "pairs_all.extend(pair2)\n",
        "\n",
        "pairs = processor(text=pairs_all, return_tensors=\"pt\", padding=True)\n",
        "prob_per_pair = model(input_ids=pairs['input_ids'], attention_mask=pairs['attention_mask'])\n",
        "print(\"Logit per pair:\",prob_per_pair.shape, prob_per_pair.sum(dim=1))"
      ],
      "metadata": {
        "id": "BwsxrenARHWB",
        "outputId": "eb70533b-ff82-4e2b-9056-92cd6f2005a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 512])\n",
            "Logit per pair: torch.Size([2, 512]) tensor([1.0000, 0.8543], grad_fn=<SumBackward1>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CLIP Language-Language Custom Vector Probability or Feature Map Double Pair [Efficient]:"
      ],
      "metadata": {
        "id": "v48-kMQFaBc-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "class CLIPModel_Text(nn.Module):\n",
        "    def __init__(self, ):\n",
        "        super(CLIPModel_Text, self).__init__()\n",
        "        model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "        self.config = model.config\n",
        "        self.text_model = model.text_model\n",
        "        self.text_projection = model.text_projection\n",
        "        self.logit_scale = model.logit_scale\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids = None,\n",
        "        attention_mask = None,\n",
        "        position_ids = None,\n",
        "        return_loss = None,\n",
        "        output_attentions = None,\n",
        "        output_hidden_states = None,\n",
        "        return_dict = None,\n",
        "    ):\n",
        "\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states)\n",
        "\n",
        "        text_outputs = self.text_model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            position_ids=position_ids,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        text_embeds = text_outputs[1]\n",
        "        text_embeds = self.text_projection(text_embeds)\n",
        "\n",
        "        # normalized features\n",
        "        text_embeds = text_embeds / text_embeds.norm(p=2, dim=-1, keepdim=True)\n",
        "        print(text_embeds.shape)\n",
        "        # cosine similarity as logits\n",
        "        total_samples = len(text_embeds)\n",
        "        prob_per_pair = text_embeds[torch.arange(0,total_samples,2),:] * text_embeds[torch.arange(1,total_samples,2),:]\n",
        "\n",
        "        return prob_per_pair\n",
        "\n",
        "\n",
        "model = CLIPModel_Text()\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "pairs_all = [\"a photo of a dog\", \"a photo of a dog\"]\n",
        "pair2 = [\"a photo of a cat cow\", \"a photo of a dog\"]\n",
        "pairs_all.extend(pair2)\n",
        "\n",
        "pairs = processor(text=pairs_all, return_tensors=\"pt\", padding=True)\n",
        "prob_per_pair = model(input_ids=pairs['input_ids'], attention_mask=pairs['attention_mask'])\n",
        "print(\"Logit per pair:\",prob_per_pair.shape, prob_per_pair.sum(dim=1))"
      ],
      "metadata": {
        "id": "uAy9WB02RWgs",
        "outputId": "99b3a1fa-d4bb-4213-cfee-a168e665716f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 512])\n",
            "Logit per pair: torch.Size([2, 512]) tensor([1.0000, 0.8543], grad_fn=<SumBackward1>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, CLIPModel\n",
        "\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n",
        "text_features = model.get_text_features(**inputs)\n",
        "print('text_features:', text_features.shape)"
      ],
      "metadata": {
        "id": "2lrmXe0VFC5m",
        "outputId": "ea966b05-3538-4b31-af80-848e33deff44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text_features: torch.Size([2, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "from transformers import AutoProcessor, CLIPModel\n",
        "\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "inputs = processor(images=image, return_tensors=\"pt\")\n",
        "\n",
        "image_features = model.get_image_features(**inputs)\n",
        "\n",
        "print('image_features:', image_features.shape)"
      ],
      "metadata": {
        "id": "YSfr2FWnJOl2",
        "outputId": "ec0c41b6-5a8b-49aa-e7f4-1eac4ccf3117",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "image_features: torch.Size([1, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, CLIPTextModel\n",
        "\n",
        "model = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n",
        "\n",
        "outputs = model(**inputs)\n",
        "last_hidden_state = outputs.last_hidden_state\n",
        "pooled_output = outputs.pooler_output  # pooled (EOS token) states\n",
        "print('pooled_output:', pooled_output.shape)"
      ],
      "metadata": {
        "id": "sEIm54LXJZpv",
        "outputId": "613bb96e-983a-4f90-a11b-3dcab7c4981a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pooled_output: torch.Size([2, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, CLIPTextModelWithProjection\n",
        "\n",
        "model = CLIPTextModelWithProjection.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n",
        "\n",
        "outputs = model(**inputs)\n",
        "text_embeds = outputs.text_embeds\n",
        "print('text_embeds', text_embeds.shape)"
      ],
      "metadata": {
        "id": "x_2lYAEYJovY",
        "outputId": "5db89b39-fc36-4868-ff1e-6e5bba7407bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text_embeds torch.Size([2, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZZ3MnBQBJzEe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}