{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO16mpvtRGgkvAkon1lf/bh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/tutorial_notebooks/blob/main/finding_image_similarity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Endovis 2018"
      ],
      "metadata": {
        "id": "5963_eNSztPW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIqI5zG1jHw_"
      },
      "outputs": [],
      "source": [
        "import gdown\n",
        "\n",
        "#endovis18 dataset\n",
        "url = 'https://drive.google.com/uc?id=1lRNAgC-6QgIQd-vum-jr523tYPr6yNM7'\n",
        "gdown.download(url,'endovis18.zip',quiet=True)\n",
        "!unzip -q endovis18.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage.metrics import structural_similarity as ssim\n",
        "import cv2\n",
        "from glob import glob\n",
        "img_path = glob ('endovis18/*/left_frames/*.png')\n",
        "\n",
        "imageA = cv2.imread('/content/img.png') #cropped and upload the image from paper as name img.png\n",
        "imageA = cv2.cvtColor(imageA, cv2.COLOR_BGR2GRAY)\n",
        "imageA = cv2.resize(imageA, (224,224))\n",
        "\n",
        "for path in img_path:\n",
        "    imageB = cv2.imread(path)\n",
        "    imageB = cv2.cvtColor(imageB, cv2.COLOR_BGR2GRAY)\n",
        "    imageB = cv2.resize(imageB, (224, 224))\n",
        "    s = ssim(imageA, imageB)\n",
        "    if s >= .9:\n",
        "        print(path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ss-L3kUewxiD",
        "outputId": "4c18f282-8c15-4e58-d706-976ea15486fa"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "endovis18/seq_5/left_frames/frame029.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Endovis 2017"
      ],
      "metadata": {
        "id": "EmQaSBFpzvsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "\n",
        "url = 'https://drive.google.com/uc?id=1nroWvgxBRCIx9PP0bbIOFPretpmvTVuH'\n",
        "gdown.download(url,'endovis17.zip',quiet=True)\n",
        "!unzip -q endovis17.zip -d endovis17"
      ],
      "metadata": {
        "id": "dLMgVcv9zw_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage.metrics import structural_similarity as ssim\n",
        "import cv2\n",
        "from glob import glob\n",
        "img_path = glob ('endovis17/*/images/*.png')\n",
        "\n",
        "imageA = cv2.imread('/content/img.png') #cropped and upload the image from paper as name img.png\n",
        "imageA = cv2.cvtColor(imageA, cv2.COLOR_BGR2GRAY)\n",
        "imageA = cv2.resize(imageA, (224,224))\n",
        "\n",
        "for path in img_path:\n",
        "    imageB = cv2.imread(path)\n",
        "    imageB = cv2.cvtColor(imageB, cv2.COLOR_BGR2GRAY)\n",
        "    imageB = cv2.resize(imageB, (224, 224))\n",
        "    s = ssim(imageA, imageB)\n",
        "    if s >= .9:\n",
        "        print(path)\n"
      ],
      "metadata": {
        "id": "9ffskmJpz8b9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import gdown\n",
        "\n",
        "url = 'https://drive.google.com/uc?id=1hY6jXjREiDXGx25bSdzNYb58amYkTzCL'\n",
        "gdown.download(url,'paper_fig.zip',quiet=True)\n",
        "!unzip -q paper_fig.zip"
      ],
      "metadata": {
        "id": "KPJ-ZdtiGIOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage.metrics import structural_similarity as ssim\n",
        "import cv2\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "img_path_all = glob ('endovis18/*/left_frames/*.png')\n",
        "img_path_all.extend(glob ('endovis17/*/images/*.png'))\n",
        "paper_img_path_all = glob('/content/paper_fig/*.png')\n",
        "\n",
        "for paper_img_path in paper_img_path_all:\n",
        "\n",
        "    print('Paper image:', paper_img_path)\n",
        "    imageA = cv2.imread(paper_img_path) #cropped and upload the image from paper as name img.png\n",
        "    imageA = cv2.cvtColor(imageA, cv2.COLOR_BGR2GRAY)\n",
        "    imageA = cv2.resize(imageA, (24,24))\n",
        "    ssim_all = []\n",
        "    for img_path in img_path_all:\n",
        "        imageB = cv2.imread(img_path)\n",
        "        imageB = cv2.cvtColor(imageB, cv2.COLOR_BGR2GRAY)\n",
        "        imageB = cv2.resize(imageB, (24, 24))\n",
        "        s = ssim(imageA, imageB)\n",
        "        ssim_all.append(s)\n",
        "        # if s >= .9:\n",
        "        #     print(img_path)\n",
        "    sorted_idx = np.array(ssim_all).argsort()[::-1]\n",
        "    print('best match 1:', ssim_all[sorted_idx[0]], ' Path:', img_path_all[sorted_idx[0]])\n",
        "    print('best match 2:', ssim_all[sorted_idx[1]], ' Path:', img_path_all[sorted_idx[1]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9LMMvz4GOl7",
        "outputId": "42803221-45ce-4198-b46d-97c62b235d43"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paper image: /content/paper_fig/img5.png\n",
            "best match 1: 0.3468168106885421  Path: endovis18/seq_6/left_frames/frame117.png\n",
            "best match 2: 0.33931758295432235  Path: endovis18/seq_15/left_frames/frame106.png\n",
            "Paper image: /content/paper_fig/img3.png\n",
            "best match 1: 0.3007221609290306  Path: endovis18/seq_4/left_frames/frame094.png\n",
            "best match 2: 0.2599373316133494  Path: endovis18/seq_3/left_frames/frame135.png\n",
            "Paper image: /content/paper_fig/img1.png\n",
            "best match 1: 0.972921409270982  Path: endovis18/seq_5/left_frames/frame029.png\n",
            "best match 2: 0.6889995415171442  Path: endovis18/seq_5/left_frames/frame028.png\n",
            "Paper image: /content/paper_fig/img8.png\n",
            "best match 1: 0.25414857451348455  Path: endovis18/seq_6/left_frames/frame080.png\n",
            "best match 2: 0.2396823007672693  Path: endovis18/seq_5/left_frames/frame077.png\n",
            "Paper image: /content/paper_fig/img17.png\n",
            "best match 1: 0.26614651879432777  Path: endovis17/instrument_dataset_10/images/frame011.png\n",
            "best match 2: 0.2647758203013191  Path: endovis18/seq_15/left_frames/frame105.png\n",
            "Paper image: /content/paper_fig/img11.png\n",
            "best match 1: 0.2825157232853685  Path: endovis18/seq_11/left_frames/frame039.png\n",
            "best match 2: 0.27149473056432494  Path: endovis18/seq_11/left_frames/frame028.png\n",
            "Paper image: /content/paper_fig/img7.png\n",
            "best match 1: 0.2907858895832434  Path: endovis18/seq_16/left_frames/frame133.png\n",
            "best match 2: 0.2719821602204258  Path: endovis18/seq_12/left_frames/frame142.png\n",
            "Paper image: /content/paper_fig/img14.png\n",
            "best match 1: 0.3179362998241698  Path: endovis18/seq_12/left_frames/frame077.png\n",
            "best match 2: 0.3148895063864799  Path: endovis17/instrument_dataset_10/images/frame016.png\n",
            "Paper image: /content/paper_fig/img9.png\n",
            "best match 1: 0.2545646404153241  Path: endovis17/instrument_dataset_10/images/frame263.png\n",
            "best match 2: 0.25444299903085627  Path: endovis18/seq_3/left_frames/frame074.png\n",
            "Paper image: /content/paper_fig/img15.png\n",
            "best match 1: 0.2671601681215578  Path: endovis17/instrument_dataset_10/images/frame268.png\n",
            "best match 2: 0.253367885585754  Path: endovis18/seq_1/left_frames/frame124.png\n",
            "Paper image: /content/paper_fig/img6.png\n",
            "best match 1: 0.3581461548329198  Path: endovis18/seq_15/left_frames/frame105.png\n",
            "best match 2: 0.35157757636148645  Path: endovis18/seq_15/left_frames/frame102.png\n",
            "Paper image: /content/paper_fig/img12.png\n",
            "best match 1: 0.36582732135860246  Path: endovis18/seq_14/left_frames/frame010.png\n",
            "best match 2: 0.3390748485734376  Path: endovis18/seq_6/left_frames/frame049.png\n",
            "Paper image: /content/paper_fig/img2.png\n",
            "best match 1: 0.28411714615778993  Path: endovis17/instrument_dataset_10/images/frame032.png\n",
            "best match 2: 0.27879034931494817  Path: endovis18/seq_14/left_frames/frame017.png\n",
            "Paper image: /content/paper_fig/img10.png\n",
            "best match 1: 0.4331232420441828  Path: endovis18/seq_9/left_frames/frame140.png\n",
            "best match 2: 0.3977143732177066  Path: endovis18/seq_10/left_frames/frame147.png\n",
            "Paper image: /content/paper_fig/img4.png\n",
            "best match 1: 0.9828159026948905  Path: endovis18/seq_2/left_frames/frame037.png\n",
            "best match 2: 0.4709976421956305  Path: endovis18/seq_2/left_frames/frame035.png\n",
            "Paper image: /content/paper_fig/img13.png\n",
            "best match 1: 0.477782263798422  Path: endovis18/seq_10/left_frames/frame140.png\n",
            "best match 2: 0.4498873255913062  Path: endovis18/seq_10/left_frames/frame139.png\n",
            "Paper image: /content/paper_fig/img16.png\n",
            "best match 1: 0.2797614160382401  Path: endovis18/seq_2/left_frames/frame047.png\n",
            "best match 2: 0.2794613605923047  Path: endovis18/seq_7/left_frames/frame068.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using OpenCLIP"
      ],
      "metadata": {
        "id": "v1sCeG8155Qn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install git+https://github.com/openai/CLIP.git\n",
        "!pip -q install open_clip_torch\n",
        "!pip -q install sentence_transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJuqXFOVMeWu",
        "outputId": "bdcdc15a-4805-4bca-a04d-80729d8ae3cf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m96.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import open_clip\n",
        "import cv2\n",
        "from sentence_transformers import util\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "# Load images\n",
        "img_path = '/content/paper_fig/img1.png'\n",
        "# image1 = cv2.imread(img_path)\n",
        "# image2 = cv2.imread(img_path)\n",
        "# image1 = cv2.resize(image1, (24, 24))\n",
        "# image2 = cv2.resize(image2, (24, 24))\n",
        "# image processing model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-16-plus-240', pretrained=\"laion400m_e32\")\n",
        "model.to(device)\n",
        "def imageEncoder(img):\n",
        "    img1 = Image.fromarray(img).convert('RGB')\n",
        "    img1 = preprocess(img1).unsqueeze(0).to(device)\n",
        "    img1 = model.encode_image(img1)\n",
        "    return img1\n",
        "def generateScore(image1, image2):\n",
        "    test_img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
        "    data_img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
        "    img1 = imageEncoder(test_img)\n",
        "    img2 = imageEncoder(data_img)\n",
        "    cos_scores = util.pytorch_cos_sim(img1, img2)\n",
        "    score = round(float(cos_scores[0][0])*100, 2)\n",
        "    return score\n",
        "\n",
        "\n",
        "print(f\"similarity Score: \", round(generateScore(img_path, img_path), 2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SoeO2NKe57tK",
        "outputId": "cc337788-7996-432e-dcf8-fa3850a949b7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "similarity Score:  100.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "from sentence_transformers import util\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, root_paths =None, transform=None):\n",
        "        self.img_path_all = []\n",
        "        for root_path in root_paths:\n",
        "            self.img_path_all.extend(glob (root_path))\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_path_all)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        img = Image.open(self.img_path_all[i]).convert('RGB')\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, self.img_path_all[i]\n",
        "\n",
        "# def main():\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-16-plus-240', pretrained=\"laion400m_e32\")\n",
        "model.to(device)\n",
        "root_paths = ['endovis18/*/left_frames/*.png', 'endovis17/*/images/*.png']\n",
        "paper_root_paths = ['/content/paper_fig/*.png']\n",
        "img_dataset = ImageDataset(root_paths=root_paths, transform=preprocess)\n",
        "paper_img_dataset = ImageDataset(root_paths=paper_root_paths, transform=preprocess)\n",
        "\n",
        "img_dataloader = DataLoader(dataset=img_dataset, batch_size=80, shuffle=False, num_workers=2)\n",
        "paper_img_dataloader = DataLoader(dataset=paper_img_dataset, batch_size=20, shuffle=False, num_workers=2)\n",
        "\n",
        "paper_paths_all = []\n",
        "with torch.no_grad():\n",
        "    for paper_imgs, paper_paths in paper_img_dataloader:\n",
        "        paper_img_batch = paper_imgs.to(device)\n",
        "        paper_img_features = model.encode_image(paper_img_batch)\n",
        "        paper_paths_all.extend(paper_paths)\n",
        "\n",
        "    for paper_img_feat, paper_img_path in zip(paper_img_features, paper_paths_all):\n",
        "        print('Paper image:', paper_img_path)\n",
        "        clip_score_all = []\n",
        "        paths_all = []\n",
        "        for img_batch, paths in img_dataloader:\n",
        "            img_batch = img_batch.to(device)\n",
        "            feature_batch = model.encode_image(img_batch)\n",
        "            paths_all.extend(paths)\n",
        "            # for feature in feature_batch:\n",
        "            cos_scores = util.pytorch_cos_sim(feature_batch, paper_img_feat)\n",
        "            score = cos_scores[:,0].cpu().numpy().round(2)\n",
        "            clip_score_all.extend(score)\n",
        "\n",
        "        sorted_idx = np.array(clip_score_all).argsort()[::-1]\n",
        "        print('best match 1:', clip_score_all[sorted_idx[0]], ' Path:', paths_all[sorted_idx[0]])\n",
        "        print('best match 2:', clip_score_all[sorted_idx[1]], ' Path:', paths_all[sorted_idx[1]])\n",
        "\n",
        "    print(feature_batch.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yQ0QkPHCPPS",
        "outputId": "3f3b2f39-220c-4c03-b701-e23bb8a312cf"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paper image: /content/paper_fig/img5.png\n",
            "best match 1: 0.95  Path: endovis18/seq_6/left_frames/frame002.png\n",
            "best match 2: 0.95  Path: endovis17/instrument_dataset_10/images/frame243.png\n",
            "Paper image: /content/paper_fig/img16.png\n",
            "best match 1: 0.87  Path: endovis17/instrument_dataset_10/images/frame177.png\n",
            "best match 2: 0.87  Path: endovis17/instrument_dataset_10/images/frame148.png\n",
            "Paper image: /content/paper_fig/img17.png\n",
            "best match 1: 0.86  Path: endovis18/seq_14/left_frames/frame116.png\n",
            "best match 2: 0.85  Path: endovis17/instrument_dataset_10/images/frame070.png\n",
            "Paper image: /content/paper_fig/img12.png\n",
            "best match 1: 0.81  Path: endovis17/instrument_dataset_10/images/frame011.png\n",
            "best match 2: 0.81  Path: endovis17/instrument_dataset_10/images/frame009.png\n",
            "Paper image: /content/paper_fig/img6.png\n",
            "best match 1: 0.94  Path: endovis18/seq_14/left_frames/frame128.png\n",
            "best match 2: 0.94  Path: endovis18/seq_6/left_frames/frame010.png\n",
            "Paper image: /content/paper_fig/img9.png\n",
            "best match 1: 0.87  Path: endovis18/seq_6/left_frames/frame105.png\n",
            "best match 2: 0.87  Path: endovis18/seq_4/left_frames/frame134.png\n",
            "Paper image: /content/paper_fig/img4.png\n",
            "best match 1: 0.99  Path: endovis18/seq_2/left_frames/frame037.png\n",
            "best match 2: 0.96  Path: endovis18/seq_2/left_frames/frame064.png\n",
            "Paper image: /content/paper_fig/img14.png\n",
            "best match 1: 0.92  Path: endovis17/instrument_dataset_10/images/frame070.png\n",
            "best match 2: 0.92  Path: endovis18/seq_3/left_frames/frame021.png\n",
            "Paper image: /content/paper_fig/img1.png\n",
            "best match 1: 0.98  Path: endovis18/seq_5/left_frames/frame029.png\n",
            "best match 2: 0.96  Path: endovis18/seq_5/left_frames/frame028.png\n",
            "Paper image: /content/paper_fig/img8.png\n",
            "best match 1: 0.94  Path: endovis17/instrument_dataset_9/images/frame255.png\n",
            "best match 2: 0.94  Path: endovis18/seq_2/left_frames/frame087.png\n",
            "Paper image: /content/paper_fig/img3.png\n",
            "best match 1: 0.94  Path: endovis18/seq_14/left_frames/frame000.png\n",
            "best match 2: 0.94  Path: endovis18/seq_2/left_frames/frame131.png\n",
            "Paper image: /content/paper_fig/img13.png\n",
            "best match 1: 0.9  Path: endovis18/seq_14/left_frames/frame100.png\n",
            "best match 2: 0.9  Path: endovis18/seq_6/left_frames/frame119.png\n",
            "Paper image: /content/paper_fig/img7.png\n",
            "best match 1: 0.95  Path: endovis18/seq_6/left_frames/frame012.png\n",
            "best match 2: 0.95  Path: endovis18/seq_6/left_frames/frame082.png\n",
            "Paper image: /content/paper_fig/img11.png\n",
            "best match 1: 0.89  Path: endovis17/instrument_dataset_10/images/frame177.png\n",
            "best match 2: 0.89  Path: endovis18/seq_3/left_frames/frame012.png\n",
            "Paper image: /content/paper_fig/img2.png\n",
            "best match 1: 0.95  Path: endovis17/instrument_dataset_10/images/frame210.png\n",
            "best match 2: 0.94  Path: endovis17/instrument_dataset_10/images/frame218.png\n",
            "Paper image: /content/paper_fig/img10.png\n",
            "best match 1: 0.84  Path: endovis17/instrument_dataset_10/images/frame010.png\n",
            "best match 2: 0.84  Path: endovis18/seq_14/left_frames/frame116.png\n",
            "Paper image: /content/paper_fig/img15.png\n",
            "best match 1: 0.86  Path: endovis17/instrument_dataset_9/images/frame058.png\n",
            "best match 2: 0.85  Path: endovis18/seq_6/left_frames/frame143.png\n",
            "torch.Size([46, 640])\n"
          ]
        }
      ]
    }
  ]
}