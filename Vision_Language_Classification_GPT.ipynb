{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/tutorial_notebooks/blob/main/Vision_Language_Classification_GPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download Dataset"
      ],
      "metadata": {
        "id": "nPY5qD6tVTwt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fICJtLD5tteM"
      },
      "outputs": [],
      "source": [
        "import gdown\n",
        "url = 'https://drive.google.com/uc?id=1AOuJXt9yWZfLwPoZsFfWFEgrcERGehOm'\n",
        "gdown.download(url,'archive.zip',quiet=True)\n",
        "!unzip -q archive.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Packages"
      ],
      "metadata": {
        "id": "7W4KvlbeVXbA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoEoTEsv4U7t",
        "outputId": "2a1641e2-acba-41a4-e55c-714c900c0834"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare Dataloader"
      ],
      "metadata": {
        "id": "HbuBYtALVa1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizerFast\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data_root=None, transform=None, istrain=True):\n",
        "        if istrain:\n",
        "            self.data_json= pd.read_json(path_or_buf=os.path.join(data_root,'train.jsonl'), lines=True)\n",
        "        else:\n",
        "            self.data_json= pd.read_json(path_or_buf=os.path.join(data_root,'dev.jsonl'), lines=True)\n",
        "\n",
        "        self.transform = transform\n",
        "        self.img_root = os.path.join( data_root, 'img')\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.data_json['id'])\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        img = Image.open(os.path.join( data_root, 'img', str(\"{:05d}\".format(self.data_json['id'][i]))+'.png')).convert('RGB')\n",
        "        text = self.data_json['text'][i]\n",
        "        label = self.data_json['label'][i]\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, text, label\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "        )\n",
        "    ])\n",
        "\n",
        "data_root = '/content/data'\n",
        "dataset_train = CustomDataset(data_root=data_root, transform=transform, istrain=True)\n",
        "dataset_test = CustomDataset(data_root=data_root, transform=transform, istrain=False)\n",
        "print('Number of training samples:', len(dataset_train), 'Number of test samples:',len(dataset_test))\n",
        "\n",
        "dataloader_train = DataLoader(dataset_train, batch_size=2, shuffle=True, num_workers=2)\n",
        "dataloader_test = DataLoader(dataset_test, batch_size=4, shuffle=False, num_workers=2)\n",
        "print('Number of training loader:', len(dataloader_train), 'Number of test loader:',len(dataloader_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qbt3NhwQ80hZ",
        "outputId": "7e866119-e5b5-4543-ad2a-5c40520779f5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training samples: 8500 Number of test samples: 500\n",
            "Number of training loader: 4250 Number of test loader: 125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training V1: (ResNet18 and without VQA pretrained weights)"
      ],
      "metadata": {
        "id": "rnQzH8JUVeGF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "import argparse\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision.models import resnet18\n",
        "from transformers import VisualBertModel, VisualBertConfig, BertTokenizerFast\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def get_args():\n",
        "    parser = argparse.ArgumentParser(description='CIFAR-10H Training')\n",
        "    parser.add_argument('--lr', default=0.00001, type=float, help='learning rate')\n",
        "    parser.add_argument('--batch_size', default=20, type=int, help='batch size')\n",
        "    parser.add_argument('--test_batch_size', default=40, type=int, help='batch size')\n",
        "    parser.add_argument('--num_epoch', default=2, type=int, help='epoch number')\n",
        "    parser.add_argument('--num_classes', type=int, default=2, help='number classes')\n",
        "\n",
        "    if 'ipykernel' in sys.modules:\n",
        "        args = parser.parse_args([])\n",
        "    else:\n",
        "        args = parser.parse_args()\n",
        "\n",
        "    return args\n",
        "\n",
        "\n",
        "class visual_feat_extractor(nn.Module):\n",
        "    def __init__(self, ):\n",
        "        super(visual_feat_extractor, self).__init__()\n",
        "        self.model_visual_feat = resnet18(pretrained=True)#b 1000\n",
        "        self.model_visual_feat.avgpool = nn.Identity()\n",
        "        self.model_visual_feat.fc = nn.Identity()\n",
        "        self.model_visual_feat.to(device)\n",
        "        self.model_visual_feat.eval()\n",
        "\n",
        "    def forward(self, img):\n",
        "        visual_embeds = self.model_visual_feat(img).view(-1, 49, 512) #b 49 512\n",
        "        return visual_embeds\n",
        "\n",
        "class VisualBERT_VQA(nn.Module):\n",
        "    def __init__(self, num_labels=2):\n",
        "        super(VisualBERT_VQA, self).__init__()\n",
        "        self.config = VisualBertConfig.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n",
        "        self.config.visual_embedding_dim = 512\n",
        "        self.visualbert = VisualBertModel(config=self.config)\n",
        "        self.cls = nn.Linear(768, num_labels)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        last_hidden_state = self.visualbert(**inputs).last_hidden_state #[1, 56, 768]\n",
        "\n",
        "        # Get the index of the last text token\n",
        "        index_to_gather = inputs['attention_mask'].sum(1) - 2  # as in original\n",
        "        index_to_gather = (\n",
        "            index_to_gather.unsqueeze(-1).unsqueeze(-1).expand(index_to_gather.size(0), 1, last_hidden_state.size(-1))\n",
        "        ) # [b c hw]=[1, 1, 768]\n",
        "        pooled_output = torch.gather(last_hidden_state, 1, index_to_gather) # [1, 1, 768]\n",
        "        logits = self.cls(pooled_output).squeeze(1)\n",
        "        return logits\n",
        "\n",
        "def train_epoch(model_vqa, text_model, img_model, dataloader_train, criterion, optimizer):\n",
        "    model_vqa.train()\n",
        "    loss_all = 0\n",
        "    for batch_idx, (imgs, texts, targets) in enumerate(dataloader_train):\n",
        "        imgs, targets = imgs.to(device), targets.to(device)\n",
        "        inputs = text_model(texts, return_tensors=\"pt\", padding=\"max_length\", max_length=20, truncation=True).to(device)#b 20\n",
        "        with torch.no_grad():\n",
        "            img_embed = img_model(imgs)#b 512\n",
        "\n",
        "        visual_token_type_ids = torch.ones(img_embed.shape[:-1], dtype=torch.long).to(device)\n",
        "        visual_attention_mask = torch.ones(img_embed.shape[:-1], dtype=torch.float).to(device)\n",
        "        inputs.update({\n",
        "                \"visual_embeds\": img_embed,\n",
        "                \"visual_token_type_ids\": visual_token_type_ids,\n",
        "                \"visual_attention_mask\": visual_attention_mask,\n",
        "            })\n",
        "        optimizer.zero_grad()\n",
        "        logits = model_vqa(inputs)\n",
        "        loss = criterion(logits, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss_all += loss.item()\n",
        "\n",
        "    return loss_all/len(dataloader_train.dataset)\n",
        "\n",
        "def test_epoch(model, text_model, img_model, dataloader_test, criterion):\n",
        "    model.eval()\n",
        "    test_sample_size = len(dataloader_test.dataset)\n",
        "    correct = 0\n",
        "    loss_all = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (imgs, texts, targets) in enumerate(dataloader_test):\n",
        "            imgs, targets = imgs.to(device), targets.to(device)\n",
        "            inputs = text_model(texts, return_tensors=\"pt\", padding=\"max_length\", max_length=20, truncation=True).to(device)\n",
        "            with torch.no_grad():\n",
        "                img_embed = img_model(imgs)\n",
        "\n",
        "            visual_token_type_ids = torch.ones(img_embed.shape[:-1], dtype=torch.long).to(device)\n",
        "            visual_attention_mask = torch.ones(img_embed.shape[:-1], dtype=torch.float).to(device)\n",
        "            inputs.update({\n",
        "                    \"visual_embeds\": img_embed,\n",
        "                    \"visual_token_type_ids\": visual_token_type_ids,\n",
        "                    \"visual_attention_mask\": visual_attention_mask,\n",
        "                })\n",
        "            logits = model_vqa(inputs)# b C logits/Prob. 40 2 1 img [0.1 0.9]\n",
        "            loss = criterion(logits, targets)\n",
        "            loss_all += loss.item()\n",
        "            _, predicted = logits.max(1)\n",
        "            #targets 40\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        print('Correct:', correct, 'Total Sample:', test_sample_size)\n",
        "\n",
        "    return correct / test_sample_size, loss_all / test_sample_size\n",
        "\n",
        "\n",
        "args = get_args()\n",
        "visual_embeds_model = visual_feat_extractor().to(device)\n",
        "visual_embeds_model.eval()\n",
        "bert_tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "model_vqa = VisualBERT_VQA(num_labels=args.num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model_vqa.parameters(), lr=args.lr)\n",
        "\n",
        "data_root = '/content/data'\n",
        "dataset_train = CustomDataset(data_root=data_root, transform=transform, istrain=True)\n",
        "dataset_test = CustomDataset(data_root=data_root, transform=transform, istrain=False)\n",
        "print('Number of training samples:', len(dataset_train), 'Number of test samples:',len(dataset_test))\n",
        "dataloader_train = DataLoader(dataset_train, batch_size=args.batch_size, shuffle=True, num_workers=2)\n",
        "dataloader_test = DataLoader(dataset_test, batch_size=args.test_batch_size, shuffle=False, num_workers=2)\n",
        "best_epoch, best_acc = 0.0, 0\n",
        "\n",
        "for epoch in range(args.num_epoch):\n",
        "    train_loss = train_epoch(model_vqa, bert_tokenizer, visual_embeds_model, dataloader_train, criterion, optimizer)\n",
        "    accuracy, test_loss = test_epoch(model_vqa, bert_tokenizer, visual_embeds_model, dataloader_test, criterion)\n",
        "    if accuracy > best_acc:\n",
        "        best_acc = accuracy\n",
        "        best_epoch = epoch\n",
        "        torch.save(model_vqa.state_dict(), 'best_model.pth.tar')\n",
        "\n",
        "    print('epoch: {}/{}  current:[train loss: {:.4f} test loss:{:.4f} acc: {:.4f}]  best epoch: {}  best acc: {:.4f}'.format(\n",
        "                epoch, args.num_epoch, train_loss, test_loss, accuracy, best_epoch, best_acc))\n",
        "\n"
      ],
      "metadata": {
        "id": "-_6957STC3bl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training V2: (ResNet101 and with VQA pretrained weights)"
      ],
      "metadata": {
        "id": "rHEEgTVfm_7S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "import argparse\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision.models import resnet18, resnet101\n",
        "from transformers import VisualBertModel, VisualBertConfig, BertTokenizerFast\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def get_args():\n",
        "    parser = argparse.ArgumentParser(description='CIFAR-10H Training')\n",
        "    parser.add_argument('--lr', default=0.00001, type=float, help='learning rate')\n",
        "    parser.add_argument('--batch_size', default=20, type=int, help='batch size')\n",
        "    parser.add_argument('--test_batch_size', default=40, type=int, help='batch size')\n",
        "    parser.add_argument('--num_epoch', default=50, type=int, help='epoch number')\n",
        "    parser.add_argument('--num_classes', type=int, default=2, help='number classes')\n",
        "\n",
        "    if 'ipykernel' in sys.modules:\n",
        "        args = parser.parse_args([])\n",
        "    else:\n",
        "        args = parser.parse_args()\n",
        "\n",
        "    return args\n",
        "\n",
        "\n",
        "class visual_feat_extractor(nn.Module):\n",
        "    def __init__(self, ):\n",
        "        super(visual_feat_extractor, self).__init__()\n",
        "        self.model_visual_feat = resnet101(pretrained=True)#b 1000\n",
        "        self.model_visual_feat.avgpool = nn.Identity()\n",
        "        self.model_visual_feat.fc = nn.Identity()\n",
        "        self.model_visual_feat.to(device)\n",
        "        self.model_visual_feat.eval()\n",
        "\n",
        "    def forward(self, img):\n",
        "        visual_embeds = self.model_visual_feat(img).view(-1, 49, 2048) #b 49 2048\n",
        "        return visual_embeds\n",
        "\n",
        "class VisualBERT_VQA(nn.Module):\n",
        "    def __init__(self, num_labels=2):\n",
        "        super(VisualBERT_VQA, self).__init__()\n",
        "        self.config = VisualBertConfig.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n",
        "        # self.config.visual_embedding_dim = 512\n",
        "        self.visualbert = VisualBertModel.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n",
        "        self.cls = nn.Linear(768, num_labels)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        last_hidden_state = self.visualbert(**inputs).last_hidden_state #[1, 56, 768]\n",
        "\n",
        "        # Get the index of the last text token\n",
        "        index_to_gather = inputs['attention_mask'].sum(1) - 2  # as in original\n",
        "        index_to_gather = (\n",
        "            index_to_gather.unsqueeze(-1).unsqueeze(-1).expand(index_to_gather.size(0), 1, last_hidden_state.size(-1))\n",
        "        ) # [b c hw]=[1, 1, 768]\n",
        "        pooled_output = torch.gather(last_hidden_state, 1, index_to_gather) # [1, 1, 768]\n",
        "        logits = self.cls(pooled_output).squeeze(1)\n",
        "        return logits\n",
        "\n",
        "def train_epoch(model, text_model, img_model, dataloader_train, criterion, optimizer):\n",
        "    model.train()\n",
        "    loss_all = 0\n",
        "    for batch_idx, (imgs, texts, targets) in enumerate(dataloader_train):\n",
        "        imgs, targets = imgs.to(device), targets.to(device)\n",
        "        inputs = text_model(texts, return_tensors=\"pt\", padding=\"max_length\", max_length=20, truncation=True).to(device)#b 20\n",
        "        with torch.no_grad():\n",
        "            img_embed = img_model(imgs)#b 512\n",
        "\n",
        "        visual_token_type_ids = torch.ones(img_embed.shape[:-1], dtype=torch.long).to(device)\n",
        "        visual_attention_mask = torch.ones(img_embed.shape[:-1], dtype=torch.float).to(device)\n",
        "        inputs.update({\n",
        "                \"visual_embeds\": img_embed,\n",
        "                \"visual_token_type_ids\": visual_token_type_ids,\n",
        "                \"visual_attention_mask\": visual_attention_mask,\n",
        "            })\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(inputs)\n",
        "        loss = criterion(logits, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss_all += loss.item()\n",
        "\n",
        "    return loss_all/len(dataloader_train.dataset)\n",
        "\n",
        "def test_epoch(model, text_model, img_model, dataloader_test, criterion):\n",
        "    model.eval()\n",
        "    test_sample_size = len(dataloader_test.dataset)\n",
        "    correct = 0\n",
        "    loss_all = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (imgs, texts, targets) in enumerate(dataloader_test):\n",
        "            imgs, targets = imgs.to(device), targets.to(device)\n",
        "            inputs = text_model(texts, return_tensors=\"pt\", padding=\"max_length\", max_length=20, truncation=True).to(device)\n",
        "            with torch.no_grad():\n",
        "                img_embed = img_model(imgs)\n",
        "\n",
        "            visual_token_type_ids = torch.ones(img_embed.shape[:-1], dtype=torch.long).to(device)\n",
        "            visual_attention_mask = torch.ones(img_embed.shape[:-1], dtype=torch.float).to(device)\n",
        "            inputs.update({\n",
        "                    \"visual_embeds\": img_embed,\n",
        "                    \"visual_token_type_ids\": visual_token_type_ids,\n",
        "                    \"visual_attention_mask\": visual_attention_mask,\n",
        "                })\n",
        "            logits = model_vqa(inputs)# b C logits/Prob. 40 2 1 img [0.1 0.9]\n",
        "            loss = criterion(logits, targets)\n",
        "            loss_all += loss.item()\n",
        "            _, predicted = logits.max(1)\n",
        "            #targets 40\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        print('Correct:', correct, 'Total Sample:', test_sample_size)\n",
        "\n",
        "    return correct / test_sample_size, loss_all / test_sample_size\n",
        "\n",
        "\n",
        "args = get_args()\n",
        "visual_embeds_model = visual_feat_extractor().to(device)\n",
        "visual_embeds_model.eval()\n",
        "bert_tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "model_vqa = VisualBERT_VQA(num_labels=args.num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model_vqa.parameters(), lr=args.lr)\n",
        "\n",
        "data_root = '/content/data'\n",
        "dataset_train = CustomDataset(data_root=data_root, transform=transform, istrain=True)\n",
        "dataset_test = CustomDataset(data_root=data_root, transform=transform, istrain=False)\n",
        "print('Number of training samples:', len(dataset_train), 'Number of test samples:',len(dataset_test))\n",
        "dataloader_train = DataLoader(dataset_train, batch_size=args.batch_size, shuffle=True, num_workers=2)\n",
        "dataloader_test = DataLoader(dataset_test, batch_size=args.test_batch_size, shuffle=False, num_workers=2)\n",
        "best_epoch, best_acc = 0.0, 0\n",
        "\n",
        "for epoch in range(args.num_epoch):\n",
        "    train_loss = train_epoch(model_vqa, bert_tokenizer, visual_embeds_model, dataloader_train, criterion, optimizer)\n",
        "    accuracy, test_loss = test_epoch(model_vqa, bert_tokenizer, visual_embeds_model, dataloader_test, criterion)\n",
        "    if accuracy > best_acc:\n",
        "        best_acc = accuracy\n",
        "        best_epoch = epoch\n",
        "        torch.save(model_vqa.state_dict(), 'best_model.pth.tar')\n",
        "\n",
        "    print('epoch: {}/{}  current:[train loss: {:.4f} test loss:{:.4f} test acc: {:.4f}]  best epoch: {}  best test acc: {:.4f}'.format(\n",
        "                epoch, args.num_epoch, train_loss, test_loss, accuracy, best_epoch, best_acc))\n",
        "\n"
      ],
      "metadata": {
        "id": "SSRT8e1rbELa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "outputId": "26874740-e487-46cf-b47c-920777721b43"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-e4a0f6b18d3e>\u001b[0m in \u001b[0;36m<cell line: 134>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_vqa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisual_embeds_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m     \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_vqa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisual_embeds_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_acc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-e4a0f6b18d3e>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model_vqa, text_model, img_model, dataloader_train, criterion, optimizer)\u001b[0m\n\u001b[1;32m     75\u001b[0m             })\n\u001b[1;32m     76\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_vqa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-e4a0f6b18d3e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mlast_hidden_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m \u001b[0;31m#[1, 56, 768]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m# Get the index of the last text token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/visual_bert/modeling_visual_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, visual_embeds, visual_attention_mask, visual_token_type_ids, image_text_alignment, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 844\u001b[0;31m             encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    845\u001b[0m                 \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/visual_bert/modeling_visual_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    433\u001b[0m                 )\n\u001b[1;32m    434\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m                 \u001b[0mlayer_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_head_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/visual_bert/modeling_visual_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m     ):\n\u001b[0;32m--> 372\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    373\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/visual_bert/modeling_visual_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     ):\n\u001b[0;32m--> 314\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/visual_bert/modeling_visual_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0mkey_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m         \u001b[0mvalue_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0mquery_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmixed_query_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mbound\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ViL with GPT2"
      ],
      "metadata": {
        "id": "BJYsWtC4MS7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "from torchvision.models import resnet18, resnet101\n",
        "from torchvision import transforms\n",
        "from transformers import BertTokenizer, GPT2Model, GPT2Tokenizer\n",
        "from transformers import VisualBertModel, BertTokenizer, VisualBertConfig, GPT2Model, GPT2Tokenizer, GPT2Config\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizerFast\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data_root=None, transform=None, istrain=True):\n",
        "        if istrain:\n",
        "            self.data_json= pd.read_json(path_or_buf=os.path.join(data_root,'train.jsonl'), lines=True)\n",
        "        else:\n",
        "            self.data_json= pd.read_json(path_or_buf=os.path.join(data_root,'dev.jsonl'), lines=True)\n",
        "\n",
        "        self.transform = transform\n",
        "        self.img_root = os.path.join( data_root, 'img')\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.data_json['id'])\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        img = Image.open(os.path.join( data_root, 'img', str(\"{:05d}\".format(self.data_json['id'][i]))+'.png')).convert('RGB')\n",
        "        text = self.data_json['text'][i]\n",
        "        label = self.data_json['label'][i]\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, text, label\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "        )\n",
        "    ])\n",
        "\n",
        "def get_args():\n",
        "    parser = argparse.ArgumentParser(description='CIFAR-10H Training')\n",
        "    parser.add_argument('--lr', default=0.00001, type=float, help='learning rate')\n",
        "    parser.add_argument('--batch_size', default=20, type=int, help='batch size')\n",
        "    parser.add_argument('--test_batch_size', default=40, type=int, help='batch size')\n",
        "    parser.add_argument('--num_epoch', default=50, type=int, help='epoch number')\n",
        "    parser.add_argument('--num_classes', type=int, default=2, help='number classes')\n",
        "\n",
        "    if 'ipykernel' in sys.modules:\n",
        "        args = parser.parse_args([])\n",
        "    else:\n",
        "        args = parser.parse_args()\n",
        "\n",
        "    return args\n",
        "\n",
        "class visual_feat_extractor(nn.Module):\n",
        "    def __init__(self, ):\n",
        "        super(visual_feat_extractor, self).__init__()\n",
        "        self.model_visual_feat = resnet18(pretrained=True)#b 1000\n",
        "        self.model_visual_feat.avgpool = nn.Identity()\n",
        "        self.model_visual_feat.fc = nn.Identity()\n",
        "        self.model_visual_feat.to(device)\n",
        "        self.model_visual_feat.eval()\n",
        "\n",
        "    def forward(self, img):\n",
        "        visual_embeds = self.model_visual_feat(img).view(-1, 49, 512) #b 49 2048\n",
        "        return visual_embeds\n",
        "\n",
        "class GPT2_VQA(nn.Module):\n",
        "    def __init__(self, num_class=2):\n",
        "        super(GPT2_VQA, self).__init__()\n",
        "        self.gpt2 = GPT2Model.from_pretrained('gpt2')\n",
        "        self.config = GPT2Config.from_pretrained(\"gpt2\")\n",
        "        self.classifier = nn.Linear(69 * 768, num_class)\n",
        "\n",
        "        self.config_bert = VisualBertConfig.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n",
        "        self.config_bert.visual_embedding_dim = 512 #most right dim of the visual features\n",
        "        self.config_bert.hidden_size = self.config.hidden_size\n",
        "        self.config_bert.vocab_size = self.config.vocab_size\n",
        "        self.config_bert.pad_token_id = self.config.pad_token_id\n",
        "\n",
        "        self.visualbert = VisualBertModel(config=self.config_bert)\n",
        "        # self.visualbert = VisualBertModel.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n",
        "        self.embeddings = self.visualbert.embeddings\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        hidden_states = self.embeddings(\n",
        "            input_ids=inputs['input_ids'],\n",
        "            token_type_ids=inputs['token_type_ids'],\n",
        "            position_ids=None,\n",
        "            inputs_embeds=None,\n",
        "            visual_embeds=inputs['visual_embeds'],\n",
        "            visual_token_type_ids=inputs['visual_token_type_ids'],\n",
        "            image_text_alignment=None,\n",
        "        )\n",
        "\n",
        "        hidden_states = self.gpt2.drop(hidden_states)\n",
        "        input_shape = inputs['input_ids'].size()\n",
        "        visual_input_shape = inputs['visual_embeds'].size()[:-1]\n",
        "        combined_attention_mask = torch.cat((inputs['attention_mask'], inputs['visual_attention_mask']), dim=-1)\n",
        "        extended_attention_mask: torch.Tensor = self.gpt2.get_extended_attention_mask(\n",
        "            combined_attention_mask, (input_shape[0], input_shape + visual_input_shape)\n",
        "        )\n",
        "        output_attentions = self.config.output_attentions\n",
        "        head_mask = self.gpt2.get_head_mask(None, self.config.n_layer)\n",
        "        past_key_values = tuple([None] * len(self.gpt2.h))\n",
        "        for i, (block, layer_past) in enumerate(zip(self.gpt2.h, past_key_values)):\n",
        "            outputs = block(\n",
        "                    hidden_states,\n",
        "                    layer_past=layer_past,\n",
        "                    attention_mask=extended_attention_mask,\n",
        "                    head_mask=head_mask[i],\n",
        "                    encoder_hidden_states=None,\n",
        "                    encoder_attention_mask=None,\n",
        "                    use_cache=None,\n",
        "                    output_attentions=output_attentions,\n",
        "                )\n",
        "\n",
        "            hidden_states = outputs[0]\n",
        "\n",
        "        hidden_states = self.gpt2.ln_f(hidden_states) #[2, 59, 768]\n",
        "        x = torch.flatten(hidden_states, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "def train_epoch(model, text_model, img_model, dataloader_train, criterion, optimizer):\n",
        "    model.train()\n",
        "    loss_all = 0\n",
        "    for batch_idx, (imgs, texts, targets) in enumerate(dataloader_train):\n",
        "        imgs, targets = imgs.to(device), targets.to(device)\n",
        "        inputs = text_model(texts, padding=\"max_length\",max_length= 20, return_tensors=\"pt\", truncation=True).to(device)\n",
        "        token_type_ids = torch.zeros(inputs['input_ids'].shape, dtype=torch.long).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            img_embed = img_model(imgs)#b 512\n",
        "\n",
        "        visual_token_type_ids = torch.ones(img_embed.shape[:-1], dtype=torch.long).to(device)\n",
        "        visual_attention_mask = torch.ones(img_embed.shape[:-1], dtype=torch.float).to(device)\n",
        "\n",
        "        inputs.update({\n",
        "            \"token_type_ids\": token_type_ids,\n",
        "            \"visual_embeds\": img_embed,\n",
        "            \"visual_token_type_ids\": visual_token_type_ids,\n",
        "            \"visual_attention_mask\": visual_attention_mask,\n",
        "            })\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(inputs)\n",
        "        loss = criterion(logits, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss_all += loss.item()\n",
        "        # break\n",
        "\n",
        "    return loss_all/len(dataloader_train.dataset)\n",
        "\n",
        "def test_epoch(model, text_model, img_model, dataloader_test, criterion):\n",
        "    model.eval()\n",
        "    test_sample_size = len(dataloader_test.dataset)\n",
        "    correct = 0\n",
        "    loss_all = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (imgs, texts, targets) in enumerate(dataloader_test):\n",
        "            imgs, targets = imgs.to(device), targets.to(device)\n",
        "            inputs = text_model(texts, padding=\"max_length\", max_length= 20, return_tensors=\"pt\", truncation=True).to(device)\n",
        "            token_type_ids = torch.zeros(inputs['input_ids'].shape, dtype=torch.long).to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                img_embed = img_model(imgs)\n",
        "\n",
        "            visual_token_type_ids = torch.ones(img_embed.shape[:-1], dtype=torch.long).to(device)\n",
        "            visual_attention_mask = torch.ones(img_embed.shape[:-1], dtype=torch.float).to(device)\n",
        "            inputs.update({\n",
        "                \"token_type_ids\": token_type_ids,\n",
        "                \"visual_embeds\": img_embed,\n",
        "                \"visual_token_type_ids\": visual_token_type_ids,\n",
        "                \"visual_attention_mask\": visual_attention_mask,\n",
        "                })\n",
        "            logits = model_vqa(inputs)# b C logits/Prob. 40 2 1 img [0.1 0.9]\n",
        "            loss = criterion(logits, targets)\n",
        "            loss_all += loss.item()\n",
        "            _, predicted = logits.max(1)\n",
        "            #targets 40\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "            # break\n",
        "\n",
        "        print('Correct:', correct, 'Total Sample:', test_sample_size)\n",
        "    return correct / test_sample_size, loss_all / test_sample_size\n",
        "\n",
        "args = get_args()\n",
        "visual_embeds_model = visual_feat_extractor().to(device)\n",
        "visual_embeds_model.eval()\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model_vqa = GPT2_VQA(num_class=args.num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model_vqa.parameters(), lr=args.lr)\n",
        "\n",
        "data_root = '/content/data'\n",
        "dataset_train = CustomDataset(data_root=data_root, transform=transform, istrain=True)\n",
        "dataset_test = CustomDataset(data_root=data_root, transform=transform, istrain=False)\n",
        "print('Number of training samples:', len(dataset_train), 'Number of test samples:',len(dataset_test))\n",
        "dataloader_train = DataLoader(dataset_train, batch_size=args.batch_size, shuffle=True, num_workers=2)\n",
        "dataloader_test = DataLoader(dataset_test, batch_size=args.test_batch_size, shuffle=False, num_workers=2)\n",
        "best_epoch, best_acc = 0.0, 0\n",
        "\n",
        "for epoch in range(args.num_epoch):\n",
        "    train_loss = train_epoch(model_vqa, tokenizer, visual_embeds_model, dataloader_train, criterion, optimizer)\n",
        "    accuracy, test_loss = test_epoch(model_vqa, tokenizer, visual_embeds_model, dataloader_test, criterion)\n",
        "    if accuracy > best_acc:\n",
        "        best_acc = accuracy\n",
        "        best_epoch = epoch\n",
        "        torch.save(model_vqa.state_dict(), 'best_model.pth.tar')\n",
        "\n",
        "    print('epoch: {}/{}  current:[train loss: {:.4f} test loss:{:.4f} test acc: {:.4f}]  best epoch: {}  best test acc: {:.4f}'.format(\n",
        "                epoch, args.num_epoch, train_loss, test_loss, accuracy, best_epoch, best_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCmYytFcEi2m",
        "outputId": "b7e53687-c724-4a36-ce46-72029b654ca6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training samples: 8500 Number of test samples: 500\n",
            "Correct: 250 Total Sample: 500\n",
            "epoch: 0/50  current:[train loss: 0.0349 test loss:0.0198 test acc: 0.5000]  best epoch: 0  best test acc: 0.5000\n",
            "Correct: 251 Total Sample: 500\n",
            "epoch: 1/50  current:[train loss: 0.0322 test loss:0.0197 test acc: 0.5020]  best epoch: 1  best test acc: 0.5020\n",
            "Correct: 267 Total Sample: 500\n",
            "epoch: 2/50  current:[train loss: 0.0315 test loss:0.0187 test acc: 0.5340]  best epoch: 2  best test acc: 0.5340\n",
            "Correct: 250 Total Sample: 500\n",
            "epoch: 3/50  current:[train loss: 0.0309 test loss:0.0239 test acc: 0.5000]  best epoch: 2  best test acc: 0.5340\n",
            "Correct: 263 Total Sample: 500\n",
            "epoch: 4/50  current:[train loss: 0.0302 test loss:0.0196 test acc: 0.5260]  best epoch: 2  best test acc: 0.5340\n",
            "Correct: 258 Total Sample: 500\n",
            "epoch: 5/50  current:[train loss: 0.0291 test loss:0.0209 test acc: 0.5160]  best epoch: 2  best test acc: 0.5340\n",
            "Correct: 261 Total Sample: 500\n",
            "epoch: 6/50  current:[train loss: 0.0283 test loss:0.0219 test acc: 0.5220]  best epoch: 2  best test acc: 0.5340\n",
            "Correct: 250 Total Sample: 500\n",
            "epoch: 7/50  current:[train loss: 0.0268 test loss:0.0308 test acc: 0.5000]  best epoch: 2  best test acc: 0.5340\n",
            "Correct: 255 Total Sample: 500\n",
            "epoch: 8/50  current:[train loss: 0.0256 test loss:0.0295 test acc: 0.5100]  best epoch: 2  best test acc: 0.5340\n",
            "Correct: 252 Total Sample: 500\n",
            "epoch: 9/50  current:[train loss: 0.0240 test loss:0.0317 test acc: 0.5040]  best epoch: 2  best test acc: 0.5340\n",
            "Correct: 263 Total Sample: 500\n",
            "epoch: 10/50  current:[train loss: 0.0228 test loss:0.0289 test acc: 0.5260]  best epoch: 2  best test acc: 0.5340\n",
            "Correct: 257 Total Sample: 500\n",
            "epoch: 11/50  current:[train loss: 0.0211 test loss:0.0351 test acc: 0.5140]  best epoch: 2  best test acc: 0.5340\n",
            "Correct: 252 Total Sample: 500\n",
            "epoch: 12/50  current:[train loss: 0.0197 test loss:0.0451 test acc: 0.5040]  best epoch: 2  best test acc: 0.5340\n",
            "Correct: 257 Total Sample: 500\n",
            "epoch: 13/50  current:[train loss: 0.0187 test loss:0.0407 test acc: 0.5140]  best epoch: 2  best test acc: 0.5340\n",
            "Correct: 254 Total Sample: 500\n",
            "epoch: 14/50  current:[train loss: 0.0173 test loss:0.0435 test acc: 0.5080]  best epoch: 2  best test acc: 0.5340\n",
            "Correct: 264 Total Sample: 500\n",
            "epoch: 15/50  current:[train loss: 0.0152 test loss:0.0457 test acc: 0.5280]  best epoch: 2  best test acc: 0.5340\n",
            "Correct: 262 Total Sample: 500\n",
            "epoch: 16/50  current:[train loss: 0.0148 test loss:0.0402 test acc: 0.5240]  best epoch: 2  best test acc: 0.5340\n",
            "Correct: 260 Total Sample: 500\n",
            "epoch: 17/50  current:[train loss: 0.0132 test loss:0.0502 test acc: 0.5200]  best epoch: 2  best test acc: 0.5340\n",
            "Correct: 262 Total Sample: 500\n",
            "epoch: 18/50  current:[train loss: 0.0124 test loss:0.0482 test acc: 0.5240]  best epoch: 2  best test acc: 0.5340\n",
            "Correct: 259 Total Sample: 500\n",
            "epoch: 19/50  current:[train loss: 0.0114 test loss:0.0548 test acc: 0.5180]  best epoch: 2  best test acc: 0.5340\n",
            "Correct: 256 Total Sample: 500\n",
            "epoch: 20/50  current:[train loss: 0.0102 test loss:0.0471 test acc: 0.5120]  best epoch: 2  best test acc: 0.5340\n",
            "Correct: 262 Total Sample: 500\n",
            "epoch: 21/50  current:[train loss: 0.0097 test loss:0.0543 test acc: 0.5240]  best epoch: 2  best test acc: 0.5340\n",
            "Correct: 263 Total Sample: 500\n",
            "epoch: 22/50  current:[train loss: 0.0091 test loss:0.0634 test acc: 0.5260]  best epoch: 2  best test acc: 0.5340\n",
            "Correct: 260 Total Sample: 500\n",
            "epoch: 23/50  current:[train loss: 0.0080 test loss:0.0625 test acc: 0.5200]  best epoch: 2  best test acc: 0.5340\n",
            "Correct: 260 Total Sample: 500\n",
            "epoch: 24/50  current:[train loss: 0.0076 test loss:0.0626 test acc: 0.5200]  best epoch: 2  best test acc: 0.5340\n",
            "Correct: 262 Total Sample: 500\n",
            "epoch: 25/50  current:[train loss: 0.0071 test loss:0.0630 test acc: 0.5240]  best epoch: 2  best test acc: 0.5340\n",
            "Correct: 263 Total Sample: 500\n",
            "epoch: 26/50  current:[train loss: 0.0063 test loss:0.0704 test acc: 0.5260]  best epoch: 2  best test acc: 0.5340\n",
            "Correct: 260 Total Sample: 500\n",
            "epoch: 27/50  current:[train loss: 0.0059 test loss:0.0690 test acc: 0.5200]  best epoch: 2  best test acc: 0.5340\n",
            "Correct: 270 Total Sample: 500\n",
            "epoch: 28/50  current:[train loss: 0.0052 test loss:0.0616 test acc: 0.5400]  best epoch: 28  best test acc: 0.5400\n",
            "Correct: 267 Total Sample: 500\n",
            "epoch: 29/50  current:[train loss: 0.0050 test loss:0.0699 test acc: 0.5340]  best epoch: 28  best test acc: 0.5400\n",
            "Correct: 266 Total Sample: 500\n",
            "epoch: 30/50  current:[train loss: 0.0048 test loss:0.0709 test acc: 0.5320]  best epoch: 28  best test acc: 0.5400\n",
            "Correct: 271 Total Sample: 500\n",
            "epoch: 31/50  current:[train loss: 0.0043 test loss:0.0694 test acc: 0.5420]  best epoch: 31  best test acc: 0.5420\n",
            "Correct: 271 Total Sample: 500\n",
            "epoch: 32/50  current:[train loss: 0.0041 test loss:0.0727 test acc: 0.5420]  best epoch: 31  best test acc: 0.5420\n",
            "Correct: 268 Total Sample: 500\n",
            "epoch: 33/50  current:[train loss: 0.0039 test loss:0.0821 test acc: 0.5360]  best epoch: 31  best test acc: 0.5420\n",
            "Correct: 273 Total Sample: 500\n",
            "epoch: 34/50  current:[train loss: 0.0036 test loss:0.0628 test acc: 0.5460]  best epoch: 34  best test acc: 0.5460\n",
            "Correct: 267 Total Sample: 500\n",
            "epoch: 35/50  current:[train loss: 0.0033 test loss:0.0811 test acc: 0.5340]  best epoch: 34  best test acc: 0.5460\n",
            "Correct: 266 Total Sample: 500\n",
            "epoch: 36/50  current:[train loss: 0.0032 test loss:0.0863 test acc: 0.5320]  best epoch: 34  best test acc: 0.5460\n",
            "Correct: 272 Total Sample: 500\n",
            "epoch: 37/50  current:[train loss: 0.0031 test loss:0.0820 test acc: 0.5440]  best epoch: 34  best test acc: 0.5460\n",
            "Correct: 272 Total Sample: 500\n",
            "epoch: 38/50  current:[train loss: 0.0027 test loss:0.0744 test acc: 0.5440]  best epoch: 34  best test acc: 0.5460\n",
            "Correct: 270 Total Sample: 500\n",
            "epoch: 39/50  current:[train loss: 0.0023 test loss:0.0836 test acc: 0.5400]  best epoch: 34  best test acc: 0.5460\n",
            "Correct: 269 Total Sample: 500\n",
            "epoch: 40/50  current:[train loss: 0.0022 test loss:0.0868 test acc: 0.5380]  best epoch: 34  best test acc: 0.5460\n",
            "Correct: 272 Total Sample: 500\n",
            "epoch: 41/50  current:[train loss: 0.0023 test loss:0.0818 test acc: 0.5440]  best epoch: 34  best test acc: 0.5460\n",
            "Correct: 264 Total Sample: 500\n",
            "epoch: 42/50  current:[train loss: 0.0023 test loss:0.0900 test acc: 0.5280]  best epoch: 34  best test acc: 0.5460\n",
            "Correct: 263 Total Sample: 500\n",
            "epoch: 43/50  current:[train loss: 0.0022 test loss:0.1023 test acc: 0.5260]  best epoch: 34  best test acc: 0.5460\n",
            "Correct: 265 Total Sample: 500\n",
            "epoch: 44/50  current:[train loss: 0.0020 test loss:0.0906 test acc: 0.5300]  best epoch: 34  best test acc: 0.5460\n",
            "Correct: 270 Total Sample: 500\n",
            "epoch: 45/50  current:[train loss: 0.0018 test loss:0.1003 test acc: 0.5400]  best epoch: 34  best test acc: 0.5460\n",
            "Correct: 271 Total Sample: 500\n",
            "epoch: 46/50  current:[train loss: 0.0016 test loss:0.1032 test acc: 0.5420]  best epoch: 34  best test acc: 0.5460\n",
            "Correct: 267 Total Sample: 500\n",
            "epoch: 47/50  current:[train loss: 0.0014 test loss:0.0988 test acc: 0.5340]  best epoch: 34  best test acc: 0.5460\n",
            "Correct: 270 Total Sample: 500\n",
            "epoch: 48/50  current:[train loss: 0.0014 test loss:0.0969 test acc: 0.5400]  best epoch: 34  best test acc: 0.5460\n",
            "Correct: 271 Total Sample: 500\n",
            "epoch: 49/50  current:[train loss: 0.0015 test loss:0.1021 test acc: 0.5420]  best epoch: 34  best test acc: 0.5460\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p-IqOH9Ph7Db"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}