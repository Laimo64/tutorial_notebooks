{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformers_All_VQA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ed3b5ccbc2e241a19839c7723de64861": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f1513d34784f44f0adadd538df911353",
              "IPY_MODEL_a41b9802caca40cfa535291674b55fb9",
              "IPY_MODEL_c377127209a7456bb4648e2c2ac11c45"
            ],
            "layout": "IPY_MODEL_6d23a8d7cac24fbd8a5f8be9656e1536"
          }
        },
        "f1513d34784f44f0adadd538df911353": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8e40d4aee25498fbbf1c6163ed57af4",
            "placeholder": "​",
            "style": "IPY_MODEL_3bed478bdb65400482b7aab2337ccb46",
            "value": "100%"
          }
        },
        "a41b9802caca40cfa535291674b55fb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd9fa404f6db42aab37725f49cbc70bf",
            "max": 46830571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f442692d0f034d13a37f3ebc0da31798",
            "value": 46830571
          }
        },
        "c377127209a7456bb4648e2c2ac11c45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f77bbb33d1c429e91fd785a737276cd",
            "placeholder": "​",
            "style": "IPY_MODEL_698c8ed936154aa18b6c543707014b42",
            "value": " 44.7M/44.7M [00:00&lt;00:00, 70.5MB/s]"
          }
        },
        "6d23a8d7cac24fbd8a5f8be9656e1536": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8e40d4aee25498fbbf1c6163ed57af4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3bed478bdb65400482b7aab2337ccb46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cd9fa404f6db42aab37725f49cbc70bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f442692d0f034d13a37f3ebc0da31798": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1f77bbb33d1c429e91fd785a737276cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "698c8ed936154aa18b6c543707014b42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/tutorial_notebooks/blob/main/Transformers_All_VQA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HnAbtUBvpHZk",
        "outputId": "1f2487f3-a9c5-4a79-e888-16b438c9d478",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 4.7 MB 30.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 120 kB 58.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 50.6 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers\n",
        "!pip -q install timm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from transformers import VisualBertModel, VisualBertConfig, BertTokenizerFast\n",
        "from PIL import Image\n",
        "import requests\n",
        "from torchvision.models import resnet18, resnet34, resnet101\n",
        "from torchvision import transforms\n",
        "from timm import create_model\n",
        "\n",
        "img_url = 'https://www.animalfunfacts.net/images/stories/pets/dogs/pembroke_welsh_corgi_l.jpg'\n",
        "img_raw = Image.open(requests.get(img_url, stream=True).raw)\n",
        "mean, std = torch.tensor([0.485, 0.456, 0.406]), torch.tensor([0.229, 0.224, 0.225])\n",
        "transform = transforms.Compose([transforms.Resize((224, 224)), \n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize(mean=mean, std=std)])\n",
        "img = transform(img_raw)[None]\n",
        "\n",
        "test_question = [\"Where is the dog?\"]\n",
        "bert_tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
        "inputs = bert_tokenizer(test_question, return_tensors=\"pt\", padding=\"max_length\",max_length=20,)\n"
      ],
      "metadata": {
        "id": "Fug7RdPopWRC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "VisualBERT (ResNet101)"
      ],
      "metadata": {
        "id": "HfDvrDa47r2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VisualBERT_VQA(nn.Module):\n",
        "    def __init__(self, num_labels=2):\n",
        "        super(VisualBERT_VQA, self).__init__()\n",
        "        self.visualbert = VisualBertModel.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n",
        "        self.cls = nn.Linear(768, num_labels)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        last_hidden_state = self.visualbert(**inputs).last_hidden_state #[1, 56, 768]\n",
        "\n",
        "        # Get the index of the last text token\n",
        "        index_to_gather = inputs['attention_mask'].sum(1) - 2  # as in original code 5\n",
        "        index_to_gather = (\n",
        "            index_to_gather.unsqueeze(-1).unsqueeze(-1).expand(index_to_gather.size(0), 1, last_hidden_state.size(-1))\n",
        "        ) # [b c hw]=[1, 1, 768]\n",
        "\n",
        "        pooled_output = torch.gather(last_hidden_state, 1, index_to_gather) # [1, 1, 768]\n",
        "        logits = self.cls(pooled_output).squeeze(1)\n",
        "        return logits\n",
        "\n",
        "model_visual_feat = resnet101(pretrained=True)\n",
        "model_visual_feat.avgpool = nn.Identity()\n",
        "model_visual_feat.fc = nn.Identity()\n",
        "model_visual_feat.eval()\n",
        "visual_embeds = model_visual_feat(img).view(-1, 49, 2048)\n",
        "visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long)\n",
        "visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float)\n",
        "\n",
        "inputs.update(\n",
        "    {\n",
        "        \"visual_embeds\": visual_embeds,\n",
        "        \"visual_token_type_ids\": visual_token_type_ids,\n",
        "        \"visual_attention_mask\": visual_attention_mask,\n",
        "    }\n",
        ")\n",
        "\n",
        "print('visual_embeds', visual_embeds.shape, 'Text:', inputs['input_ids'].shape)\n",
        "model = VisualBERT_VQA()\n",
        "model.eval()\n",
        "logits = model(inputs)\n",
        "pred_vqa = logits.argmax(-1)\n",
        "print('Logits:',logits, 'Prediction:', pred_vqa)  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofC-BJx421in",
        "outputId": "dbfbacc8-d8c3-4654-8891-0e1f341bde5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "visual_embeds torch.Size([1, 49, 2048]) Text: torch.Size([1, 20])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at uclanlp/visualbert-vqa-coco-pre were not used when initializing VisualBertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing VisualBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing VisualBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits: tensor([[-0.4455,  0.3589]], grad_fn=<SqueezeBackward1>) Prediction: tensor([1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "VisualBERT (ResNet34)"
      ],
      "metadata": {
        "id": "0ZWZfrMd7y9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VisualBERT_VQA(nn.Module):\n",
        "    def __init__(self, num_labels=2):\n",
        "        super(VisualBERT_VQA, self).__init__()\n",
        "        self.config = VisualBertConfig.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n",
        "        self.config.visual_embedding_dim = 512\n",
        "        self.visualbert = VisualBertModel(config=self.config)#.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n",
        "        #self.embeddings = self.visual_bert.embeddings\n",
        "        self.cls = nn.Linear(768, num_labels)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        last_hidden_state = self.visualbert(**inputs).last_hidden_state #[1, 56, 768]\n",
        "\n",
        "        # Get the index of the last text token\n",
        "        index_to_gather = inputs['attention_mask'].sum(1) - 2  # as in original code 5\n",
        "        index_to_gather = (\n",
        "            index_to_gather.unsqueeze(-1).unsqueeze(-1).expand(index_to_gather.size(0), 1, last_hidden_state.size(-1))\n",
        "        ) # [b c hw]=[1, 1, 768]\n",
        "        pooled_output = torch.gather(last_hidden_state, 1, index_to_gather) # [1, 1, 768]\n",
        "        logits = self.cls(pooled_output).squeeze(1)\n",
        "        return logits\n",
        "\n",
        "model_visual_feat = resnet34(pretrained=True)\n",
        "model_visual_feat.avgpool = nn.Identity()\n",
        "model_visual_feat.fc = nn.Identity()\n",
        "model_visual_feat.eval()\n",
        "visual_embeds = model_visual_feat(img).view(-1, 49, 512)\n",
        "visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long)\n",
        "visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float)\n",
        "inputs.update(\n",
        "    {\n",
        "        \"visual_embeds\": visual_embeds,\n",
        "        \"visual_token_type_ids\": visual_token_type_ids,\n",
        "        \"visual_attention_mask\": visual_attention_mask,\n",
        "    }\n",
        ")\n",
        "\n",
        "print('visual_embeds', visual_embeds.shape, 'Text:', inputs['input_ids'].shape)\n",
        "\n",
        "model = VisualBERT_VQA()\n",
        "model.eval()\n",
        "logits = model(inputs)\n",
        "pred_vqa = logits.argmax(-1)\n",
        "print('Logits:',logits, 'Prediction:', pred_vqa)        \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZvDd2GcsaF1",
        "outputId": "fa561909-0011-4cf3-fc0f-c1e2474ac606"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "visual_embeds torch.Size([1, 49, 512]) Text: torch.Size([1, 20])\n",
            "self.visualbert.config.visual_embedding_dim: 512\n",
            "tensor([5]) 1 768\n",
            "torch.Size([1, 1, 768])\n",
            "Logits: tensor([[ 0.5161, -0.5943]], grad_fn=<SqueezeBackward1>) Prediction: tensor([0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ViT_VQA(ResNet18)\n",
        "\n",
        "(BertTokenizerFast = AutoTokenizer)"
      ],
      "metadata": {
        "id": "L_t1Wgkr8CXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from timm import create_model\n",
        "class ViT_VQA(nn.Module):\n",
        "    def __init__(self, num_labels=2):\n",
        "        super(ViT_VQA, self).__init__()\n",
        "        self.config = VisualBertConfig.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n",
        "        self.config.visual_embedding_dim = 512\n",
        "        self.visualbert = VisualBertModel(config=self.config)\n",
        "        self.embeddings = self.visualbert.embeddings\n",
        "\n",
        "        self.vit = create_model(\"vit_base_patch16_224\", pretrained=True)\n",
        "        self.cls = nn.Linear(768, num_labels)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embedding_output = self.embeddings(\n",
        "            input_ids=inputs['input_ids'],\n",
        "            token_type_ids=inputs['token_type_ids'],\n",
        "            position_ids=None,\n",
        "            inputs_embeds=None,\n",
        "            visual_embeds=inputs['visual_embeds'],\n",
        "            visual_token_type_ids=inputs['visual_token_type_ids'],\n",
        "            image_text_alignment=None,\n",
        "        ) #[1, 56, 768]\n",
        "        x = self.vit.blocks(embedding_output)\n",
        "        x = self.vit.norm(x)\n",
        "        x = x.mean(dim=1)\n",
        "        logits = self.cls(x)\n",
        "        return logits\n",
        "\n",
        "model_visual_feat = resnet18(pretrained=True)\n",
        "model_visual_feat.avgpool = nn.Identity()\n",
        "model_visual_feat.fc = nn.Identity()\n",
        "model_visual_feat.eval()\n",
        "visual_embeds = model_visual_feat(img).view(-1, 49, 512)\n",
        "visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long)\n",
        "visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float)\n",
        "inputs.update(\n",
        "    {\n",
        "        \"visual_embeds\": visual_embeds,\n",
        "        \"visual_token_type_ids\": visual_token_type_ids,\n",
        "        \"visual_attention_mask\": visual_attention_mask,\n",
        "    }\n",
        ")\n",
        "\n",
        "print('visual_embeds', visual_embeds.shape, 'Text:', inputs['input_ids'].shape)\n",
        "\n",
        "model = ViT_VQA(num_labels=2)\n",
        "model.eval()\n",
        "logits = model(inputs)\n",
        "pred_vqa = logits.argmax(-1)\n",
        "print('Logits:',logits, 'Prediction:', pred_vqa) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191,
          "referenced_widgets": [
            "ed3b5ccbc2e241a19839c7723de64861",
            "f1513d34784f44f0adadd538df911353",
            "a41b9802caca40cfa535291674b55fb9",
            "c377127209a7456bb4648e2c2ac11c45",
            "6d23a8d7cac24fbd8a5f8be9656e1536",
            "e8e40d4aee25498fbbf1c6163ed57af4",
            "3bed478bdb65400482b7aab2337ccb46",
            "cd9fa404f6db42aab37725f49cbc70bf",
            "f442692d0f034d13a37f3ebc0da31798",
            "1f77bbb33d1c429e91fd785a737276cd",
            "698c8ed936154aa18b6c543707014b42"
          ]
        },
        "id": "0z5BDwfv-hy8",
        "outputId": "07285f11-5867-4827-c5db-526fcdc1cfc7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/44.7M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ed3b5ccbc2e241a19839c7723de64861"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "visual_embeds torch.Size([1, 49, 512]) Text: torch.Size([1, 20])\n",
            "Logits: tensor([[-1.1709, -0.9624]], grad_fn=<AddmmBackward0>) Prediction: tensor([1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Swin-Transformer_VQA(ResNet34)"
      ],
      "metadata": {
        "id": "WWn9EtjVJ2yd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SwinTranformer_VQA(nn.Module):\n",
        "    def __init__(self, num_labels=2):\n",
        "        super(SwinTranformer_VQA, self).__init__()\n",
        "        self.config = VisualBertConfig.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n",
        "        self.config.visual_embedding_dim = 512\n",
        "        self.visualbert = VisualBertModel(config=self.config)\n",
        "        self.embeddings = self.visualbert.embeddings\n",
        "\n",
        "        self.swintran = create_model(\"swin_base_patch4_window7_224\", pretrained=True)\n",
        "        self.cls = nn.Linear(768, num_labels)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embedding_output = self.embeddings(\n",
        "            input_ids=inputs['input_ids'],\n",
        "            token_type_ids=inputs['token_type_ids'],\n",
        "            position_ids=None,\n",
        "            inputs_embeds=None,\n",
        "            visual_embeds=inputs['visual_embeds'],\n",
        "            visual_token_type_ids=inputs['visual_token_type_ids'],\n",
        "            image_text_alignment=None,\n",
        "        ) #[1, 56, 768]\n",
        "        #x = self.swintran.patch_embed(x)\n",
        "        x = self.swintran.layers(embedding_output)\n",
        "        x = self.swintran.norm(x)\n",
        "        x = x.mean(dim=1)\n",
        "        logits = self.cls(x)\n",
        "        return logits\n",
        "\n",
        "model_visual_feat = resnet34(pretrained=True)\n",
        "model_visual_feat.avgpool = nn.Identity()\n",
        "model_visual_feat.fc = nn.Identity()\n",
        "model_visual_feat.eval()\n",
        "visual_embeds = model_visual_feat(img).view(-1, 49, 512)\n",
        "visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long)\n",
        "visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float)\n",
        "inputs.update(\n",
        "    {\n",
        "        \"visual_embeds\": visual_embeds,\n",
        "        \"visual_token_type_ids\": visual_token_type_ids,\n",
        "        \"visual_attention_mask\": visual_attention_mask,\n",
        "    }\n",
        ")\n",
        "\n",
        "print('visual_embeds', visual_embeds.shape, 'Text:', inputs['input_ids'].shape)\n",
        "\n",
        "model = SwinTranformer_VQA(num_labels=2)\n",
        "model.eval()\n",
        "logits = model(inputs)\n",
        "pred_vqa = logits.argmax(-1)\n",
        "print('Logits:',logits, 'Prediction:', pred_vqa) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "t_XpabovKIHM",
        "outputId": "57c7222d-ce54-4658-a6ac-0c553a7a6464"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "visual_embeds torch.Size([1, 49, 512]) Text: torch.Size([1, 20])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-b7a1b01daa1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSwinTranformer_VQA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0mpred_vqa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Logits:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Prediction:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_vqa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-45-b7a1b01daa1b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     21\u001b[0m         ) #[1, 56, 768]\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m#x = self.swintran.patch_embed(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswintran\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswintran\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/timm/models/swin_transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/timm/models/swin_transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_resolution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m         \u001b[0m_assert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mH\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"input feature has wrong size\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mshortcut\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m_assert\u001b[0;34m(condition, message)\u001b[0m\n\u001b[1;32m    831\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_assert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m \u001b[0;31m################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: input feature has wrong size"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Tokenizer (Text)"
      ],
      "metadata": {
        "id": "3bakV9B0I9yq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\" #\"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "test_question = [\"We are very happy to show you the Transformers library\"]\n",
        "inputs_a = tokenizer(\n",
        "    test_question,  truncation=True, return_tensors=\"pt\", padding=\"max_length\", max_length=20,\n",
        ")\n",
        "\n",
        "bert_tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
        "inputs = bert_tokenizer(test_question, truncation=True, return_tensors=\"pt\", padding=\"max_length\", max_length=20,)\n",
        "inputs_a['input_ids'], inputs['input_ids']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBkb7EC4EyGJ",
        "outputId": "d415a8fe-5ab3-4c08-b70f-97e61ac035b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[  101,  2057,  2024,  2200,  3407,  2000,  2265,  2017,  1996, 19081,\n",
              "           3075,   102,     0,     0,     0,     0,     0,     0,     0,     0]]),\n",
              " tensor([[  101,  2057,  2024,  2200,  3407,  2000,  2265,  2017,  1996, 19081,\n",
              "           3075,   102,     0,     0,     0,     0,     0,     0,     0,     0]]))"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "from timm import create_model\n",
        "\n",
        "\n",
        "class SwinTranformer_Features(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SwinTranformer_Features, self).__init__()\n",
        "        self.swintran = create_model(\"swin_base_patch4_window7_224\", pretrained=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.swintran.patch_embed(x) # [1, 3136, 128]\n",
        "        print(x.shape)\n",
        "        x = self.swintran.layers(x)#[1, 49, 1024]\n",
        "        print(x.shape)\n",
        "        x = self.swintran.norm(x)#[1, 49, 1024]\n",
        "        print(x.shape)\n",
        "        x = x.mean(dim=1)#[1, 1024]\n",
        "        print(x.shape)\n",
        "        logits = self.swintran.head(x)#\n",
        "        return logits\n",
        "\n",
        "model = SwinTranformer_Features()\n",
        "model.eval()\n",
        "logits = model(img)\n",
        "pred = logits.argmax(dim=1).item()\n",
        "print('prediction:', int(torch.argmax(logits)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DEK1j9ZKYT4",
        "outputId": "0161f2a8-f977-47b9-8664-8accbfc6eb66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "Downloading: \"https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_base_patch4_window7_224_22kto1k.pth\" to /root/.cache/torch/hub/checkpoints/swin_base_patch4_window7_224_22kto1k.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 3136, 128])\n",
            "torch.Size([1, 49, 1024])\n",
            "torch.Size([1, 49, 1024])\n",
            "torch.Size([1, 1024])\n",
            "prediction: 263\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoFeatureExtractor, SwinForImageClassification\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\")\n",
        "model = SwinForImageClassification.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\")\n",
        "model.eval()\n",
        "img2 = feature_extractor(img[0], return_tensors=\"pt\")\n",
        "print(img2['pixel_values'].shape)\n",
        "logits = model(**img2).logits\n",
        "pred = logits.argmax(dim=1).item()\n",
        "print('prediction:', int(torch.argmax(logits)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRndShu2Mxdy",
        "outputId": "fc0320d2-a9a8-4107-9a2f-0ff26f8bd3fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 3, 224, 224])\n",
            "prediction: 264\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "from timm import create_model\n",
        "\n",
        "\n",
        "class ViT_Features(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ViT_Features, self).__init__()\n",
        "        model_name = \"vit_base_patch16_224\"\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.vit = create_model(model_name, pretrained=True).to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        patches = self.vit.patch_embed(x) # [1, 196, 768]\n",
        "        pos_embed = self.vit.pos_embed # [1, 197, 768]\n",
        "        x = torch.cat((self.vit.cls_token, patches), dim=1) + pos_embed #[1, 197, 768]\n",
        "        x = self.vit.blocks(x)\n",
        "        # for i, blk in enumerate(self.vit.blocks):\n",
        "        #     x = blk(x)\n",
        "        x = self.vit.norm(x)\n",
        "        x = x.mean(dim=1)\n",
        "        logits = self.vit.head(x)\n",
        "        return logits\n",
        "\n",
        "model = ViT_Features()\n",
        "model.eval()\n",
        "logits = model(img)\n",
        "pred = logits.argmax(dim=1).item()\n",
        "print('prediction:', int(torch.argmax(logits)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pP1ww_-p8_0Z",
        "outputId": "4d9e39e8-8c1f-4a9b-e7ba-eef98635e06d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "patches torch.Size([1, 196, 768])\n",
            "pos_embed torch.Size([1, 197, 768])\n",
            "prediction: 263\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JOYHhMJv9O_l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}