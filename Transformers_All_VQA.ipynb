{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformers_All_VQA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyObTi0r8AaydAi/YZkcsXwJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/tutorial_notebooks/blob/main/Transformers_All_VQA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HnAbtUBvpHZk"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from transformers import VisualBertModel, VisualBertConfig, BertTokenizerFast\n",
        "from PIL import Image\n",
        "import requests\n",
        "from torchvision.models import resnet34, resnet101\n",
        "from torchvision import transforms\n",
        "\n",
        "img_url = 'https://www.animalfunfacts.net/images/stories/pets/dogs/pembroke_welsh_corgi_l.jpg'\n",
        "img_raw = Image.open(requests.get(img_url, stream=True).raw)\n",
        "mean, std = torch.tensor([0.485, 0.456, 0.406]), torch.tensor([0.229, 0.224, 0.225])\n",
        "transform = transforms.Compose([transforms.Resize((224, 224)), \n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize(mean=mean, std=std)])\n",
        "img = transform(img_raw)[None]\n",
        "\n",
        "test_question = [\"Where is the dog?\"]\n",
        "bert_tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
        "inputs = bert_tokenizer(test_question, return_tensors=\"pt\", padding=\"max_length\",max_length=20,)\n"
      ],
      "metadata": {
        "id": "Fug7RdPopWRC"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "VisualBERT (ResNet101)"
      ],
      "metadata": {
        "id": "HfDvrDa47r2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VisualBERT_VQA(nn.Module):\n",
        "    def __init__(self, num_labels=2):\n",
        "        super(VisualBERT_VQA, self).__init__()\n",
        "        self.visualbert = VisualBertModel.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n",
        "        self.cls = nn.Linear(768, num_labels)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        last_hidden_state = self.visualbert(**inputs).last_hidden_state #[1, 56, 768]\n",
        "\n",
        "        # Get the index of the last text token\n",
        "        index_to_gather = inputs['attention_mask'].sum(1) - 2  # as in original code 5\n",
        "        index_to_gather = (\n",
        "            index_to_gather.unsqueeze(-1).unsqueeze(-1).expand(index_to_gather.size(0), 1, last_hidden_state.size(-1))\n",
        "        ) # [b c hw]=[1, 1, 768]\n",
        "\n",
        "        pooled_output = torch.gather(last_hidden_state, 1, index_to_gather) # [1, 1, 768]\n",
        "        logits = self.cls(pooled_output).squeeze(1)\n",
        "        return logits\n",
        "\n",
        "model_visual_feat = resnet101(pretrained=True)\n",
        "model_visual_feat.avgpool = nn.Identity()\n",
        "model_visual_feat.fc = nn.Identity()\n",
        "model_visual_feat.eval()\n",
        "visual_embeds = model_visual_feat(img).view(-1, 49, 2048)\n",
        "visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long)\n",
        "visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float)\n",
        "\n",
        "inputs.update(\n",
        "    {\n",
        "        \"visual_embeds\": visual_embeds,\n",
        "        \"visual_token_type_ids\": visual_token_type_ids,\n",
        "        \"visual_attention_mask\": visual_attention_mask,\n",
        "    }\n",
        ")\n",
        "\n",
        "print('visual_embeds', visual_embeds.shape, 'Text:', inputs['input_ids'].shape)\n",
        "model = VisualBERT_VQA()\n",
        "model.eval()\n",
        "logits = model(inputs)\n",
        "pred_vqa = logits.argmax(-1)\n",
        "print('Logits:',logits, 'Prediction:', pred_vqa)  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofC-BJx421in",
        "outputId": "dbfbacc8-d8c3-4654-8891-0e1f341bde5d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "visual_embeds torch.Size([1, 49, 2048]) Text: torch.Size([1, 20])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at uclanlp/visualbert-vqa-coco-pre were not used when initializing VisualBertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing VisualBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing VisualBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits: tensor([[-0.4455,  0.3589]], grad_fn=<SqueezeBackward1>) Prediction: tensor([1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "VisualBERT (ResNet34)"
      ],
      "metadata": {
        "id": "0ZWZfrMd7y9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VisualBERT_VQA(nn.Module):\n",
        "    def __init__(self, num_labels=2):\n",
        "        super(VisualBERT_VQA, self).__init__()\n",
        "        self.config = VisualBertConfig.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n",
        "        self.config.visual_embedding_dim = 512\n",
        "        self.visualbert = VisualBertModel(config=self.config)#.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n",
        "        #self.embeddings = self.visual_bert.embeddings\n",
        "        self.cls = nn.Linear(768, num_labels)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        last_hidden_state = self.visualbert(**inputs).last_hidden_state #[1, 56, 768]\n",
        "\n",
        "        # Get the index of the last text token\n",
        "        index_to_gather = inputs['attention_mask'].sum(1) - 2  # as in original code 5\n",
        "        index_to_gather = (\n",
        "            index_to_gather.unsqueeze(-1).unsqueeze(-1).expand(index_to_gather.size(0), 1, last_hidden_state.size(-1))\n",
        "        ) # [b c hw]=[1, 1, 768]\n",
        "        pooled_output = torch.gather(last_hidden_state, 1, index_to_gather) # [1, 1, 768]\n",
        "        logits = self.cls(pooled_output).squeeze(1)\n",
        "        return logits\n",
        "\n",
        "model_visual_feat = resnet34(pretrained=True)\n",
        "model_visual_feat.avgpool = nn.Identity()\n",
        "model_visual_feat.fc = nn.Identity()\n",
        "model_visual_feat.eval()\n",
        "visual_embeds = model_visual_feat(img).view(-1, 49, 512)\n",
        "visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long)\n",
        "visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float)\n",
        "inputs.update(\n",
        "    {\n",
        "        \"visual_embeds\": visual_embeds,\n",
        "        \"visual_token_type_ids\": visual_token_type_ids,\n",
        "        \"visual_attention_mask\": visual_attention_mask,\n",
        "    }\n",
        ")\n",
        "\n",
        "print('visual_embeds', visual_embeds.shape, 'Text:', inputs['input_ids'].shape)\n",
        "\n",
        "model = VisualBERT_VQA()\n",
        "model.eval()\n",
        "logits = model(inputs)\n",
        "pred_vqa = logits.argmax(-1)\n",
        "print('Logits:',logits, 'Prediction:', pred_vqa)        \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZvDd2GcsaF1",
        "outputId": "fa561909-0011-4cf3-fc0f-c1e2474ac606"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "visual_embeds torch.Size([1, 49, 512]) Text: torch.Size([1, 20])\n",
            "self.visualbert.config.visual_embedding_dim: 512\n",
            "tensor([5]) 1 768\n",
            "torch.Size([1, 1, 768])\n",
            "Logits: tensor([[ 0.5161, -0.5943]], grad_fn=<SqueezeBackward1>) Prediction: tensor([0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ViT_VQA(ResNet34)\n",
        "\n",
        "(BertTokenizerFast = AutoTokenizer)"
      ],
      "metadata": {
        "id": "L_t1Wgkr8CXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install timm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EL7Wukx99F20",
        "outputId": "6e8419c1-b1d8-4337-e610-ce8eba8553a1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |▋                               | 10 kB 40.4 MB/s eta 0:00:01\r\u001b[K     |█▎                              | 20 kB 51.2 MB/s eta 0:00:01\r\u001b[K     |██                              | 30 kB 58.7 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 40 kB 34.3 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 51 kB 39.0 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 61 kB 44.2 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 71 kB 33.7 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 81 kB 35.0 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 92 kB 37.8 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 102 kB 37.2 MB/s eta 0:00:01\r\u001b[K     |███████                         | 112 kB 37.2 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 122 kB 37.2 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 133 kB 37.2 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 143 kB 37.2 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 153 kB 37.2 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 163 kB 37.2 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 174 kB 37.2 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 184 kB 37.2 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 194 kB 37.2 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 204 kB 37.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 215 kB 37.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 225 kB 37.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 235 kB 37.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 245 kB 37.2 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 256 kB 37.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 266 kB 37.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 276 kB 37.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 286 kB 37.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 296 kB 37.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 307 kB 37.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 317 kB 37.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 327 kB 37.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 337 kB 37.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 348 kB 37.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 358 kB 37.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 368 kB 37.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 378 kB 37.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 389 kB 37.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 399 kB 37.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 409 kB 37.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 419 kB 37.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 430 kB 37.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 440 kB 37.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 450 kB 37.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 460 kB 37.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 471 kB 37.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 481 kB 37.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 491 kB 37.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 501 kB 37.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 509 kB 37.2 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XbI3bq0LI6c9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ViT_VQA(nn.Module):\n",
        "    def __init__(self, num_labels=2):\n",
        "        super(ViT_VQA, self).__init__()\n",
        "        self.config = VisualBertConfig.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n",
        "        self.config.visual_embedding_dim = 512\n",
        "        self.visualbert = VisualBertModel(config=self.config)\n",
        "        self.embeddings = self.visualbert.embeddings\n",
        "\n",
        "        self.vit = create_model(\"vit_base_patch16_224\", pretrained=True)\n",
        "        self.cls = nn.Linear(768, num_labels)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embedding_output = self.embeddings(\n",
        "            input_ids=inputs['input_ids'],\n",
        "            token_type_ids=inputs['token_type_ids'],\n",
        "            position_ids=None,\n",
        "            inputs_embeds=None,\n",
        "            visual_embeds=inputs['visual_embeds'],\n",
        "            visual_token_type_ids=inputs['visual_token_type_ids'],\n",
        "            image_text_alignment=None,\n",
        "        ) #[1, 56, 768]\n",
        "        x = self.vit.blocks(embedding_output)\n",
        "        x = self.vit.norm(x)\n",
        "        x = x.mean(dim=1)\n",
        "        logits = self.cls(x)\n",
        "        return logits\n",
        "\n",
        "model_visual_feat = resnet34(pretrained=True)\n",
        "model_visual_feat.avgpool = nn.Identity()\n",
        "model_visual_feat.fc = nn.Identity()\n",
        "model_visual_feat.eval()\n",
        "visual_embeds = model_visual_feat(img).view(-1, 49, 512)\n",
        "visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long)\n",
        "visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float)\n",
        "inputs.update(\n",
        "    {\n",
        "        \"visual_embeds\": visual_embeds,\n",
        "        \"visual_token_type_ids\": visual_token_type_ids,\n",
        "        \"visual_attention_mask\": visual_attention_mask,\n",
        "    }\n",
        ")\n",
        "\n",
        "print('visual_embeds', visual_embeds.shape, 'Text:', inputs['input_ids'].shape)\n",
        "\n",
        "model = ViT_VQA(num_labels=2)\n",
        "model.eval()\n",
        "logits = model(inputs)\n",
        "pred_vqa = logits.argmax(-1)\n",
        "print('Logits:',logits, 'Prediction:', pred_vqa) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0z5BDwfv-hy8",
        "outputId": "772c5c50-b195-4f6d-ad49-0389589b1090"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "visual_embeds torch.Size([1, 49, 512]) Text: torch.Size([1, 20])\n",
            "Logits: tensor([[0.7204, 0.4291]], grad_fn=<AddmmBackward0>) Prediction: tensor([0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Swin-Transformer_VQA(ResNet34)"
      ],
      "metadata": {
        "id": "WWn9EtjVJ2yd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SwinTranformer_VQA(nn.Module):\n",
        "    def __init__(self, num_labels=2):\n",
        "        super(SwinTranformer_VQA, self).__init__()\n",
        "        self.config = VisualBertConfig.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n",
        "        self.config.visual_embedding_dim = 512\n",
        "        self.visualbert = VisualBertModel(config=self.config)\n",
        "        self.embeddings = self.visualbert.embeddings\n",
        "\n",
        "        self.swintran = create_model(\"swin_base_patch4_window7_224\", pretrained=True)\n",
        "        self.cls = nn.Linear(768, num_labels)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embedding_output = self.embeddings(\n",
        "            input_ids=inputs['input_ids'],\n",
        "            token_type_ids=inputs['token_type_ids'],\n",
        "            position_ids=None,\n",
        "            inputs_embeds=None,\n",
        "            visual_embeds=inputs['visual_embeds'],\n",
        "            visual_token_type_ids=inputs['visual_token_type_ids'],\n",
        "            image_text_alignment=None,\n",
        "        ) #[1, 56, 768]\n",
        "        #x = self.swintran.patch_embed(x)\n",
        "        x = self.swintran.layers(embedding_output)\n",
        "        x = self.swintran.norm(x)\n",
        "        x = x.mean(dim=1)\n",
        "        logits = self.cls(x)\n",
        "        return logits\n",
        "\n",
        "model_visual_feat = resnet34(pretrained=True)\n",
        "model_visual_feat.avgpool = nn.Identity()\n",
        "model_visual_feat.fc = nn.Identity()\n",
        "model_visual_feat.eval()\n",
        "visual_embeds = model_visual_feat(img).view(-1, 49, 512)\n",
        "visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long)\n",
        "visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float)\n",
        "inputs.update(\n",
        "    {\n",
        "        \"visual_embeds\": visual_embeds,\n",
        "        \"visual_token_type_ids\": visual_token_type_ids,\n",
        "        \"visual_attention_mask\": visual_attention_mask,\n",
        "    }\n",
        ")\n",
        "\n",
        "print('visual_embeds', visual_embeds.shape, 'Text:', inputs['input_ids'].shape)\n",
        "\n",
        "model = SwinTranformer_VQA(num_labels=2)\n",
        "model.eval()\n",
        "logits = model(inputs)\n",
        "pred_vqa = logits.argmax(-1)\n",
        "print('Logits:',logits, 'Prediction:', pred_vqa) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "t_XpabovKIHM",
        "outputId": "57c7222d-ce54-4658-a6ac-0c553a7a6464"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "visual_embeds torch.Size([1, 49, 512]) Text: torch.Size([1, 20])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-b7a1b01daa1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSwinTranformer_VQA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0mpred_vqa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Logits:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Prediction:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_vqa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-45-b7a1b01daa1b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     21\u001b[0m         ) #[1, 56, 768]\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m#x = self.swintran.patch_embed(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswintran\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswintran\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/timm/models/swin_transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/timm/models/swin_transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_resolution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m         \u001b[0m_assert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mH\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"input feature has wrong size\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mshortcut\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m_assert\u001b[0;34m(condition, message)\u001b[0m\n\u001b[1;32m    831\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_assert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m \u001b[0;31m################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: input feature has wrong size"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Tokenizer (Text)"
      ],
      "metadata": {
        "id": "3bakV9B0I9yq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\" #\"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "test_question = [\"We are very happy to show you the Transformers library\"]\n",
        "inputs_a = tokenizer(\n",
        "    test_question,  truncation=True, return_tensors=\"pt\", padding=\"max_length\", max_length=20,\n",
        ")\n",
        "\n",
        "bert_tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
        "inputs = bert_tokenizer(test_question, truncation=True, return_tensors=\"pt\", padding=\"max_length\", max_length=20,)\n",
        "inputs_a['input_ids'], inputs['input_ids']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBkb7EC4EyGJ",
        "outputId": "d415a8fe-5ab3-4c08-b70f-97e61ac035b7"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[  101,  2057,  2024,  2200,  3407,  2000,  2265,  2017,  1996, 19081,\n",
              "           3075,   102,     0,     0,     0,     0,     0,     0,     0,     0]]),\n",
              " tensor([[  101,  2057,  2024,  2200,  3407,  2000,  2265,  2017,  1996, 19081,\n",
              "           3075,   102,     0,     0,     0,     0,     0,     0,     0,     0]]))"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "from timm import create_model\n",
        "\n",
        "\n",
        "class SwinTranformer_Features(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SwinTranformer_Features, self).__init__()\n",
        "        self.swintran = create_model(\"swin_base_patch4_window7_224\", pretrained=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.swintran.patch_embed(x) # [1, 3136, 128]\n",
        "        print(x.shape)\n",
        "        x = self.swintran.layers(x)#[1, 49, 1024]\n",
        "        print(x.shape)\n",
        "        x = self.swintran.norm(x)#[1, 49, 1024]\n",
        "        print(x.shape)\n",
        "        x = x.mean(dim=1)#[1, 1024]\n",
        "        print(x.shape)\n",
        "        logits = self.swintran.head(x)#\n",
        "        return logits\n",
        "\n",
        "model = SwinTranformer_Features()\n",
        "model.eval()\n",
        "logits = model(img)\n",
        "pred = logits.argmax(dim=1).item()\n",
        "print('prediction:', int(torch.argmax(logits)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DEK1j9ZKYT4",
        "outputId": "0161f2a8-f977-47b9-8664-8accbfc6eb66"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "Downloading: \"https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_base_patch4_window7_224_22kto1k.pth\" to /root/.cache/torch/hub/checkpoints/swin_base_patch4_window7_224_22kto1k.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 3136, 128])\n",
            "torch.Size([1, 49, 1024])\n",
            "torch.Size([1, 49, 1024])\n",
            "torch.Size([1, 1024])\n",
            "prediction: 263\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoFeatureExtractor, SwinForImageClassification\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\")\n",
        "model = SwinForImageClassification.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\")\n",
        "model.eval()\n",
        "img2 = feature_extractor(img[0], return_tensors=\"pt\")\n",
        "print(img2['pixel_values'].shape)\n",
        "logits = model(**img2).logits\n",
        "pred = logits.argmax(dim=1).item()\n",
        "print('prediction:', int(torch.argmax(logits)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRndShu2Mxdy",
        "outputId": "fc0320d2-a9a8-4107-9a2f-0ff26f8bd3fd"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 3, 224, 224])\n",
            "prediction: 264\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "from timm import create_model\n",
        "\n",
        "\n",
        "class ViT_Features(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ViT_Features, self).__init__()\n",
        "        model_name = \"vit_base_patch16_224\"\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.vit = create_model(model_name, pretrained=True).to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        patches = self.vit.patch_embed(x) # [1, 196, 768]\n",
        "        pos_embed = self.vit.pos_embed # [1, 197, 768]\n",
        "        x = torch.cat((self.vit.cls_token, patches), dim=1) + pos_embed #[1, 197, 768]\n",
        "        x = self.vit.blocks(x)\n",
        "        # for i, blk in enumerate(self.vit.blocks):\n",
        "        #     x = blk(x)\n",
        "        x = self.vit.norm(x)\n",
        "        x = x.mean(dim=1)\n",
        "        logits = self.vit.head(x)\n",
        "        return logits\n",
        "\n",
        "model = ViT_Features()\n",
        "model.eval()\n",
        "logits = model(img)\n",
        "pred = logits.argmax(dim=1).item()\n",
        "print('prediction:', int(torch.argmax(logits)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pP1ww_-p8_0Z",
        "outputId": "4d9e39e8-8c1f-4a9b-e7ba-eef98635e06d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "patches torch.Size([1, 196, 768])\n",
            "pos_embed torch.Size([1, 197, 768])\n",
            "prediction: 263\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JOYHhMJv9O_l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}