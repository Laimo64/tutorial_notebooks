{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPBGcpFeCTDy9Ltmmzjb8Rd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/tutorial_notebooks/blob/main/xclip_video_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#X-clip video processing: https://arxiv.org/pdf/2207.07285"
      ],
      "metadata": {
        "id": "ZtTpR2yTvOR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install av"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6igMKhThpF4V",
        "outputId": "0915b892-2c88-417b-a62a-9e96be65a9ca"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.0/33.0 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Extracting Video Features:(8 frames by default)"
      ],
      "metadata": {
        "id": "eA5Kuj9jtvZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import av\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from transformers import AutoProcessor, AutoModel\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "\n",
        "def read_video_pyav(container, indices):\n",
        "    '''\n",
        "    Decode the video with PyAV decoder.\n",
        "    Args:\n",
        "        container (`av.container.input.InputContainer`): PyAV container.\n",
        "        indices (`List[int]`): List of frame indices to decode.\n",
        "    Returns:\n",
        "        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n",
        "    '''\n",
        "    frames = []\n",
        "    container.seek(0)\n",
        "    start_index = indices[0]\n",
        "    end_index = indices[-1]\n",
        "    for i, frame in enumerate(container.decode(video=0)):\n",
        "        if i > end_index:\n",
        "            break\n",
        "        if i >= start_index and i in indices:\n",
        "            frames.append(frame)\n",
        "    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
        "\n",
        "\n",
        "def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n",
        "    '''\n",
        "    Sample a given number of frame indices from the video.\n",
        "    Args:\n",
        "        clip_len (`int`): Total number of frames to sample.\n",
        "        frame_sample_rate (`int`): Sample every n-th frame.\n",
        "        seg_len (`int`): Maximum allowed index of sample's last frame.\n",
        "    Returns:\n",
        "        indices (`List[int]`): List of sampled frame indices\n",
        "    '''\n",
        "    converted_len = int(clip_len * frame_sample_rate)\n",
        "    end_idx = np.random.randint(converted_len, seg_len)\n",
        "    start_idx = end_idx - converted_len\n",
        "    indices = np.linspace(start_idx, end_idx, num=clip_len)\n",
        "    indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n",
        "    return indices\n",
        "\n",
        "\n",
        "# video clip consists of 300 frames (10 seconds at 30 FPS)\n",
        "file_path = hf_hub_download(\n",
        "    repo_id=\"nielsr/video-demo\", filename=\"eating_spaghetti.mp4\", repo_type=\"dataset\"\n",
        ")\n",
        "container = av.open(file_path)\n",
        "\n",
        "# sample 8 frames\n",
        "indices = sample_frame_indices(clip_len=8, frame_sample_rate=1, seg_len=container.streams.video[0].frames)\n",
        "video = read_video_pyav(container, indices)\n",
        "print('video size:', video.shape)\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\"microsoft/xclip-base-patch32\")\n",
        "model = AutoModel.from_pretrained(\"microsoft/xclip-base-patch32\")\n",
        "\n",
        "inputs = processor(videos=list(video), return_tensors=\"pt\")\n",
        "\n",
        "video_features = model.get_video_features(**inputs)\n",
        "print('extracted video features size:', video_features.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRulqGB0tawi",
        "outputId": "9d3b825a-7d63-4a7b-ebaf-bc09b3e17b0e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "video size: (8, 360, 640, 3)\n",
            "extracted video features size: torch.Size([1, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Extracting Frame-wise Feature (8 frames by default)"
      ],
      "metadata": {
        "id": "q4hcaa1Gt3vK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GeyLEyUwopnY",
        "outputId": "4f159c87-5b65-4afe-c7a5-02c1c19fb47f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "video size: (8, 360, 640, 3)\n",
            "extracted frame wise features size: torch.Size([8, 50, 768])\n"
          ]
        }
      ],
      "source": [
        "import av\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from transformers import AutoProcessor, XCLIPVisionModel\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "\n",
        "def read_video_pyav(container, indices):\n",
        "    '''\n",
        "    Decode the video with PyAV decoder.\n",
        "    Args:\n",
        "        container (`av.container.input.InputContainer`): PyAV container.\n",
        "        indices (`List[int]`): List of frame indices to decode.\n",
        "    Returns:\n",
        "        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n",
        "    '''\n",
        "    frames = []\n",
        "    container.seek(0)\n",
        "    start_index = indices[0]\n",
        "    end_index = indices[-1]\n",
        "    for i, frame in enumerate(container.decode(video=0)):\n",
        "        if i > end_index:\n",
        "            break\n",
        "        if i >= start_index and i in indices:\n",
        "            frames.append(frame)\n",
        "    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
        "\n",
        "\n",
        "def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n",
        "    '''\n",
        "    Sample a given number of frame indices from the video.\n",
        "    Args:\n",
        "        clip_len (`int`): Total number of frames to sample.\n",
        "        frame_sample_rate (`int`): Sample every n-th frame.\n",
        "        seg_len (`int`): Maximum allowed index of sample's last frame.\n",
        "    Returns:\n",
        "        indices (`List[int]`): List of sampled frame indices\n",
        "    '''\n",
        "    converted_len = int(clip_len * frame_sample_rate)\n",
        "    end_idx = np.random.randint(converted_len, seg_len)\n",
        "    start_idx = end_idx - converted_len\n",
        "    indices = np.linspace(start_idx, end_idx, num=clip_len)\n",
        "    indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n",
        "    return indices\n",
        "\n",
        "\n",
        "# video clip consists of 300 frames (10 seconds at 30 FPS) https://huggingface.co/datasets/nielsr/video-demo/blob/main/eating_spaghetti.mp4\n",
        "file_path = hf_hub_download(\n",
        "    repo_id=\"nielsr/video-demo\", filename=\"eating_spaghetti.mp4\", repo_type=\"dataset\"\n",
        ")\n",
        "container = av.open(file_path)\n",
        "\n",
        "# sample 16 frames\n",
        "indices = sample_frame_indices(clip_len=8, frame_sample_rate=1, seg_len=container.streams.video[0].frames)\n",
        "video = read_video_pyav(container, indices)\n",
        "print('video size:', video.shape)\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\"microsoft/xclip-base-patch32\")\n",
        "model = XCLIPVisionModel.from_pretrained(\"microsoft/xclip-base-patch32\")\n",
        "\n",
        "pixel_values = processor(videos=list(video), return_tensors=\"pt\").pixel_values\n",
        "\n",
        "batch_size, num_frames, num_channels, height, width = pixel_values.shape\n",
        "pixel_values = pixel_values.reshape(-1, num_channels, height, width)\n",
        "\n",
        "outputs = model(pixel_values)\n",
        "last_hidden_state = outputs.last_hidden_state\n",
        "\n",
        "print('extracted frame wise features size:', last_hidden_state.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Extracting Frame-wise Feature (5 frames customized)"
      ],
      "metadata": {
        "id": "1C68uoOvuNvb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import XCLIPVisionConfig\n",
        "configuration = XCLIPVisionConfig()\n",
        "configuration.num_frames = 5\n",
        "# configuration.mit_num_attention_heads = 5\n",
        "\n",
        "\n",
        "indices = sample_frame_indices(clip_len=5, frame_sample_rate=1, seg_len=container.streams.video[0].frames)\n",
        "video = read_video_pyav(container, indices)\n",
        "print('video size:', video.shape)\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\"microsoft/xclip-base-patch32\")\n",
        "model = XCLIPVisionModel(configuration)\n",
        "\n",
        "pixel_values = processor(videos=list(video), return_tensors=\"pt\").pixel_values\n",
        "\n",
        "batch_size, num_frames, num_channels, height, width = pixel_values.shape\n",
        "pixel_values = pixel_values.reshape(-1, num_channels, height, width)\n",
        "\n",
        "outputs = model(pixel_values)\n",
        "last_hidden_state = outputs.last_hidden_state\n",
        "\n",
        "print('extracted frame wise features size:', last_hidden_state.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jG8rZRAuRP0",
        "outputId": "7e5e3a86-024a-4d8b-b914-0b2ac06ec05f"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "video size: (5, 360, 640, 3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "extracted frame wise features size: torch.Size([5, 50, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zL41qKNCsqNI",
        "outputId": "e16f3102-d239-4bbb-da46-768602b19495"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XCLIPVisionModel(\n",
              "  (vision_model): XCLIPVisionTransformer(\n",
              "    (embeddings): XCLIPVisionEmbeddings(\n",
              "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
              "      (position_embedding): Embedding(50, 768)\n",
              "    )\n",
              "    (pre_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (encoder): XCLIPVisionEncoder(\n",
              "      (layers): ModuleList(\n",
              "        (0-11): 12 x XCLIPVisionEncoderLayer(\n",
              "          (message_fc): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (message_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (message_attn): XCLIPAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (self_attn): XCLIPAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): XCLIPMLP(\n",
              "            (activation_fn): QuickGELUActivation()\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs.keys(), outputs.pooler_output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6vWmY8Sq4vF",
        "outputId": "6a9b3206-ec2b-4548-fbb5-d2d60534fe41"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(odict_keys(['last_hidden_state', 'pooler_output']), torch.Size([8, 768]))"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVqc-ErboxqT",
        "outputId": "1c34a32b-d830-4920-e54b-9452154f8a15"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "video_features: torch.Size([1, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHRa4E8qsn-q",
        "outputId": "5aff2a67-ce8e-451a-901d-65b1391228a0"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XCLIPModel(\n",
              "  (text_model): XCLIPTextTransformer(\n",
              "    (embeddings): XCLIPTextEmbeddings(\n",
              "      (token_embedding): Embedding(49408, 512)\n",
              "      (position_embedding): Embedding(77, 512)\n",
              "    )\n",
              "    (encoder): XCLIPEncoder(\n",
              "      (layers): ModuleList(\n",
              "        (0-11): 12 x XCLIPEncoderLayer(\n",
              "          (self_attn): XCLIPAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): XCLIPMLP(\n",
              "            (activation_fn): QuickGELUActivation()\n",
              "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (vision_model): XCLIPVisionTransformer(\n",
              "    (embeddings): XCLIPVisionEmbeddings(\n",
              "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
              "      (position_embedding): Embedding(50, 768)\n",
              "    )\n",
              "    (pre_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (encoder): XCLIPVisionEncoder(\n",
              "      (layers): ModuleList(\n",
              "        (0-11): 12 x XCLIPVisionEncoderLayer(\n",
              "          (message_fc): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (message_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (message_attn): XCLIPAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (self_attn): XCLIPAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): XCLIPMLP(\n",
              "            (activation_fn): QuickGELUActivation()\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
              "  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
              "  (prompts_visual_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  (mit): XCLIPMultiframeIntegrationTransformer(\n",
              "    (encoder): XCLIPEncoder(\n",
              "      (layers): ModuleList(\n",
              "        (0): XCLIPEncoderLayer(\n",
              "          (self_attn): XCLIPAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): XCLIPMLP(\n",
              "            (activation_fn): QuickGELUActivation()\n",
              "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (prompts_generator): XCLIPPromptGenerator(\n",
              "    (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "    (decoder): ModuleList(\n",
              "      (0-1): 2 x PromptGeneratorLayer(\n",
              "        (cross_attn): XCLIPCrossAttention(\n",
              "          (q_proj): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (v_proj): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): Sequential(\n",
              "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (1): QuickGELUActivation()\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import XCLIPVisionConfig\n",
        "configuration = XCLIPVisionConfig()"
      ],
      "metadata": {
        "id": "lFD5U6gopbI9"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "configuration.num_frames = 5"
      ],
      "metadata": {
        "id": "Fau4TpkxqJSQ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7eWVrz2tqWaM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}