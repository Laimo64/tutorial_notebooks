{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO5L42e7lMzFMko3sK1Dy4X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/tutorial_notebooks/blob/main/Video_ViT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeMhjnILgM4w",
        "outputId": "bd6ec0c5-8682-49fe-fd77-cf679e4f5f9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.0/33.0 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install av"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Set a custom Hugging Face home directory\n",
        "os.environ['HF_HOME'] = '/content'"
      ],
      "metadata": {
        "id": "Ln-6PWxfgSJ-"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video Classification (deafult 32 frames)"
      ],
      "metadata": {
        "id": "bw2ab7w6gXwL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import av\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from transformers import VivitImageProcessor, VivitForVideoClassification\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "\n",
        "def read_video_pyav(container, indices):\n",
        "    '''\n",
        "    Decode the video with PyAV decoder.\n",
        "    Args:\n",
        "        container (`av.container.input.InputContainer`): PyAV container.\n",
        "        indices (`List[int]`): List of frame indices to decode.\n",
        "    Returns:\n",
        "        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n",
        "    '''\n",
        "    frames = []\n",
        "    container.seek(0)\n",
        "    start_index = indices[0]\n",
        "    end_index = indices[-1]\n",
        "    for i, frame in enumerate(container.decode(video=0)):\n",
        "        if i > end_index:\n",
        "            break\n",
        "        if i >= start_index and i in indices:\n",
        "            frames.append(frame)\n",
        "    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
        "\n",
        "\n",
        "def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n",
        "    '''\n",
        "    Sample a given number of frame indices from the video.\n",
        "    Args:\n",
        "        clip_len (`int`): Total number of frames to sample.\n",
        "        frame_sample_rate (`int`): Sample every n-th frame.\n",
        "        seg_len (`int`): Maximum allowed index of sample's last frame.\n",
        "    Returns:\n",
        "        indices (`List[int]`): List of sampled frame indices\n",
        "    '''\n",
        "    converted_len = int(clip_len * frame_sample_rate)\n",
        "    end_idx = np.random.randint(converted_len, seg_len)\n",
        "    start_idx = end_idx - converted_len\n",
        "    indices = np.linspace(start_idx, end_idx, num=clip_len)\n",
        "    indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n",
        "    return indices\n",
        "\n",
        "\n",
        "# video clip consists of 300 frames (10 seconds at 30 FPS)\n",
        "file_path = hf_hub_download(\n",
        "    repo_id=\"nielsr/video-demo\", filename=\"eating_spaghetti.mp4\", repo_type=\"dataset\"\n",
        ")\n",
        "container = av.open(file_path)\n",
        "\n",
        "# sample 32 frames\n",
        "indices = sample_frame_indices(clip_len=32, frame_sample_rate=4, seg_len=container.streams.video[0].frames)\n",
        "video = read_video_pyav(container=container, indices=indices)\n",
        "\n",
        "image_processor = VivitImageProcessor.from_pretrained(\"google/vivit-b-16x2-kinetics400\")\n",
        "model = VivitForVideoClassification.from_pretrained(\"google/vivit-b-16x2-kinetics400\")\n",
        "\n",
        "inputs = image_processor(list(video), return_tensors=\"pt\")\n",
        "print('input video:', inputs['pixel_values'].shape)\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "\n",
        "# model predicts one of the 400 Kinetics-400 classes\n",
        "predicted_label = logits.argmax(-1).item()\n",
        "print(model.config.id2label[predicted_label])\n",
        "print('Predicted Label:', predicted_label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPdBJDs_gUa1",
        "outputId": "a5de3472-082a-4f87-dc27-b0d70d79caf5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input video: torch.Size([1, 32, 3, 224, 224])\n",
            "LABEL_116\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video Classification (customized 5 frames)"
      ],
      "metadata": {
        "id": "o1vpiWWEgbXY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import VivitImageProcessor, VivitForVideoClassification, VivitConfig\n",
        "from safetensors.torch import load_file\n",
        "from transformers import AutoConfig\n",
        "\n",
        "from collections import OrderedDict\n",
        "\n",
        "\n",
        "def partial_loading(model, state_dict):\n",
        "    new_state_dict = OrderedDict()\n",
        "\n",
        "    for k, v in state_dict.items():\n",
        "        if k in model.state_dict() and model.state_dict()[k].shape == v.shape:\n",
        "            new_state_dict[k] = v\n",
        "\n",
        "    model.load_state_dict(new_state_dict, strict=False)\n",
        "    return model\n",
        "\n",
        "# sample 5 frames\n",
        "num_frames = 5\n",
        "indices = sample_frame_indices(clip_len=5, frame_sample_rate=4, seg_len=container.streams.video[0].frames)\n",
        "video = read_video_pyav(container=container, indices=indices)\n",
        "\n",
        "image_processor = VivitImageProcessor.from_pretrained(\"google/vivit-b-16x2-kinetics400\")\n",
        "# model = VivitForVideoClassification.from_pretrained(\"google/vivit-b-16x2-kinetics400\")\n",
        "\n",
        "\n",
        "config = AutoConfig.from_pretrained(\"google/vivit-b-16x2-kinetics400\")\n",
        "config.num_frames = num_frames\n",
        "config.video_size = [num_frames, 224, 224]\n",
        "\n",
        "model = VivitForVideoClassification(config)\n",
        "model_weight = '/content/hub/models--google--vivit-b-16x2-kinetics400/snapshots/8a7171a57f79b9aaa58bc8d977c002a0ea0f0d42/pytorch_model.bin'\n",
        "state_dict = torch.load(model_weight, weights_only=True)\n",
        "model = partial_loading(model, state_dict)\n",
        "\n",
        "\n",
        "\n",
        "inputs = image_processor(list(video), return_tensors=\"pt\")\n",
        "print('input video:', inputs['pixel_values'].shape)\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "\n",
        "# model predicts one of the 400 Kinetics-400 classes\n",
        "predicted_label = logits.argmax(-1).item()\n",
        "# print(model.config.id2label[predicted_label])\n",
        "print('Predicted Label:', predicted_label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpHelksXgWA-",
        "outputId": "04ce47df-d431-4b63-ecfc-cface25ca9f0"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input video: torch.Size([1, 5, 3, 224, 224])\n",
            "Predicted Label: 339\n"
          ]
        }
      ]
    }
  ]
}