{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/tutorial_notebooks/blob/main/Depth_Anything_Foundation_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Depth Anything\n",
        "Paper: https://arxiv.org/pdf/2401.10891.pdf"
      ],
      "metadata": {
        "id": "nY_I9UZs5Rph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install git+https://github.com/huggingface/transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qa7QRof6zUsa",
        "outputId": "0bbd1b0c-5948-4f73-bdae-a791791a3109"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Please restart the session"
      ],
      "metadata": {
        "id": "NAaMHZ3HzuRI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_-NEJlMzFlJ"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoImageProcessor, AutoModelForDepthEstimation, DepthAnythingForDepthEstimation\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "image_processor = AutoImageProcessor.from_pretrained(\"LiheYoung/depth-anything-small-hf\")\n",
        "# model = AutoModelForDepthEstimation.from_pretrained(\"LiheYoung/depth-anything-small-hf\")\n",
        "model = DepthAnythingForDepthEstimation.from_pretrained(\"LiheYoung/depth-anything-small-hf\")\n",
        "\n",
        "# prepare image for the model\n",
        "inputs = image_processor(images=image, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    predicted_depth = outputs.predicted_depth\n",
        "\n",
        "# interpolate to original size\n",
        "prediction = torch.nn.functional.interpolate(\n",
        "    predicted_depth.unsqueeze(1),\n",
        "    size=image.size[::-1],\n",
        "    mode=\"bicubic\",\n",
        "    align_corners=False,\n",
        ")\n",
        "\n",
        "# visualize the prediction\n",
        "output = prediction.squeeze().cpu().numpy()\n",
        "\n",
        "print('Original model train on the scale of 0 to 30')\n",
        "print('prediction: min scale =', output.min(), 'max scale=', output.max())\n",
        "\n",
        "#Rescale the reconstruction according to your application\n",
        "formatted = (output * 255 / np.max(output)).astype(\"uint8\")\n",
        "depth = Image.fromarray(formatted)\n",
        "\n",
        "plt.figure(121),plt.axis('OFF'),plt.imshow(image)\n",
        "plt.figure(122),plt.axis('OFF'),plt.imshow(depth);"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    predicted_depth = outputs.predicted_depth\n",
        "\n",
        "predicted_depth.shape"
      ],
      "metadata": {
        "id": "aZzgItSCXjPY",
        "outputId": "acbe3ce2-557b-4a82-869c-8a394ef277f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 518, 686])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = image_processor(images=image, return_tensors=\"pt\")\n",
        "input_res = input.copy()\n",
        "pixel_values = input['pixel_values']\n",
        "print(pixel_values.shape)\n",
        "image_shape=(224,280)\n",
        "input_res['pixel_values'] = torch.nn.functional.interpolate(pixel_values, size=image_shape, mode=\"bilinear\", align_corners=True)\n",
        "with torch.no_grad():\n",
        "    outputs = model(**input_res)\n",
        "    predicted_depth = outputs.predicted_depth\n",
        "\n",
        "print('predicted_depth:', predicted_depth.shape)\n",
        "# print(pixel_values.shape)\n",
        "# h, w = pixel_values.shape[-2:]\n",
        "# features = model.get_intermediate_layers(pixel_values, 4, return_class_token=True)"
      ],
      "metadata": {
        "id": "h4FJHe8uXlfl",
        "outputId": "52c55894-50cb-41d3-aa26-c579e1662ddd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 3, 518, 686])\n",
            "predicted_depth: torch.Size([1, 224, 280])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class DARES(nn.Module):\n",
        "    def __init__(self,):\n",
        "        super(DARES, self).__init__()\n",
        "        model = DepthAnythingForDepthEstimation.from_pretrained(\"LiheYoung/depth-anything-small-hf\")\n",
        "\n",
        "    def forward(self, pixel_values):\n",
        "        return 0\n"
      ],
      "metadata": {
        "id": "DIsZ2dVmXqJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DepthAnythingConfig, DepthAnythingForDepthEstimation\n",
        "\n",
        "# Initializing a DepthAnything small style configuration\n",
        "configuration = DepthAnythingConfig()\n",
        "\n",
        "# Initializing a model from the DepthAnything small style configuration\n",
        "model = DepthAnythingForDepthEstimation(configuration)\n",
        "# model = AutoModelForDepthEstimation.from_pretrained(\"LiheYoung/depth-anything-small-hf\")\n",
        "\n",
        "# Accessing the model configuration\n",
        "configuration = model.config"
      ],
      "metadata": {
        "id": "ROSMRxk70yt6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = DepthAnythingForDepthEstimation.from_pretrained(\"LiheYoung/depth-anything-small-hf\")"
      ],
      "metadata": {
        "id": "hchAnlP21L81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Liver: SCARED"
      ],
      "metadata": {
        "id": "ZZWBvrUv2c-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "\n",
        "d3k4_url = 'https://drive.google.com/uc?id=1_HrQTeZgU3AE88p1Ykawi2lkdijRNQVz'\n",
        "gdown.download(d3k4_url,'d3k4.zip',quiet=True)\n",
        "!unzip -q d3k4.zip\n",
        "!mkdir liver\n",
        "!mv -f d3k4 liver/"
      ],
      "metadata": {
        "id": "6zWQVgP50NrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "others"
      ],
      "metadata": {
        "id": "kiWoJe0U41U8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "vidcap = cv2.VideoCapture('video.L.avi')\n",
        "def getFrame(sec):\n",
        "    vidcap.set(cv2.CAP_PROP_POS_MSEC,sec*1000)\n",
        "    hasFrames,image = vidcap.read() #1920x1080\n",
        "    try:\n",
        "        image = cv2.resize(image, (1920, 1080))\n",
        "    except:\n",
        "        print('type:', type(image))\n",
        "\n",
        "    if hasFrames:\n",
        "        cv2.imwrite(\"liver2/image\"+str(count)+\".png\", image)     # save frame as JPG file\n",
        "    return hasFrames\n",
        "sec = 0\n",
        "frameRate = 0.5 #//it will capture image in each 0.5 second\n",
        "count=1\n",
        "success = getFrame(sec)\n",
        "while success:\n",
        "    count = count + 1\n",
        "    sec = sec + frameRate\n",
        "    sec = round(sec, 2)\n",
        "    success = getFrame(sec)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZR9lKGjX3FAK",
        "outputId": "9ab66268-0f43-4ebb-bc66-b8e3fe277646"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "type: <class 'NoneType'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image = Image.open('/content/liver2/image10.png')\n",
        "\n",
        "\n",
        "# prepare image for the model\n",
        "inputs = image_processor(images=image, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    predicted_depth = outputs.predicted_depth\n",
        "\n",
        "# interpolate to original size\n",
        "prediction = torch.nn.functional.interpolate(\n",
        "    predicted_depth.unsqueeze(1),\n",
        "    size=image.size[::-1],\n",
        "    mode=\"bicubic\",\n",
        "    align_corners=False,\n",
        ")\n",
        "\n",
        "# visualize the prediction\n",
        "output = prediction.squeeze().cpu().numpy()\n",
        "\n",
        "print('Original model train on the scale of 0 to 30')\n",
        "print('prediction: min scale =', output.min(), 'max scale=', output.max())\n",
        "\n",
        "#Rescale the reconstruction according to your application\n",
        "formatted = (output * 255 / np.max(output)).astype(\"uint8\")\n",
        "depth = Image.fromarray(formatted)\n",
        "\n",
        "plt.figure(121),plt.axis('OFF'),plt.imshow(image)\n",
        "plt.figure(122),plt.axis('OFF'),plt.imshow(depth);"
      ],
      "metadata": {
        "id": "JImE8PSp3SYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V9clTBhg3wMG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}