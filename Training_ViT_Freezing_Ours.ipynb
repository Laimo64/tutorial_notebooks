{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Training_ViT_Freezing_Ours.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP2DOMb/GdifBgcThgFNDep",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/tutorial_notebooks/blob/main/Training_ViT_Freezing_Ours.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3vJcq5Ybx8P",
        "outputId": "2ec6f420-734d-4e56-ad5b-83954c6f2ae5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  localization_2.zip\n",
            "  inflating: localization/001202.png  \n",
            "  inflating: localization/001203.png  \n",
            "  inflating: localization/1_frame132.png  \n",
            "  inflating: localization/1_frame146.png  \n",
            "  inflating: localization/16_frame119.png  \n",
            "  inflating: localization/16_frame122.png  \n",
            "  inflating: localization/5_frame078.png  \n",
            "  inflating: localization/5_frame118.png  \n",
            "  inflating: localization/combine_miccai18_ResNet50_256,320_170_best_checkpoint.pth.tar  \n",
            "  inflating: localization/raw_image.png  \n"
          ]
        }
      ],
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "# id = ['18A9mAt9r4CoXLeoCK2muPNvIEqVKvOaw']\n",
        "id = ['1pZgspfWi1Mh8sqV_lbJLQTuFZyhnTPlA']\n",
        "downloaded = drive.CreateFile({'id':id[0]}) \n",
        "downloaded.GetContentFile('localization_2.zip')\n",
        "!unzip localization_2.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip -q install ml_collections\n",
        "! wget https://storage.googleapis.com/vit_models/imagenet21k%2Bimagenet2012/R50%2BViT-B_16.npz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfUHzpglcFnO",
        "outputId": "4d215bf8-ca7f-4582-8532-8a4f523e1284"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |████▏                           | 10 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 20 kB 10.8 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 30 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 40 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 51 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 61 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 71 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 77 kB 4.1 MB/s \n",
            "\u001b[?25h  Building wheel for ml-collections (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "--2022-08-19 00:27:42--  https://storage.googleapis.com/vit_models/imagenet21k%2Bimagenet2012/R50%2BViT-B_16.npz\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.20.128, 108.177.98.128, 74.125.142.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.20.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 395916008 (378M) [application/octet-stream]\n",
            "Saving to: ‘R50+ViT-B_16.npz’\n",
            "\n",
            "R50+ViT-B_16.npz    100%[===================>] 377.57M  39.9MB/s    in 9.6s    \n",
            "\n",
            "2022-08-19 00:27:52 (39.5 MB/s) - ‘R50+ViT-B_16.npz’ saved [395916008/395916008]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ResNet"
      ],
      "metadata": {
        "id": "xHtTHCy6c22R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright 2020 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#      http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "# Lint as: python3\n",
        "\"\"\"Bottleneck ResNet v2 with GroupNorm and Weight Standardization.\"\"\"\n",
        "import math\n",
        "\n",
        "from os.path import join as pjoin\n",
        "\n",
        "from collections import OrderedDict  # pylint: disable=g-importing-member\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def np2th(weights, conv=False):\n",
        "    \"\"\"Possibly convert HWIO to OIHW.\"\"\"\n",
        "    if conv:\n",
        "        weights = weights.transpose([3, 2, 0, 1])\n",
        "    return torch.from_numpy(weights)\n",
        "\n",
        "\n",
        "class StdConv2d(nn.Conv2d):\n",
        "\n",
        "    def forward(self, x):\n",
        "        w = self.weight\n",
        "        v, m = torch.var_mean(w, dim=[1, 2, 3], keepdim=True, unbiased=False)\n",
        "        w = (w - m) / torch.sqrt(v + 1e-5)\n",
        "        return F.conv2d(x, w, self.bias, self.stride, self.padding,\n",
        "                        self.dilation, self.groups)\n",
        "\n",
        "\n",
        "def conv3x3(cin, cout, stride=1, groups=1, bias=False):\n",
        "    return StdConv2d(cin, cout, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=bias, groups=groups)\n",
        "\n",
        "\n",
        "def conv1x1(cin, cout, stride=1, bias=False):\n",
        "    return StdConv2d(cin, cout, kernel_size=1, stride=stride,\n",
        "                     padding=0, bias=bias)\n",
        "\n",
        "\n",
        "class PreActBottleneck(nn.Module):\n",
        "    \"\"\"Pre-activation (v2) bottleneck block.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, cin, cout=None, cmid=None, stride=1):\n",
        "        super().__init__()\n",
        "        cout = cout or cin\n",
        "        cmid = cmid or cout//4\n",
        "\n",
        "        self.gn1 = nn.GroupNorm(32, cmid, eps=1e-6)\n",
        "        self.conv1 = conv1x1(cin, cmid, bias=False)\n",
        "        self.gn2 = nn.GroupNorm(32, cmid, eps=1e-6)\n",
        "        self.conv2 = conv3x3(cmid, cmid, stride, bias=False)  # Original code has it on conv1!!\n",
        "        self.gn3 = nn.GroupNorm(32, cout, eps=1e-6)\n",
        "        self.conv3 = conv1x1(cmid, cout, bias=False)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        if (stride != 1 or cin != cout):\n",
        "            # Projection also with pre-activation according to paper.\n",
        "            self.downsample = conv1x1(cin, cout, stride, bias=False)\n",
        "            self.gn_proj = nn.GroupNorm(cout, cout)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Residual branch\n",
        "        residual = x\n",
        "        if hasattr(self, 'downsample'):\n",
        "            residual = self.downsample(x)\n",
        "            residual = self.gn_proj(residual)\n",
        "\n",
        "        # Unit's branch\n",
        "        y = self.relu(self.gn1(self.conv1(x)))\n",
        "        y = self.relu(self.gn2(self.conv2(y)))\n",
        "        y = self.gn3(self.conv3(y))\n",
        "\n",
        "        y = self.relu(residual + y)\n",
        "        return y\n",
        "\n",
        "    def load_from(self, weights, n_block, n_unit):\n",
        "        conv1_weight = np2th(weights[pjoin(n_block, n_unit, \"conv1/kernel\")], conv=True)\n",
        "        conv2_weight = np2th(weights[pjoin(n_block, n_unit, \"conv2/kernel\")], conv=True)\n",
        "        conv3_weight = np2th(weights[pjoin(n_block, n_unit, \"conv3/kernel\")], conv=True)\n",
        "\n",
        "        gn1_weight = np2th(weights[pjoin(n_block, n_unit, \"gn1/scale\")])\n",
        "        gn1_bias = np2th(weights[pjoin(n_block, n_unit, \"gn1/bias\")])\n",
        "\n",
        "        gn2_weight = np2th(weights[pjoin(n_block, n_unit, \"gn2/scale\")])\n",
        "        gn2_bias = np2th(weights[pjoin(n_block, n_unit, \"gn2/bias\")])\n",
        "\n",
        "        gn3_weight = np2th(weights[pjoin(n_block, n_unit, \"gn3/scale\")])\n",
        "        gn3_bias = np2th(weights[pjoin(n_block, n_unit, \"gn3/bias\")])\n",
        "\n",
        "        self.conv1.weight.copy_(conv1_weight)\n",
        "        self.conv2.weight.copy_(conv2_weight)\n",
        "        self.conv3.weight.copy_(conv3_weight)\n",
        "\n",
        "        self.gn1.weight.copy_(gn1_weight.view(-1))\n",
        "        self.gn1.bias.copy_(gn1_bias.view(-1))\n",
        "\n",
        "        self.gn2.weight.copy_(gn2_weight.view(-1))\n",
        "        self.gn2.bias.copy_(gn2_bias.view(-1))\n",
        "\n",
        "        self.gn3.weight.copy_(gn3_weight.view(-1))\n",
        "        self.gn3.bias.copy_(gn3_bias.view(-1))\n",
        "\n",
        "        if hasattr(self, 'downsample'):\n",
        "            proj_conv_weight = np2th(weights[pjoin(n_block, n_unit, \"conv_proj/kernel\")], conv=True)\n",
        "            proj_gn_weight = np2th(weights[pjoin(n_block, n_unit, \"gn_proj/scale\")])\n",
        "            proj_gn_bias = np2th(weights[pjoin(n_block, n_unit, \"gn_proj/bias\")])\n",
        "\n",
        "            self.downsample.weight.copy_(proj_conv_weight)\n",
        "            self.gn_proj.weight.copy_(proj_gn_weight.view(-1))\n",
        "            self.gn_proj.bias.copy_(proj_gn_bias.view(-1))\n",
        "\n",
        "class ResNetV2(nn.Module):\n",
        "    \"\"\"Implementation of Pre-activation (v2) ResNet mode.\"\"\"\n",
        "\n",
        "    def __init__(self, block_units, width_factor):\n",
        "        super().__init__()\n",
        "        width = int(64 * width_factor)\n",
        "        self.width = width\n",
        "\n",
        "        # The following will be unreadable if we split lines.\n",
        "        # pylint: disable=line-too-long\n",
        "        self.root = nn.Sequential(OrderedDict([\n",
        "            ('conv', StdConv2d(3, width, kernel_size=7, stride=2, bias=False, padding=3)),\n",
        "            ('gn', nn.GroupNorm(32, width, eps=1e-6)),\n",
        "            ('relu', nn.ReLU(inplace=True)),\n",
        "            ('pool', nn.MaxPool2d(kernel_size=3, stride=2, padding=0))\n",
        "        ]))\n",
        "\n",
        "        self.body = nn.Sequential(OrderedDict([\n",
        "            ('block1', nn.Sequential(OrderedDict(\n",
        "                [('unit1', PreActBottleneck(cin=width, cout=width*4, cmid=width))] +\n",
        "                [(f'unit{i:d}', PreActBottleneck(cin=width*4, cout=width*4, cmid=width)) for i in range(2, block_units[0] + 1)],\n",
        "                ))),\n",
        "            ('block2', nn.Sequential(OrderedDict(\n",
        "                [('unit1', PreActBottleneck(cin=width*4, cout=width*8, cmid=width*2, stride=2))] +\n",
        "                [(f'unit{i:d}', PreActBottleneck(cin=width*8, cout=width*8, cmid=width*2)) for i in range(2, block_units[1] + 1)],\n",
        "                ))),    \n",
        "            ('block3', nn.Sequential(OrderedDict(\n",
        "                [('unit1', PreActBottleneck(cin=width*8, cout=width*16, cmid=width*4, stride=2))] +\n",
        "                [(f'unit{i:d}', PreActBottleneck(cin=width*16, cout=width*16, cmid=width*4)) for i in range(2, block_units[2] + 1)],\n",
        "                ))),\n",
        "        ]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.root(x)\n",
        "        x = self.body(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "6lzvUrKrcq3o"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ViT"
      ],
      "metadata": {
        "id": "8M24S858c5Mz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding=utf-8\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import copy\n",
        "import logging\n",
        "import math\n",
        "\n",
        "from os.path import join as pjoin\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "from torch.nn import CrossEntropyLoss, Dropout, Softmax, Linear, Conv2d, LayerNorm\n",
        "from torch.nn.modules.utils import _pair\n",
        "from scipy import ndimage\n",
        "\n",
        "#import models.configs as configs\n",
        "\n",
        "#from .modeling_resnet import ResNetV2\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "ATTENTION_Q = \"MultiHeadDotProductAttention_1/query\"\n",
        "ATTENTION_K = \"MultiHeadDotProductAttention_1/key\"\n",
        "ATTENTION_V = \"MultiHeadDotProductAttention_1/value\"\n",
        "ATTENTION_OUT = \"MultiHeadDotProductAttention_1/out\"\n",
        "FC_0 = \"MlpBlock_3/Dense_0\"\n",
        "FC_1 = \"MlpBlock_3/Dense_1\"\n",
        "ATTENTION_NORM = \"LayerNorm_0\"\n",
        "MLP_NORM = \"LayerNorm_2\"\n",
        "\n",
        "\n",
        "def np2th(weights, conv=False):\n",
        "    \"\"\"Possibly convert HWIO to OIHW.\"\"\"\n",
        "    if conv:\n",
        "        weights = weights.transpose([3, 2, 0, 1])\n",
        "    return torch.from_numpy(weights)\n",
        "\n",
        "\n",
        "def swish(x):\n",
        "    return x * torch.sigmoid(x)\n",
        "\n",
        "\n",
        "ACT2FN = {\"gelu\": torch.nn.functional.gelu, \"relu\": torch.nn.functional.relu, \"swish\": swish}\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, config, vis):\n",
        "        super(Attention, self).__init__()\n",
        "        self.vis = vis\n",
        "        self.num_attention_heads = config.transformer[\"num_heads\"]\n",
        "        self.attention_head_size = int(config.hidden_size / self.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.query = Linear(config.hidden_size, self.all_head_size)\n",
        "        self.key = Linear(config.hidden_size, self.all_head_size)\n",
        "        self.value = Linear(config.hidden_size, self.all_head_size)\n",
        "\n",
        "        self.out = Linear(config.hidden_size, config.hidden_size)\n",
        "        self.attn_dropout = Dropout(config.transformer[\"attention_dropout_rate\"])\n",
        "        self.proj_dropout = Dropout(config.transformer[\"attention_dropout_rate\"])\n",
        "\n",
        "        self.softmax = Softmax(dim=-1)\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "        mixed_key_layer = self.key(hidden_states)\n",
        "        mixed_value_layer = self.value(hidden_states)\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
        "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
        "\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "        attention_probs = self.softmax(attention_scores)\n",
        "        weights = attention_probs if self.vis else None\n",
        "        attention_probs = self.attn_dropout(attention_probs)\n",
        "\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(*new_context_layer_shape)\n",
        "        attention_output = self.out(context_layer)\n",
        "        attention_output = self.proj_dropout(attention_output)\n",
        "        return attention_output, weights\n",
        "\n",
        "\n",
        "class Mlp(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(Mlp, self).__init__()\n",
        "        self.fc1 = Linear(config.hidden_size, config.transformer[\"mlp_dim\"])\n",
        "        self.fc2 = Linear(config.transformer[\"mlp_dim\"], config.hidden_size)\n",
        "        self.act_fn = ACT2FN[\"gelu\"]\n",
        "        self.dropout = Dropout(config.transformer[\"dropout_rate\"])\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        nn.init.xavier_uniform_(self.fc1.weight)\n",
        "        nn.init.xavier_uniform_(self.fc2.weight)\n",
        "        nn.init.normal_(self.fc1.bias, std=1e-6)\n",
        "        nn.init.normal_(self.fc2.bias, std=1e-6)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act_fn(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Embeddings(nn.Module):\n",
        "    \"\"\"Construct the embeddings from patch, position embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, config, img_size, in_channels=3):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.hybrid = None\n",
        "        img_size = _pair(img_size)\n",
        "\n",
        "        if config.patches.get(\"grid\") is not None:\n",
        "            grid_size = config.patches[\"grid\"]\n",
        "            patch_size = (img_size[0] // 16 // grid_size[0], img_size[1] // 16 // grid_size[1])\n",
        "            n_patches = (img_size[0] // 16) * (img_size[1] // 16)\n",
        "            self.hybrid = True\n",
        "        else:\n",
        "            patch_size = _pair(config.patches[\"size\"])\n",
        "            n_patches = (img_size[0] // patch_size[0]) * (img_size[1] // patch_size[1])\n",
        "            self.hybrid = False\n",
        "\n",
        "        if self.hybrid:\n",
        "            self.hybrid_model = ResNetV2(block_units=config.resnet.num_layers,\n",
        "                                         width_factor=config.resnet.width_factor)\n",
        "            in_channels = self.hybrid_model.width * 16\n",
        "        self.patch_embeddings = Conv2d(in_channels=in_channels,\n",
        "                                       out_channels=config.hidden_size,\n",
        "                                       kernel_size=patch_size,\n",
        "                                       stride=patch_size)\n",
        "        self.position_embeddings = nn.Parameter(torch.zeros(1, n_patches+1, config.hidden_size))\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, config.hidden_size))\n",
        "\n",
        "        self.dropout = Dropout(config.transformer[\"dropout_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.shape[0]\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "\n",
        "        if self.hybrid:\n",
        "            x = self.hybrid_model(x)\n",
        "        x = self.patch_embeddings(x)\n",
        "        x = x.flatten(2)\n",
        "        x = x.transpose(-1, -2)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "        embeddings = x + self.position_embeddings\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config, vis):\n",
        "        super(Block, self).__init__()\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.attention_norm = LayerNorm(config.hidden_size, eps=1e-6)\n",
        "        self.ffn_norm = LayerNorm(config.hidden_size, eps=1e-6)\n",
        "        self.ffn = Mlp(config)\n",
        "        self.attn = Attention(config, vis)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = x\n",
        "        x = self.attention_norm(x)\n",
        "        x, weights = self.attn(x)\n",
        "        x = x + h\n",
        "\n",
        "        h = x\n",
        "        x = self.ffn_norm(x)\n",
        "        x = self.ffn(x)\n",
        "        x = x + h\n",
        "        return x, weights\n",
        "\n",
        "    def load_from(self, weights, n_block):\n",
        "        ROOT = f\"Transformer/encoderblock_{n_block}\"\n",
        "        with torch.no_grad():\n",
        "            query_weight = np2th(weights[pjoin(ROOT, ATTENTION_Q, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
        "            key_weight = np2th(weights[pjoin(ROOT, ATTENTION_K, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
        "            value_weight = np2th(weights[pjoin(ROOT, ATTENTION_V, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
        "            out_weight = np2th(weights[pjoin(ROOT, ATTENTION_OUT, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
        "\n",
        "            query_bias = np2th(weights[pjoin(ROOT, ATTENTION_Q, \"bias\")]).view(-1)\n",
        "            key_bias = np2th(weights[pjoin(ROOT, ATTENTION_K, \"bias\")]).view(-1)\n",
        "            value_bias = np2th(weights[pjoin(ROOT, ATTENTION_V, \"bias\")]).view(-1)\n",
        "            out_bias = np2th(weights[pjoin(ROOT, ATTENTION_OUT, \"bias\")]).view(-1)\n",
        "\n",
        "            self.attn.query.weight.copy_(query_weight)\n",
        "            self.attn.key.weight.copy_(key_weight)\n",
        "            self.attn.value.weight.copy_(value_weight)\n",
        "            self.attn.out.weight.copy_(out_weight)\n",
        "            self.attn.query.bias.copy_(query_bias)\n",
        "            self.attn.key.bias.copy_(key_bias)\n",
        "            self.attn.value.bias.copy_(value_bias)\n",
        "            self.attn.out.bias.copy_(out_bias)\n",
        "\n",
        "            mlp_weight_0 = np2th(weights[pjoin(ROOT, FC_0, \"kernel\")]).t()\n",
        "            mlp_weight_1 = np2th(weights[pjoin(ROOT, FC_1, \"kernel\")]).t()\n",
        "            mlp_bias_0 = np2th(weights[pjoin(ROOT, FC_0, \"bias\")]).t()\n",
        "            mlp_bias_1 = np2th(weights[pjoin(ROOT, FC_1, \"bias\")]).t()\n",
        "\n",
        "            self.ffn.fc1.weight.copy_(mlp_weight_0)\n",
        "            self.ffn.fc2.weight.copy_(mlp_weight_1)\n",
        "            self.ffn.fc1.bias.copy_(mlp_bias_0)\n",
        "            self.ffn.fc2.bias.copy_(mlp_bias_1)\n",
        "\n",
        "            self.attention_norm.weight.copy_(np2th(weights[pjoin(ROOT, ATTENTION_NORM, \"scale\")]))\n",
        "            self.attention_norm.bias.copy_(np2th(weights[pjoin(ROOT, ATTENTION_NORM, \"bias\")]))\n",
        "            self.ffn_norm.weight.copy_(np2th(weights[pjoin(ROOT, MLP_NORM, \"scale\")]))\n",
        "            self.ffn_norm.bias.copy_(np2th(weights[pjoin(ROOT, MLP_NORM, \"bias\")]))\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, config, vis):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.vis = vis\n",
        "        self.layer = nn.ModuleList()\n",
        "        self.encoder_norm = LayerNorm(config.hidden_size, eps=1e-6)\n",
        "        for _ in range(config.transformer[\"num_layers\"]):\n",
        "            layer = Block(config, vis)\n",
        "            self.layer.append(copy.deepcopy(layer))\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        attn_weights = []\n",
        "        for layer_block in self.layer:\n",
        "            hidden_states, weights = layer_block(hidden_states)\n",
        "            if self.vis:\n",
        "                attn_weights.append(weights)\n",
        "        encoded = self.encoder_norm(hidden_states)\n",
        "        return encoded, attn_weights\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, config, img_size, vis):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.embeddings = Embeddings(config, img_size=img_size)\n",
        "        self.encoder = Encoder(config, vis)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        embedding_output = self.embeddings(input_ids)\n",
        "        encoded, attn_weights = self.encoder(embedding_output)\n",
        "        return encoded, attn_weights\n",
        "\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, config, img_size=224, num_classes=21843, zero_head=False, vis=False):\n",
        "        super(VisionTransformer, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.zero_head = zero_head\n",
        "        self.classifier = config.classifier\n",
        "\n",
        "        self.transformer = Transformer(config, img_size, vis)\n",
        "        self.head = Linear(config.hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x, labels=None):\n",
        "        x, _ = self.transformer(x)\n",
        "        x_feat = x.clone()\n",
        "        logits = self.head(x[:, 0])\n",
        "        return logits, x_feat\n",
        "\n",
        "    def load_from(self, weights):\n",
        "        with torch.no_grad():\n",
        "            if self.zero_head:\n",
        "                nn.init.zeros_(self.head.weight)\n",
        "                nn.init.zeros_(self.head.bias)\n",
        "            else:\n",
        "                self.head.weight.copy_(np2th(weights[\"head/kernel\"]).t())\n",
        "                self.head.bias.copy_(np2th(weights[\"head/bias\"]).t())\n",
        "\n",
        "            self.transformer.embeddings.patch_embeddings.weight.copy_(np2th(weights[\"embedding/kernel\"], conv=True))\n",
        "            self.transformer.embeddings.patch_embeddings.bias.copy_(np2th(weights[\"embedding/bias\"]))\n",
        "            self.transformer.embeddings.cls_token.copy_(np2th(weights[\"cls\"]))\n",
        "            self.transformer.encoder.encoder_norm.weight.copy_(np2th(weights[\"Transformer/encoder_norm/scale\"]))\n",
        "            self.transformer.encoder.encoder_norm.bias.copy_(np2th(weights[\"Transformer/encoder_norm/bias\"]))\n",
        "\n",
        "            posemb = np2th(weights[\"Transformer/posembed_input/pos_embedding\"])\n",
        "            posemb_new = self.transformer.embeddings.position_embeddings\n",
        "            if posemb.size() == posemb_new.size():\n",
        "                self.transformer.embeddings.position_embeddings.copy_(posemb)\n",
        "            else:\n",
        "                logger.info(\"load_pretrained: resized variant: %s to %s\" % (posemb.size(), posemb_new.size()))\n",
        "                ntok_new = posemb_new.size(1)\n",
        "\n",
        "                if self.classifier == \"token\":\n",
        "                    posemb_tok, posemb_grid = posemb[:, :1], posemb[0, 1:]\n",
        "                    ntok_new -= 1\n",
        "                else:\n",
        "                    posemb_tok, posemb_grid = posemb[:, :0], posemb[0]\n",
        "\n",
        "                gs_old = int(np.sqrt(len(posemb_grid)))\n",
        "                gs_new = int(np.sqrt(ntok_new))\n",
        "                print('load_pretrained: grid-size from %s to %s' % (gs_old, gs_new))\n",
        "                posemb_grid = posemb_grid.reshape(gs_old, gs_old, -1)\n",
        "\n",
        "                zoom = (gs_new / gs_old, gs_new / gs_old, 1)\n",
        "                posemb_grid = ndimage.zoom(posemb_grid, zoom, order=1)\n",
        "                posemb_grid = posemb_grid.reshape(1, gs_new * gs_new, -1)\n",
        "                posemb = np.concatenate([posemb_tok, posemb_grid], axis=1)\n",
        "                self.transformer.embeddings.position_embeddings.copy_(np2th(posemb))\n",
        "\n",
        "            for bname, block in self.transformer.encoder.named_children():\n",
        "                for uname, unit in block.named_children():\n",
        "                    unit.load_from(weights, n_block=uname)\n",
        "\n",
        "            if self.transformer.embeddings.hybrid:\n",
        "                self.transformer.embeddings.hybrid_model.root.conv.weight.copy_(np2th(weights[\"conv_root/kernel\"], conv=True))\n",
        "                gn_weight = np2th(weights[\"gn_root/scale\"]).view(-1)\n",
        "                gn_bias = np2th(weights[\"gn_root/bias\"]).view(-1)\n",
        "                self.transformer.embeddings.hybrid_model.root.gn.weight.copy_(gn_weight)\n",
        "                self.transformer.embeddings.hybrid_model.root.gn.bias.copy_(gn_bias)\n",
        "\n",
        "                for bname, block in self.transformer.embeddings.hybrid_model.body.named_children():\n",
        "                    for uname, unit in block.named_children():\n",
        "                        unit.load_from(weights, n_block=bname, n_unit=uname)\n",
        "\n",
        "\n",
        "\n",
        "import ml_collections\n",
        "def get_b16_config():\n",
        "    \"\"\"Returns the ViT-B/16 configuration.\"\"\"\n",
        "    config = ml_collections.ConfigDict()\n",
        "    config.patches = ml_collections.ConfigDict({'size': (16, 16)})\n",
        "    config.hidden_size = 768\n",
        "    config.transformer = ml_collections.ConfigDict()\n",
        "    config.transformer.mlp_dim = 3072\n",
        "    config.transformer.num_heads = 12\n",
        "    config.transformer.num_layers = 12\n",
        "    config.transformer.attention_dropout_rate = 0.0\n",
        "    config.transformer.dropout_rate = 0.1\n",
        "    config.classifier = 'token'\n",
        "    config.representation_size = None\n",
        "    return config\n",
        "\n",
        "def get_r50_b16_config():\n",
        "    \"\"\"Returns the Resnet50 + ViT-B/16 configuration.\"\"\"\n",
        "    config = get_b16_config()\n",
        "    del config.patches.size\n",
        "    config.patches.grid = (14, 14)\n",
        "    config.resnet = ml_collections.ConfigDict()\n",
        "    config.resnet.num_layers = (3, 4, 9)\n",
        "    config.resnet.width_factor = 1\n",
        "    return config"
      ],
      "metadata": {
        "id": "bKAVtKLhc4Su"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GradCAM"
      ],
      "metadata": {
        "id": "lzXG3mV2dqeh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "import torch.nn.functional as F\n",
        "\n",
        "def reg_hook_gradcam(last_conv_layer=None):\n",
        "    # gradients for gradcam are stored here\n",
        "    stored_grads = torch.Tensor([])\n",
        "    stored_fpass = torch.Tensor([])\n",
        "\n",
        "    def bpass_hook(self, gin, gout):\n",
        "        global stored_grads\n",
        "        stored_grads = gout\n",
        "\n",
        "    def fpass_hook(self, ten_in, ten_out):\n",
        "        global stored_fpass\n",
        "        stored_fpass = ten_out\n",
        "        \n",
        "    handle_b = last_conv_layer.register_backward_hook(bpass_hook)\n",
        "    handle_f = last_conv_layer.register_forward_hook(fpass_hook)\n",
        "\n",
        "def reshape_transform(tensor, height=14, width=14):\n",
        "    result = tensor[:, 1:, :].reshape(tensor.size(0),\n",
        "                                      height, width, tensor.size(2))\n",
        "    # Bring the channels to the first dimension like in CNNs.\n",
        "    result = result.transpose(2, 3).transpose(1, 2)\n",
        "    return result\n",
        "\n",
        "def gradcam_pp(grads, activations):\n",
        "    grads_power_2 = grads**2\n",
        "    grads_power_3 = grads_power_2 * grads\n",
        "    # Equation 19 in https://arxiv.org/abs/1710.11063\n",
        "    sum_activations = torch.sum(activations, dim=[ 2, 3])\n",
        "    eps = 0.000001\n",
        "    aij = grads_power_2 / (2 * grads_power_2 +\n",
        "                            sum_activations[:, :, None, None] * grads_power_3 + eps)\n",
        "    # Now bring back the ReLU from eq.7 in the paper,\n",
        "    # And zero out aijs where the activations are 0\n",
        "    aij = torch.where(grads != 0, aij, 0)\n",
        "\n",
        "    weights = torch.maximum(grads, torch.tensor(0)) * aij\n",
        "    weights = torch.sum(weights, dim=[ 2, 3])\n",
        "    return weights\n",
        "\n",
        "def get_gradcam(model, img, lab=None, gradcampp=False):\n",
        "    model.eval()\n",
        "    last_conv_layer = model.transformer.encoder.layer[11].attention_norm\n",
        "    reg_hook_gradcam(last_conv_layer=last_conv_layer)\n",
        "    out, feature_map = model(img)\n",
        "    feature_map = reshape_transform(feature_map)\n",
        "    out = torch.sigmoid(out)\n",
        "    #pred_labels = torch.argwhere(out>0.5)[1]\n",
        "    pred_labels = torch.argwhere(out.detach().squeeze()>0.5)\n",
        "    print('Probability:',[round(tl.item(), 3) for tl in out.squeeze()],'\\nPredictions:', [pl.item() for pl in pred_labels.squeeze()])\n",
        "    pred_onehot = F.one_hot(pred_labels, num_classes=11)\n",
        "    feature_map_all = []\n",
        "    for pred_onehot_each in pred_onehot:\n",
        "        out.backward(pred_onehot_each, retain_graph=True)\n",
        "        gradients = stored_grads[0].clone()\n",
        "        activations = stored_fpass[0].clone().unsqueeze(0)\n",
        "        activations = activations.detach()\n",
        "\n",
        "        gradients = reshape_transform(gradients)\n",
        "        activations = reshape_transform(activations)\n",
        "\n",
        "        if gradcampp:\n",
        "            pooled_gradients = gradcam_pp(gradients, activations)\n",
        "        else:\n",
        "            pooled_gradients = torch.mean(gradients, dim=[2, 3])\n",
        "        activations *= pooled_gradients[:,:,None, None]\n",
        "\n",
        "        heatmap = torch.sum(activations, dim=1).squeeze().cpu()\n",
        "        heatmap = np.maximum(heatmap, 0)\n",
        "        heatmap /= torch.max(heatmap)\n",
        "        hmap = heatmap.numpy()\n",
        "        hmap[hmap<0.60] = 0\n",
        "        hmap_rep = hmap[None].repeat(feature_map.shape[1], axis=0)\n",
        "        feature_map_class = feature_map.squeeze()[hmap_rep>0]\n",
        "        feature_map_class_final = F.adaptive_avg_pool1d(torch.tensor(feature_map_class[None]),[512])\n",
        "        feature_map_all.append(feature_map_class_final)\n",
        "\n",
        "    return torch.stack(feature_map_all), pred_labels"
      ],
      "metadata": {
        "id": "eWxsVH8cdg1A"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "End-to-End GCN"
      ],
      "metadata": {
        "id": "b7hV4ZliemPt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from PIL import Image\n",
        "from torchvision import transforms as tf\n",
        "import torch.optim as optim\n",
        "\n",
        "class GadientBased_MCC(nn.Module):\n",
        "    def __init__(self, num_classes=11, checkpoint_path=None):\n",
        "        super(GadientBased_MCC, self).__init__()\n",
        "        config = get_r50_b16_config()\n",
        "        pretrained_dir = 'R50+ViT-B_16.npz' # add your trained weights\n",
        "        self.GradientModel = VisionTransformer(config, 224, zero_head=True, num_classes=num_classes)\n",
        "        #self.GradientModel.load_from(np.load(pretrained_dir))\n",
        "        #self.GradientModel.head = nn.Linear(self.GradientModel.head.in_features, num_classes) \n",
        "\n",
        "        self.GradientModel.eval()\n",
        "        self.mcc = nn.Linear(512, num_classes)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        feature_map_all, pred_labels = get_gradcam(self.GradientModel, x, gradcampp=True)\n",
        "        \n",
        "        out = self.mcc(feature_map_all)\n",
        "        return out, pred_labels\n",
        "\n",
        "def freeze_module(model_mcc):\n",
        "    for name, param in model_mcc.GradientModel.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            if len(name.split('.'))>3 and name.split('.')[3].isdigit():\n",
        "                name = name.replace('.{}.'.format(name.split('.')[3]),'[{}].'.format(name.split('.')[3]))\n",
        "                if name.split('.')[2].isdigit():\n",
        "                    name = name.replace('.{}.'.format(name.split('.')[2]),'[{}].'.format(name.split('.')[2]))\n",
        "\n",
        "            my_code = 'model_mcc.GradientModel.'+ name + '.register_hook(lambda grad:grad.mul_(0))'\n",
        "            exec(my_code)\n",
        "\n",
        "    \n",
        "model_mcc = GadientBased_MCC(num_classes=11).cuda()\n",
        "\n",
        "image = Image.open(\"localization/1_frame146.png\").convert('RGB')\n",
        "transforms = tf.Compose([tf.Resize((224,224)), \n",
        "                        tf.ToTensor(),\n",
        "                        tf.Normalize([0.4084945, 0.25513682, 0.25353566], [0.22662906, 0.20201652, 0.1962526 ])])\n",
        "\n",
        "img = transforms(image).cuda()\n",
        "#out = model_mcc(img[None]).cuda()\n",
        "lab = torch.tensor([0,3,5,10]).cuda()\n",
        "optimizer = optim.SGD(model_mcc.parameters(), lr=0.1, momentum=0.9, nesterov=False, weight_decay=0.0001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "freeze_module(model_mcc)\n",
        "\n",
        "for i in range(2):\n",
        "    print('model_mcc.GradientModel.blocks[0].attn.qkv.weight:',model_mcc.GradientModel.transformer.encoder.layer[0].ffn.fc1.weight[0,:5])\n",
        "    out, pred_labels = model_mcc(img[None])\n",
        "    print(out.shape, lab.shape, pred_labels.shape)\n",
        "    loss = criterion(out.squeeze(), pred_labels.squeeze())\n",
        "    print('loss', loss.item())\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAPRQY_oedwR",
        "outputId": "d11f6e75-e7ce-4753-eba3-c8f85278853f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_mcc.GradientModel.blocks[0].attn.qkv.weight: tensor([-0.0157, -0.0200,  0.0190,  0.0180,  0.0175], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "Probability: [0.517, 0.337, 0.745, 0.448, 0.424, 0.665, 0.579, 0.343, 0.417, 0.615, 0.538] \n",
            "Predictions: [0, 2, 5, 6, 9, 10]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([6, 1, 11]) torch.Size([4]) torch.Size([6, 1])\n",
            "loss 2.529580593109131\n",
            "model_mcc.GradientModel.blocks[0].attn.qkv.weight: tensor([-0.0157, -0.0200,  0.0190,  0.0180,  0.0175], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "Probability: [0.517, 0.337, 0.745, 0.448, 0.424, 0.665, 0.579, 0.343, 0.417, 0.615, 0.538] \n",
            "Predictions: [0, 2, 5, 6, 9, 10]\n",
            "torch.Size([6, 1, 11]) torch.Size([4]) torch.Size([6, 1])\n",
            "loss 1.6685322523117065\n",
            "torch.Size([6, 1, 11])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model_mcc.GradientModel.named_parameters():\n",
        "  if param.requires_grad:\n",
        "    print(name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MD0gVPSnfrkj",
        "outputId": "c29ee554-a96c-4690-8976-160d0022d4f1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "transformer.embeddings.position_embeddings\n",
            "transformer.embeddings.cls_token\n",
            "transformer.embeddings.hybrid_model.root.conv.weight\n",
            "transformer.embeddings.hybrid_model.root.gn.weight\n",
            "transformer.embeddings.hybrid_model.root.gn.bias\n",
            "transformer.embeddings.hybrid_model.body.block1.unit1.gn1.weight\n",
            "transformer.embeddings.hybrid_model.body.block1.unit1.gn1.bias\n",
            "transformer.embeddings.hybrid_model.body.block1.unit1.conv1.weight\n",
            "transformer.embeddings.hybrid_model.body.block1.unit1.gn2.weight\n",
            "transformer.embeddings.hybrid_model.body.block1.unit1.gn2.bias\n",
            "transformer.embeddings.hybrid_model.body.block1.unit1.conv2.weight\n",
            "transformer.embeddings.hybrid_model.body.block1.unit1.gn3.weight\n",
            "transformer.embeddings.hybrid_model.body.block1.unit1.gn3.bias\n",
            "transformer.embeddings.hybrid_model.body.block1.unit1.conv3.weight\n",
            "transformer.embeddings.hybrid_model.body.block1.unit1.downsample.weight\n",
            "transformer.embeddings.hybrid_model.body.block1.unit1.gn_proj.weight\n",
            "transformer.embeddings.hybrid_model.body.block1.unit1.gn_proj.bias\n",
            "transformer.embeddings.hybrid_model.body.block1.unit2.gn1.weight\n",
            "transformer.embeddings.hybrid_model.body.block1.unit2.gn1.bias\n",
            "transformer.embeddings.hybrid_model.body.block1.unit2.conv1.weight\n",
            "transformer.embeddings.hybrid_model.body.block1.unit2.gn2.weight\n",
            "transformer.embeddings.hybrid_model.body.block1.unit2.gn2.bias\n",
            "transformer.embeddings.hybrid_model.body.block1.unit2.conv2.weight\n",
            "transformer.embeddings.hybrid_model.body.block1.unit2.gn3.weight\n",
            "transformer.embeddings.hybrid_model.body.block1.unit2.gn3.bias\n",
            "transformer.embeddings.hybrid_model.body.block1.unit2.conv3.weight\n",
            "transformer.embeddings.hybrid_model.body.block1.unit3.gn1.weight\n",
            "transformer.embeddings.hybrid_model.body.block1.unit3.gn1.bias\n",
            "transformer.embeddings.hybrid_model.body.block1.unit3.conv1.weight\n",
            "transformer.embeddings.hybrid_model.body.block1.unit3.gn2.weight\n",
            "transformer.embeddings.hybrid_model.body.block1.unit3.gn2.bias\n",
            "transformer.embeddings.hybrid_model.body.block1.unit3.conv2.weight\n",
            "transformer.embeddings.hybrid_model.body.block1.unit3.gn3.weight\n",
            "transformer.embeddings.hybrid_model.body.block1.unit3.gn3.bias\n",
            "transformer.embeddings.hybrid_model.body.block1.unit3.conv3.weight\n",
            "transformer.embeddings.hybrid_model.body.block2.unit1.gn1.weight\n",
            "transformer.embeddings.hybrid_model.body.block2.unit1.gn1.bias\n",
            "transformer.embeddings.hybrid_model.body.block2.unit1.conv1.weight\n",
            "transformer.embeddings.hybrid_model.body.block2.unit1.gn2.weight\n",
            "transformer.embeddings.hybrid_model.body.block2.unit1.gn2.bias\n",
            "transformer.embeddings.hybrid_model.body.block2.unit1.conv2.weight\n",
            "transformer.embeddings.hybrid_model.body.block2.unit1.gn3.weight\n",
            "transformer.embeddings.hybrid_model.body.block2.unit1.gn3.bias\n",
            "transformer.embeddings.hybrid_model.body.block2.unit1.conv3.weight\n",
            "transformer.embeddings.hybrid_model.body.block2.unit1.downsample.weight\n",
            "transformer.embeddings.hybrid_model.body.block2.unit1.gn_proj.weight\n",
            "transformer.embeddings.hybrid_model.body.block2.unit1.gn_proj.bias\n",
            "transformer.embeddings.hybrid_model.body.block2.unit2.gn1.weight\n",
            "transformer.embeddings.hybrid_model.body.block2.unit2.gn1.bias\n",
            "transformer.embeddings.hybrid_model.body.block2.unit2.conv1.weight\n",
            "transformer.embeddings.hybrid_model.body.block2.unit2.gn2.weight\n",
            "transformer.embeddings.hybrid_model.body.block2.unit2.gn2.bias\n",
            "transformer.embeddings.hybrid_model.body.block2.unit2.conv2.weight\n",
            "transformer.embeddings.hybrid_model.body.block2.unit2.gn3.weight\n",
            "transformer.embeddings.hybrid_model.body.block2.unit2.gn3.bias\n",
            "transformer.embeddings.hybrid_model.body.block2.unit2.conv3.weight\n",
            "transformer.embeddings.hybrid_model.body.block2.unit3.gn1.weight\n",
            "transformer.embeddings.hybrid_model.body.block2.unit3.gn1.bias\n",
            "transformer.embeddings.hybrid_model.body.block2.unit3.conv1.weight\n",
            "transformer.embeddings.hybrid_model.body.block2.unit3.gn2.weight\n",
            "transformer.embeddings.hybrid_model.body.block2.unit3.gn2.bias\n",
            "transformer.embeddings.hybrid_model.body.block2.unit3.conv2.weight\n",
            "transformer.embeddings.hybrid_model.body.block2.unit3.gn3.weight\n",
            "transformer.embeddings.hybrid_model.body.block2.unit3.gn3.bias\n",
            "transformer.embeddings.hybrid_model.body.block2.unit3.conv3.weight\n",
            "transformer.embeddings.hybrid_model.body.block2.unit4.gn1.weight\n",
            "transformer.embeddings.hybrid_model.body.block2.unit4.gn1.bias\n",
            "transformer.embeddings.hybrid_model.body.block2.unit4.conv1.weight\n",
            "transformer.embeddings.hybrid_model.body.block2.unit4.gn2.weight\n",
            "transformer.embeddings.hybrid_model.body.block2.unit4.gn2.bias\n",
            "transformer.embeddings.hybrid_model.body.block2.unit4.conv2.weight\n",
            "transformer.embeddings.hybrid_model.body.block2.unit4.gn3.weight\n",
            "transformer.embeddings.hybrid_model.body.block2.unit4.gn3.bias\n",
            "transformer.embeddings.hybrid_model.body.block2.unit4.conv3.weight\n",
            "transformer.embeddings.hybrid_model.body.block3.unit1.gn1.weight\n",
            "transformer.embeddings.hybrid_model.body.block3.unit1.gn1.bias\n",
            "transformer.embeddings.hybrid_model.body.block3.unit1.conv1.weight\n",
            "transformer.embeddings.hybrid_model.body.block3.unit1.gn2.weight\n",
            "transformer.embeddings.hybrid_model.body.block3.unit1.gn2.bias\n",
            "transformer.embeddings.hybrid_model.body.block3.unit1.conv2.weight\n",
            "transformer.embeddings.hybrid_model.body.block3.unit1.gn3.weight\n",
            "transformer.embeddings.hybrid_model.body.block3.unit1.gn3.bias\n",
            "transformer.embeddings.hybrid_model.body.block3.unit1.conv3.weight\n",
            "transformer.embeddings.hybrid_model.body.block3.unit1.downsample.weight\n",
            "transformer.embeddings.hybrid_model.body.block3.unit1.gn_proj.weight\n",
            "transformer.embeddings.hybrid_model.body.block3.unit1.gn_proj.bias\n",
            "transformer.embeddings.hybrid_model.body.block3.unit2.gn1.weight\n",
            "transformer.embeddings.hybrid_model.body.block3.unit2.gn1.bias\n",
            "transformer.embeddings.hybrid_model.body.block3.unit2.conv1.weight\n",
            "transformer.embeddings.hybrid_model.body.block3.unit2.gn2.weight\n",
            "transformer.embeddings.hybrid_model.body.block3.unit2.gn2.bias\n",
            "transformer.embeddings.hybrid_model.body.block3.unit2.conv2.weight\n",
            "transformer.embeddings.hybrid_model.body.block3.unit2.gn3.weight\n",
            "transformer.embeddings.hybrid_model.body.block3.unit2.gn3.bias\n",
            "transformer.embeddings.hybrid_model.body.block3.unit2.conv3.weight\n",
            "transformer.embeddings.hybrid_model.body.block3.unit3.gn1.weight\n",
            "transformer.embeddings.hybrid_model.body.block3.unit3.gn1.bias\n",
            "transformer.embeddings.hybrid_model.body.block3.unit3.conv1.weight\n",
            "transformer.embeddings.hybrid_model.body.block3.unit3.gn2.weight\n",
            "transformer.embeddings.hybrid_model.body.block3.unit3.gn2.bias\n",
            "transformer.embeddings.hybrid_model.body.block3.unit3.conv2.weight\n",
            "transformer.embeddings.hybrid_model.body.block3.unit3.gn3.weight\n",
            "transformer.embeddings.hybrid_model.body.block3.unit3.gn3.bias\n",
            "transformer.embeddings.hybrid_model.body.block3.unit3.conv3.weight\n",
            "transformer.embeddings.hybrid_model.body.block3.unit4.gn1.weight\n",
            "transformer.embeddings.hybrid_model.body.block3.unit4.gn1.bias\n",
            "transformer.embeddings.hybrid_model.body.block3.unit4.conv1.weight\n",
            "transformer.embeddings.hybrid_model.body.block3.unit4.gn2.weight\n",
            "transformer.embeddings.hybrid_model.body.block3.unit4.gn2.bias\n",
            "transformer.embeddings.hybrid_model.body.block3.unit4.conv2.weight\n",
            "transformer.embeddings.hybrid_model.body.block3.unit4.gn3.weight\n",
            "transformer.embeddings.hybrid_model.body.block3.unit4.gn3.bias\n",
            "transformer.embeddings.hybrid_model.body.block3.unit4.conv3.weight\n",
            "transformer.embeddings.hybrid_model.body.block3.unit5.gn1.weight\n",
            "transformer.embeddings.hybrid_model.body.block3.unit5.gn1.bias\n",
            "transformer.embeddings.hybrid_model.body.block3.unit5.conv1.weight\n",
            "transformer.embeddings.hybrid_model.body.block3.unit5.gn2.weight\n",
            "transformer.embeddings.hybrid_model.body.block3.unit5.gn2.bias\n",
            "transformer.embeddings.hybrid_model.body.block3.unit5.conv2.weight\n",
            "transformer.embeddings.hybrid_model.body.block3.unit5.gn3.weight\n",
            "transformer.embeddings.hybrid_model.body.block3.unit5.gn3.bias\n",
            "transformer.embeddings.hybrid_model.body.block3.unit5.conv3.weight\n",
            "transformer.embeddings.hybrid_model.body.block3.unit6.gn1.weight\n",
            "transformer.embeddings.hybrid_model.body.block3.unit6.gn1.bias\n",
            "transformer.embeddings.hybrid_model.body.block3.unit6.conv1.weight\n",
            "transformer.embeddings.hybrid_model.body.block3.unit6.gn2.weight\n",
            "transformer.embeddings.hybrid_model.body.block3.unit6.gn2.bias\n",
            "transformer.embeddings.hybrid_model.body.block3.unit6.conv2.weight\n",
            "transformer.embeddings.hybrid_model.body.block3.unit6.gn3.weight\n",
            "transformer.embeddings.hybrid_model.body.block3.unit6.gn3.bias\n",
            "transformer.embeddings.hybrid_model.body.block3.unit6.conv3.weight\n",
            "transformer.embeddings.hybrid_model.body.block3.unit7.gn1.weight\n",
            "transformer.embeddings.hybrid_model.body.block3.unit7.gn1.bias\n",
            "transformer.embeddings.hybrid_model.body.block3.unit7.conv1.weight\n",
            "transformer.embeddings.hybrid_model.body.block3.unit7.gn2.weight\n",
            "transformer.embeddings.hybrid_model.body.block3.unit7.gn2.bias\n",
            "transformer.embeddings.hybrid_model.body.block3.unit7.conv2.weight\n",
            "transformer.embeddings.hybrid_model.body.block3.unit7.gn3.weight\n",
            "transformer.embeddings.hybrid_model.body.block3.unit7.gn3.bias\n",
            "transformer.embeddings.hybrid_model.body.block3.unit7.conv3.weight\n",
            "transformer.embeddings.hybrid_model.body.block3.unit8.gn1.weight\n",
            "transformer.embeddings.hybrid_model.body.block3.unit8.gn1.bias\n",
            "transformer.embeddings.hybrid_model.body.block3.unit8.conv1.weight\n",
            "transformer.embeddings.hybrid_model.body.block3.unit8.gn2.weight\n",
            "transformer.embeddings.hybrid_model.body.block3.unit8.gn2.bias\n",
            "transformer.embeddings.hybrid_model.body.block3.unit8.conv2.weight\n",
            "transformer.embeddings.hybrid_model.body.block3.unit8.gn3.weight\n",
            "transformer.embeddings.hybrid_model.body.block3.unit8.gn3.bias\n",
            "transformer.embeddings.hybrid_model.body.block3.unit8.conv3.weight\n",
            "transformer.embeddings.hybrid_model.body.block3.unit9.gn1.weight\n",
            "transformer.embeddings.hybrid_model.body.block3.unit9.gn1.bias\n",
            "transformer.embeddings.hybrid_model.body.block3.unit9.conv1.weight\n",
            "transformer.embeddings.hybrid_model.body.block3.unit9.gn2.weight\n",
            "transformer.embeddings.hybrid_model.body.block3.unit9.gn2.bias\n",
            "transformer.embeddings.hybrid_model.body.block3.unit9.conv2.weight\n",
            "transformer.embeddings.hybrid_model.body.block3.unit9.gn3.weight\n",
            "transformer.embeddings.hybrid_model.body.block3.unit9.gn3.bias\n",
            "transformer.embeddings.hybrid_model.body.block3.unit9.conv3.weight\n",
            "transformer.embeddings.patch_embeddings.weight\n",
            "transformer.embeddings.patch_embeddings.bias\n",
            "transformer.encoder.layer.0.attention_norm.weight\n",
            "transformer.encoder.layer.0.attention_norm.bias\n",
            "transformer.encoder.layer.0.ffn_norm.weight\n",
            "transformer.encoder.layer.0.ffn_norm.bias\n",
            "transformer.encoder.layer.0.ffn.fc1.weight\n",
            "transformer.encoder.layer.0.ffn.fc1.bias\n",
            "transformer.encoder.layer.0.ffn.fc2.weight\n",
            "transformer.encoder.layer.0.ffn.fc2.bias\n",
            "transformer.encoder.layer.0.attn.query.weight\n",
            "transformer.encoder.layer.0.attn.query.bias\n",
            "transformer.encoder.layer.0.attn.key.weight\n",
            "transformer.encoder.layer.0.attn.key.bias\n",
            "transformer.encoder.layer.0.attn.value.weight\n",
            "transformer.encoder.layer.0.attn.value.bias\n",
            "transformer.encoder.layer.0.attn.out.weight\n",
            "transformer.encoder.layer.0.attn.out.bias\n",
            "transformer.encoder.layer.1.attention_norm.weight\n",
            "transformer.encoder.layer.1.attention_norm.bias\n",
            "transformer.encoder.layer.1.ffn_norm.weight\n",
            "transformer.encoder.layer.1.ffn_norm.bias\n",
            "transformer.encoder.layer.1.ffn.fc1.weight\n",
            "transformer.encoder.layer.1.ffn.fc1.bias\n",
            "transformer.encoder.layer.1.ffn.fc2.weight\n",
            "transformer.encoder.layer.1.ffn.fc2.bias\n",
            "transformer.encoder.layer.1.attn.query.weight\n",
            "transformer.encoder.layer.1.attn.query.bias\n",
            "transformer.encoder.layer.1.attn.key.weight\n",
            "transformer.encoder.layer.1.attn.key.bias\n",
            "transformer.encoder.layer.1.attn.value.weight\n",
            "transformer.encoder.layer.1.attn.value.bias\n",
            "transformer.encoder.layer.1.attn.out.weight\n",
            "transformer.encoder.layer.1.attn.out.bias\n",
            "transformer.encoder.layer.2.attention_norm.weight\n",
            "transformer.encoder.layer.2.attention_norm.bias\n",
            "transformer.encoder.layer.2.ffn_norm.weight\n",
            "transformer.encoder.layer.2.ffn_norm.bias\n",
            "transformer.encoder.layer.2.ffn.fc1.weight\n",
            "transformer.encoder.layer.2.ffn.fc1.bias\n",
            "transformer.encoder.layer.2.ffn.fc2.weight\n",
            "transformer.encoder.layer.2.ffn.fc2.bias\n",
            "transformer.encoder.layer.2.attn.query.weight\n",
            "transformer.encoder.layer.2.attn.query.bias\n",
            "transformer.encoder.layer.2.attn.key.weight\n",
            "transformer.encoder.layer.2.attn.key.bias\n",
            "transformer.encoder.layer.2.attn.value.weight\n",
            "transformer.encoder.layer.2.attn.value.bias\n",
            "transformer.encoder.layer.2.attn.out.weight\n",
            "transformer.encoder.layer.2.attn.out.bias\n",
            "transformer.encoder.layer.3.attention_norm.weight\n",
            "transformer.encoder.layer.3.attention_norm.bias\n",
            "transformer.encoder.layer.3.ffn_norm.weight\n",
            "transformer.encoder.layer.3.ffn_norm.bias\n",
            "transformer.encoder.layer.3.ffn.fc1.weight\n",
            "transformer.encoder.layer.3.ffn.fc1.bias\n",
            "transformer.encoder.layer.3.ffn.fc2.weight\n",
            "transformer.encoder.layer.3.ffn.fc2.bias\n",
            "transformer.encoder.layer.3.attn.query.weight\n",
            "transformer.encoder.layer.3.attn.query.bias\n",
            "transformer.encoder.layer.3.attn.key.weight\n",
            "transformer.encoder.layer.3.attn.key.bias\n",
            "transformer.encoder.layer.3.attn.value.weight\n",
            "transformer.encoder.layer.3.attn.value.bias\n",
            "transformer.encoder.layer.3.attn.out.weight\n",
            "transformer.encoder.layer.3.attn.out.bias\n",
            "transformer.encoder.layer.4.attention_norm.weight\n",
            "transformer.encoder.layer.4.attention_norm.bias\n",
            "transformer.encoder.layer.4.ffn_norm.weight\n",
            "transformer.encoder.layer.4.ffn_norm.bias\n",
            "transformer.encoder.layer.4.ffn.fc1.weight\n",
            "transformer.encoder.layer.4.ffn.fc1.bias\n",
            "transformer.encoder.layer.4.ffn.fc2.weight\n",
            "transformer.encoder.layer.4.ffn.fc2.bias\n",
            "transformer.encoder.layer.4.attn.query.weight\n",
            "transformer.encoder.layer.4.attn.query.bias\n",
            "transformer.encoder.layer.4.attn.key.weight\n",
            "transformer.encoder.layer.4.attn.key.bias\n",
            "transformer.encoder.layer.4.attn.value.weight\n",
            "transformer.encoder.layer.4.attn.value.bias\n",
            "transformer.encoder.layer.4.attn.out.weight\n",
            "transformer.encoder.layer.4.attn.out.bias\n",
            "transformer.encoder.layer.5.attention_norm.weight\n",
            "transformer.encoder.layer.5.attention_norm.bias\n",
            "transformer.encoder.layer.5.ffn_norm.weight\n",
            "transformer.encoder.layer.5.ffn_norm.bias\n",
            "transformer.encoder.layer.5.ffn.fc1.weight\n",
            "transformer.encoder.layer.5.ffn.fc1.bias\n",
            "transformer.encoder.layer.5.ffn.fc2.weight\n",
            "transformer.encoder.layer.5.ffn.fc2.bias\n",
            "transformer.encoder.layer.5.attn.query.weight\n",
            "transformer.encoder.layer.5.attn.query.bias\n",
            "transformer.encoder.layer.5.attn.key.weight\n",
            "transformer.encoder.layer.5.attn.key.bias\n",
            "transformer.encoder.layer.5.attn.value.weight\n",
            "transformer.encoder.layer.5.attn.value.bias\n",
            "transformer.encoder.layer.5.attn.out.weight\n",
            "transformer.encoder.layer.5.attn.out.bias\n",
            "transformer.encoder.layer.6.attention_norm.weight\n",
            "transformer.encoder.layer.6.attention_norm.bias\n",
            "transformer.encoder.layer.6.ffn_norm.weight\n",
            "transformer.encoder.layer.6.ffn_norm.bias\n",
            "transformer.encoder.layer.6.ffn.fc1.weight\n",
            "transformer.encoder.layer.6.ffn.fc1.bias\n",
            "transformer.encoder.layer.6.ffn.fc2.weight\n",
            "transformer.encoder.layer.6.ffn.fc2.bias\n",
            "transformer.encoder.layer.6.attn.query.weight\n",
            "transformer.encoder.layer.6.attn.query.bias\n",
            "transformer.encoder.layer.6.attn.key.weight\n",
            "transformer.encoder.layer.6.attn.key.bias\n",
            "transformer.encoder.layer.6.attn.value.weight\n",
            "transformer.encoder.layer.6.attn.value.bias\n",
            "transformer.encoder.layer.6.attn.out.weight\n",
            "transformer.encoder.layer.6.attn.out.bias\n",
            "transformer.encoder.layer.7.attention_norm.weight\n",
            "transformer.encoder.layer.7.attention_norm.bias\n",
            "transformer.encoder.layer.7.ffn_norm.weight\n",
            "transformer.encoder.layer.7.ffn_norm.bias\n",
            "transformer.encoder.layer.7.ffn.fc1.weight\n",
            "transformer.encoder.layer.7.ffn.fc1.bias\n",
            "transformer.encoder.layer.7.ffn.fc2.weight\n",
            "transformer.encoder.layer.7.ffn.fc2.bias\n",
            "transformer.encoder.layer.7.attn.query.weight\n",
            "transformer.encoder.layer.7.attn.query.bias\n",
            "transformer.encoder.layer.7.attn.key.weight\n",
            "transformer.encoder.layer.7.attn.key.bias\n",
            "transformer.encoder.layer.7.attn.value.weight\n",
            "transformer.encoder.layer.7.attn.value.bias\n",
            "transformer.encoder.layer.7.attn.out.weight\n",
            "transformer.encoder.layer.7.attn.out.bias\n",
            "transformer.encoder.layer.8.attention_norm.weight\n",
            "transformer.encoder.layer.8.attention_norm.bias\n",
            "transformer.encoder.layer.8.ffn_norm.weight\n",
            "transformer.encoder.layer.8.ffn_norm.bias\n",
            "transformer.encoder.layer.8.ffn.fc1.weight\n",
            "transformer.encoder.layer.8.ffn.fc1.bias\n",
            "transformer.encoder.layer.8.ffn.fc2.weight\n",
            "transformer.encoder.layer.8.ffn.fc2.bias\n",
            "transformer.encoder.layer.8.attn.query.weight\n",
            "transformer.encoder.layer.8.attn.query.bias\n",
            "transformer.encoder.layer.8.attn.key.weight\n",
            "transformer.encoder.layer.8.attn.key.bias\n",
            "transformer.encoder.layer.8.attn.value.weight\n",
            "transformer.encoder.layer.8.attn.value.bias\n",
            "transformer.encoder.layer.8.attn.out.weight\n",
            "transformer.encoder.layer.8.attn.out.bias\n",
            "transformer.encoder.layer.9.attention_norm.weight\n",
            "transformer.encoder.layer.9.attention_norm.bias\n",
            "transformer.encoder.layer.9.ffn_norm.weight\n",
            "transformer.encoder.layer.9.ffn_norm.bias\n",
            "transformer.encoder.layer.9.ffn.fc1.weight\n",
            "transformer.encoder.layer.9.ffn.fc1.bias\n",
            "transformer.encoder.layer.9.ffn.fc2.weight\n",
            "transformer.encoder.layer.9.ffn.fc2.bias\n",
            "transformer.encoder.layer.9.attn.query.weight\n",
            "transformer.encoder.layer.9.attn.query.bias\n",
            "transformer.encoder.layer.9.attn.key.weight\n",
            "transformer.encoder.layer.9.attn.key.bias\n",
            "transformer.encoder.layer.9.attn.value.weight\n",
            "transformer.encoder.layer.9.attn.value.bias\n",
            "transformer.encoder.layer.9.attn.out.weight\n",
            "transformer.encoder.layer.9.attn.out.bias\n",
            "transformer.encoder.layer.10.attention_norm.weight\n",
            "transformer.encoder.layer.10.attention_norm.bias\n",
            "transformer.encoder.layer.10.ffn_norm.weight\n",
            "transformer.encoder.layer.10.ffn_norm.bias\n",
            "transformer.encoder.layer.10.ffn.fc1.weight\n",
            "transformer.encoder.layer.10.ffn.fc1.bias\n",
            "transformer.encoder.layer.10.ffn.fc2.weight\n",
            "transformer.encoder.layer.10.ffn.fc2.bias\n",
            "transformer.encoder.layer.10.attn.query.weight\n",
            "transformer.encoder.layer.10.attn.query.bias\n",
            "transformer.encoder.layer.10.attn.key.weight\n",
            "transformer.encoder.layer.10.attn.key.bias\n",
            "transformer.encoder.layer.10.attn.value.weight\n",
            "transformer.encoder.layer.10.attn.value.bias\n",
            "transformer.encoder.layer.10.attn.out.weight\n",
            "transformer.encoder.layer.10.attn.out.bias\n",
            "transformer.encoder.layer.11.attention_norm.weight\n",
            "transformer.encoder.layer.11.attention_norm.bias\n",
            "transformer.encoder.layer.11.ffn_norm.weight\n",
            "transformer.encoder.layer.11.ffn_norm.bias\n",
            "transformer.encoder.layer.11.ffn.fc1.weight\n",
            "transformer.encoder.layer.11.ffn.fc1.bias\n",
            "transformer.encoder.layer.11.ffn.fc2.weight\n",
            "transformer.encoder.layer.11.ffn.fc2.bias\n",
            "transformer.encoder.layer.11.attn.query.weight\n",
            "transformer.encoder.layer.11.attn.query.bias\n",
            "transformer.encoder.layer.11.attn.key.weight\n",
            "transformer.encoder.layer.11.attn.key.bias\n",
            "transformer.encoder.layer.11.attn.value.weight\n",
            "transformer.encoder.layer.11.attn.value.bias\n",
            "transformer.encoder.layer.11.attn.out.weight\n",
            "transformer.encoder.layer.11.attn.out.bias\n",
            "transformer.encoder.encoder_norm.weight\n",
            "transformer.encoder.encoder_norm.bias\n",
            "head.weight\n",
            "head.bias\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "4kHZct1IgEqF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}