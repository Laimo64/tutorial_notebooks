{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ViT_Scratch_CIFAR.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPrgjAxOPbc5Se0yoS3vOkY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/tutorial_notebooks/blob/main/ViT_Scratch_CIFAR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ViT from scratch\n",
        "Github: https://github.com/jeonsworld/ViT-pytorch"
      ],
      "metadata": {
        "id": "ijIEty7Hq9zZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip -q install ml_collections \n",
        "! wget https://storage.googleapis.com/vit_models/imagenet21k%2Bimagenet2012/R50%2BViT-B_16.npz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWc86jMulHpl",
        "outputId": "3802644e-a168-4f05-855c-98a4f46c1064"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |████▏                           | 10 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 20 kB 32.1 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 30 kB 37.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 40 kB 40.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 51 kB 39.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 61 kB 43.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 71 kB 34.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 77 kB 5.6 MB/s \n",
            "\u001b[?25h  Building wheel for ml-collections (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "--2022-08-09 08:14:47--  https://storage.googleapis.com/vit_models/imagenet21k%2Bimagenet2012/R50%2BViT-B_16.npz\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.68.128, 74.125.24.128, 172.217.194.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.68.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 395916008 (378M) [application/octet-stream]\n",
            "Saving to: ‘R50+ViT-B_16.npz’\n",
            "\n",
            "R50+ViT-B_16.npz    100%[===================>] 377.57M  46.9MB/s    in 8.7s    \n",
            "\n",
            "2022-08-09 08:14:57 (43.5 MB/s) - ‘R50+ViT-B_16.npz’ saved [395916008/395916008]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ResNet V2"
      ],
      "metadata": {
        "id": "WIno4wd_cyky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "from os.path import join as pjoin\n",
        "\n",
        "from collections import OrderedDict  # pylint: disable=g-importing-member\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def np2th(weights, conv=False):\n",
        "    \"\"\"Possibly convert HWIO to OIHW.\"\"\"\n",
        "    if conv:\n",
        "        weights = weights.transpose([3, 2, 0, 1])\n",
        "    return torch.from_numpy(weights)\n",
        "\n",
        "\n",
        "class StdConv2d(nn.Conv2d):\n",
        "\n",
        "    def forward(self, x):\n",
        "        w = self.weight\n",
        "        v, m = torch.var_mean(w, dim=[1, 2, 3], keepdim=True, unbiased=False)\n",
        "        w = (w - m) / torch.sqrt(v + 1e-5)\n",
        "        return F.conv2d(x, w, self.bias, self.stride, self.padding,\n",
        "                        self.dilation, self.groups)\n",
        "\n",
        "\n",
        "def conv3x3(cin, cout, stride=1, groups=1, bias=False):\n",
        "    return StdConv2d(cin, cout, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=bias, groups=groups)\n",
        "\n",
        "\n",
        "def conv1x1(cin, cout, stride=1, bias=False):\n",
        "    return StdConv2d(cin, cout, kernel_size=1, stride=stride,\n",
        "                     padding=0, bias=bias)\n",
        "\n",
        "\n",
        "class PreActBottleneck(nn.Module):\n",
        "    \"\"\"Pre-activation (v2) bottleneck block.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, cin, cout=None, cmid=None, stride=1):\n",
        "        super().__init__()\n",
        "        cout = cout or cin\n",
        "        cmid = cmid or cout//4\n",
        "\n",
        "        self.gn1 = nn.GroupNorm(32, cmid, eps=1e-6)\n",
        "        self.conv1 = conv1x1(cin, cmid, bias=False)\n",
        "        self.gn2 = nn.GroupNorm(32, cmid, eps=1e-6)\n",
        "        self.conv2 = conv3x3(cmid, cmid, stride, bias=False)  # Original code has it on conv1!!\n",
        "        self.gn3 = nn.GroupNorm(32, cout, eps=1e-6)\n",
        "        self.conv3 = conv1x1(cmid, cout, bias=False)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        if (stride != 1 or cin != cout):\n",
        "            # Projection also with pre-activation according to paper.\n",
        "            self.downsample = conv1x1(cin, cout, stride, bias=False)\n",
        "            self.gn_proj = nn.GroupNorm(cout, cout)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Residual branch\n",
        "        residual = x\n",
        "        if hasattr(self, 'downsample'):\n",
        "            residual = self.downsample(x)\n",
        "            residual = self.gn_proj(residual)\n",
        "\n",
        "        # Unit's branch\n",
        "        y = self.relu(self.gn1(self.conv1(x)))\n",
        "        y = self.relu(self.gn2(self.conv2(y)))\n",
        "        y = self.gn3(self.conv3(y))\n",
        "\n",
        "        y = self.relu(residual + y)\n",
        "        return y\n",
        "\n",
        "    def load_from(self, weights, n_block, n_unit):\n",
        "        conv1_weight = np2th(weights[pjoin(n_block, n_unit, \"conv1/kernel\")], conv=True)\n",
        "        conv2_weight = np2th(weights[pjoin(n_block, n_unit, \"conv2/kernel\")], conv=True)\n",
        "        conv3_weight = np2th(weights[pjoin(n_block, n_unit, \"conv3/kernel\")], conv=True)\n",
        "\n",
        "        gn1_weight = np2th(weights[pjoin(n_block, n_unit, \"gn1/scale\")])\n",
        "        gn1_bias = np2th(weights[pjoin(n_block, n_unit, \"gn1/bias\")])\n",
        "\n",
        "        gn2_weight = np2th(weights[pjoin(n_block, n_unit, \"gn2/scale\")])\n",
        "        gn2_bias = np2th(weights[pjoin(n_block, n_unit, \"gn2/bias\")])\n",
        "\n",
        "        gn3_weight = np2th(weights[pjoin(n_block, n_unit, \"gn3/scale\")])\n",
        "        gn3_bias = np2th(weights[pjoin(n_block, n_unit, \"gn3/bias\")])\n",
        "\n",
        "        self.conv1.weight.copy_(conv1_weight)\n",
        "        self.conv2.weight.copy_(conv2_weight)\n",
        "        self.conv3.weight.copy_(conv3_weight)\n",
        "\n",
        "        self.gn1.weight.copy_(gn1_weight.view(-1))\n",
        "        self.gn1.bias.copy_(gn1_bias.view(-1))\n",
        "\n",
        "        self.gn2.weight.copy_(gn2_weight.view(-1))\n",
        "        self.gn2.bias.copy_(gn2_bias.view(-1))\n",
        "\n",
        "        self.gn3.weight.copy_(gn3_weight.view(-1))\n",
        "        self.gn3.bias.copy_(gn3_bias.view(-1))\n",
        "\n",
        "        if hasattr(self, 'downsample'):\n",
        "            proj_conv_weight = np2th(weights[pjoin(n_block, n_unit, \"conv_proj/kernel\")], conv=True)\n",
        "            proj_gn_weight = np2th(weights[pjoin(n_block, n_unit, \"gn_proj/scale\")])\n",
        "            proj_gn_bias = np2th(weights[pjoin(n_block, n_unit, \"gn_proj/bias\")])\n",
        "\n",
        "            self.downsample.weight.copy_(proj_conv_weight)\n",
        "            self.gn_proj.weight.copy_(proj_gn_weight.view(-1))\n",
        "            self.gn_proj.bias.copy_(proj_gn_bias.view(-1))\n",
        "\n",
        "class ResNetV2(nn.Module):\n",
        "    \"\"\"Implementation of Pre-activation (v2) ResNet mode.\"\"\"\n",
        "\n",
        "    def __init__(self, block_units, width_factor):\n",
        "        super().__init__()\n",
        "        width = int(64 * width_factor)\n",
        "        self.width = width\n",
        "\n",
        "        # The following will be unreadable if we split lines.\n",
        "        # pylint: disable=line-too-long\n",
        "        self.root = nn.Sequential(OrderedDict([\n",
        "            ('conv', StdConv2d(3, width, kernel_size=7, stride=2, bias=False, padding=3)),\n",
        "            ('gn', nn.GroupNorm(32, width, eps=1e-6)),\n",
        "            ('relu', nn.ReLU(inplace=True)),\n",
        "            ('pool', nn.MaxPool2d(kernel_size=3, stride=2, padding=0))\n",
        "        ]))\n",
        "\n",
        "        self.body = nn.Sequential(OrderedDict([\n",
        "            ('block1', nn.Sequential(OrderedDict(\n",
        "                [('unit1', PreActBottleneck(cin=width, cout=width*4, cmid=width))] +\n",
        "                [(f'unit{i:d}', PreActBottleneck(cin=width*4, cout=width*4, cmid=width)) for i in range(2, block_units[0] + 1)],\n",
        "                ))),\n",
        "            ('block2', nn.Sequential(OrderedDict(\n",
        "                [('unit1', PreActBottleneck(cin=width*4, cout=width*8, cmid=width*2, stride=2))] +\n",
        "                [(f'unit{i:d}', PreActBottleneck(cin=width*8, cout=width*8, cmid=width*2)) for i in range(2, block_units[1] + 1)],\n",
        "                ))),    \n",
        "            ('block3', nn.Sequential(OrderedDict(\n",
        "                [('unit1', PreActBottleneck(cin=width*8, cout=width*16, cmid=width*4, stride=2))] +\n",
        "                [(f'unit{i:d}', PreActBottleneck(cin=width*16, cout=width*16, cmid=width*4)) for i in range(2, block_units[2] + 1)],\n",
        "                ))),\n",
        "        ]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.root(x)\n",
        "        x = self.body(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "mWYkKcXKi789"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ViT"
      ],
      "metadata": {
        "id": "CtX-REcWjHeq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding=utf-8\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import copy\n",
        "import logging\n",
        "import math\n",
        "\n",
        "from os.path import join as pjoin\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "from torch.nn import CrossEntropyLoss, Dropout, Softmax, Linear, Conv2d, LayerNorm\n",
        "from torch.nn.modules.utils import _pair\n",
        "from scipy import ndimage\n",
        "\n",
        "#import models.configs as configs\n",
        "\n",
        "#from .modeling_resnet import ResNetV2\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "ATTENTION_Q = \"MultiHeadDotProductAttention_1/query\"\n",
        "ATTENTION_K = \"MultiHeadDotProductAttention_1/key\"\n",
        "ATTENTION_V = \"MultiHeadDotProductAttention_1/value\"\n",
        "ATTENTION_OUT = \"MultiHeadDotProductAttention_1/out\"\n",
        "FC_0 = \"MlpBlock_3/Dense_0\"\n",
        "FC_1 = \"MlpBlock_3/Dense_1\"\n",
        "ATTENTION_NORM = \"LayerNorm_0\"\n",
        "MLP_NORM = \"LayerNorm_2\"\n",
        "\n",
        "\n",
        "def np2th(weights, conv=False):\n",
        "    \"\"\"Possibly convert HWIO to OIHW.\"\"\"\n",
        "    if conv:\n",
        "        weights = weights.transpose([3, 2, 0, 1])\n",
        "    return torch.from_numpy(weights)\n",
        "\n",
        "\n",
        "def swish(x):\n",
        "    return x * torch.sigmoid(x)\n",
        "\n",
        "\n",
        "ACT2FN = {\"gelu\": torch.nn.functional.gelu, \"relu\": torch.nn.functional.relu, \"swish\": swish}\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, config, vis):\n",
        "        super(Attention, self).__init__()\n",
        "        self.vis = vis\n",
        "        self.num_attention_heads = config.transformer[\"num_heads\"]\n",
        "        self.attention_head_size = int(config.hidden_size / self.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.query = Linear(config.hidden_size, self.all_head_size)\n",
        "        self.key = Linear(config.hidden_size, self.all_head_size)\n",
        "        self.value = Linear(config.hidden_size, self.all_head_size)\n",
        "\n",
        "        self.out = Linear(config.hidden_size, config.hidden_size)\n",
        "        self.attn_dropout = Dropout(config.transformer[\"attention_dropout_rate\"])\n",
        "        self.proj_dropout = Dropout(config.transformer[\"attention_dropout_rate\"])\n",
        "\n",
        "        self.softmax = Softmax(dim=-1)\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "        mixed_key_layer = self.key(hidden_states)\n",
        "        mixed_value_layer = self.value(hidden_states)\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
        "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
        "\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "        attention_probs = self.softmax(attention_scores)\n",
        "        weights = attention_probs if self.vis else None\n",
        "        attention_probs = self.attn_dropout(attention_probs)\n",
        "\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(*new_context_layer_shape)\n",
        "        attention_output = self.out(context_layer)\n",
        "        attention_output = self.proj_dropout(attention_output)\n",
        "        return attention_output, weights\n",
        "\n",
        "\n",
        "class Mlp(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(Mlp, self).__init__()\n",
        "        self.fc1 = Linear(config.hidden_size, config.transformer[\"mlp_dim\"])\n",
        "        self.fc2 = Linear(config.transformer[\"mlp_dim\"], config.hidden_size)\n",
        "        self.act_fn = ACT2FN[\"gelu\"]\n",
        "        self.dropout = Dropout(config.transformer[\"dropout_rate\"])\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        nn.init.xavier_uniform_(self.fc1.weight)\n",
        "        nn.init.xavier_uniform_(self.fc2.weight)\n",
        "        nn.init.normal_(self.fc1.bias, std=1e-6)\n",
        "        nn.init.normal_(self.fc2.bias, std=1e-6)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act_fn(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Embeddings(nn.Module):\n",
        "    \"\"\"Construct the embeddings from patch, position embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, config, img_size, in_channels=3):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.hybrid = None\n",
        "        img_size = _pair(img_size)\n",
        "\n",
        "        if config.patches.get(\"grid\") is not None:\n",
        "            grid_size = config.patches[\"grid\"]\n",
        "            patch_size = (img_size[0] // 16 // grid_size[0], img_size[1] // 16 // grid_size[1])\n",
        "            n_patches = (img_size[0] // 16) * (img_size[1] // 16)\n",
        "            self.hybrid = True\n",
        "        else:\n",
        "            patch_size = _pair(config.patches[\"size\"])\n",
        "            n_patches = (img_size[0] // patch_size[0]) * (img_size[1] // patch_size[1])\n",
        "            self.hybrid = False\n",
        "\n",
        "        if self.hybrid:\n",
        "            self.hybrid_model = ResNetV2(block_units=config.resnet.num_layers,\n",
        "                                         width_factor=config.resnet.width_factor)\n",
        "            in_channels = self.hybrid_model.width * 16\n",
        "        self.patch_embeddings = Conv2d(in_channels=in_channels,\n",
        "                                       out_channels=config.hidden_size,\n",
        "                                       kernel_size=patch_size,\n",
        "                                       stride=patch_size)\n",
        "        self.position_embeddings = nn.Parameter(torch.zeros(1, n_patches+1, config.hidden_size))\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, config.hidden_size))\n",
        "\n",
        "        self.dropout = Dropout(config.transformer[\"dropout_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.shape[0]\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "\n",
        "        if self.hybrid:\n",
        "            x = self.hybrid_model(x)\n",
        "        x = self.patch_embeddings(x)\n",
        "        x = x.flatten(2)\n",
        "        x = x.transpose(-1, -2)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "        embeddings = x + self.position_embeddings\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config, vis):\n",
        "        super(Block, self).__init__()\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.attention_norm = LayerNorm(config.hidden_size, eps=1e-6)\n",
        "        self.ffn_norm = LayerNorm(config.hidden_size, eps=1e-6)\n",
        "        self.ffn = Mlp(config)\n",
        "        self.attn = Attention(config, vis)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = x\n",
        "        x = self.attention_norm(x)\n",
        "        x, weights = self.attn(x)\n",
        "        x = x + h\n",
        "\n",
        "        h = x\n",
        "        x = self.ffn_norm(x)\n",
        "        x = self.ffn(x)\n",
        "        x = x + h\n",
        "        return x, weights\n",
        "\n",
        "    def load_from(self, weights, n_block):\n",
        "        ROOT = f\"Transformer/encoderblock_{n_block}\"\n",
        "        with torch.no_grad():\n",
        "            query_weight = np2th(weights[pjoin(ROOT, ATTENTION_Q, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
        "            key_weight = np2th(weights[pjoin(ROOT, ATTENTION_K, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
        "            value_weight = np2th(weights[pjoin(ROOT, ATTENTION_V, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
        "            out_weight = np2th(weights[pjoin(ROOT, ATTENTION_OUT, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
        "\n",
        "            query_bias = np2th(weights[pjoin(ROOT, ATTENTION_Q, \"bias\")]).view(-1)\n",
        "            key_bias = np2th(weights[pjoin(ROOT, ATTENTION_K, \"bias\")]).view(-1)\n",
        "            value_bias = np2th(weights[pjoin(ROOT, ATTENTION_V, \"bias\")]).view(-1)\n",
        "            out_bias = np2th(weights[pjoin(ROOT, ATTENTION_OUT, \"bias\")]).view(-1)\n",
        "\n",
        "            self.attn.query.weight.copy_(query_weight)\n",
        "            self.attn.key.weight.copy_(key_weight)\n",
        "            self.attn.value.weight.copy_(value_weight)\n",
        "            self.attn.out.weight.copy_(out_weight)\n",
        "            self.attn.query.bias.copy_(query_bias)\n",
        "            self.attn.key.bias.copy_(key_bias)\n",
        "            self.attn.value.bias.copy_(value_bias)\n",
        "            self.attn.out.bias.copy_(out_bias)\n",
        "\n",
        "            mlp_weight_0 = np2th(weights[pjoin(ROOT, FC_0, \"kernel\")]).t()\n",
        "            mlp_weight_1 = np2th(weights[pjoin(ROOT, FC_1, \"kernel\")]).t()\n",
        "            mlp_bias_0 = np2th(weights[pjoin(ROOT, FC_0, \"bias\")]).t()\n",
        "            mlp_bias_1 = np2th(weights[pjoin(ROOT, FC_1, \"bias\")]).t()\n",
        "\n",
        "            self.ffn.fc1.weight.copy_(mlp_weight_0)\n",
        "            self.ffn.fc2.weight.copy_(mlp_weight_1)\n",
        "            self.ffn.fc1.bias.copy_(mlp_bias_0)\n",
        "            self.ffn.fc2.bias.copy_(mlp_bias_1)\n",
        "\n",
        "            self.attention_norm.weight.copy_(np2th(weights[pjoin(ROOT, ATTENTION_NORM, \"scale\")]))\n",
        "            self.attention_norm.bias.copy_(np2th(weights[pjoin(ROOT, ATTENTION_NORM, \"bias\")]))\n",
        "            self.ffn_norm.weight.copy_(np2th(weights[pjoin(ROOT, MLP_NORM, \"scale\")]))\n",
        "            self.ffn_norm.bias.copy_(np2th(weights[pjoin(ROOT, MLP_NORM, \"bias\")]))\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, config, vis):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.vis = vis\n",
        "        self.layer = nn.ModuleList()\n",
        "        self.encoder_norm = LayerNorm(config.hidden_size, eps=1e-6)\n",
        "        for _ in range(config.transformer[\"num_layers\"]):\n",
        "            layer = Block(config, vis)\n",
        "            self.layer.append(copy.deepcopy(layer))\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        attn_weights = []\n",
        "        for layer_block in self.layer:\n",
        "            hidden_states, weights = layer_block(hidden_states)\n",
        "            if self.vis:\n",
        "                attn_weights.append(weights)\n",
        "        encoded = self.encoder_norm(hidden_states)\n",
        "        return encoded, attn_weights\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, config, img_size, vis):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.embeddings = Embeddings(config, img_size=img_size)\n",
        "        self.encoder = Encoder(config, vis)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        embedding_output = self.embeddings(input_ids)\n",
        "        encoded, attn_weights = self.encoder(embedding_output)\n",
        "        return encoded, attn_weights\n",
        "\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, config, img_size=224, num_classes=21843, zero_head=False, vis=False):\n",
        "        super(VisionTransformer, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.zero_head = zero_head\n",
        "        self.classifier = config.classifier\n",
        "\n",
        "        self.transformer = Transformer(config, img_size, vis)\n",
        "        self.head = Linear(config.hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x, labels=None):\n",
        "        x, attn_weights = self.transformer(x)\n",
        "        logits = self.head(x[:, 0])\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            # print('label:',labels)\n",
        "            # print('self.num_classes',self.num_classes)\n",
        "            loss = loss_fct(logits.view(-1, self.num_classes), labels.view(-1))\n",
        "            return loss\n",
        "        else:\n",
        "            return logits\n",
        "\n",
        "    def load_from(self, weights):\n",
        "        with torch.no_grad():\n",
        "            if self.zero_head:\n",
        "                nn.init.zeros_(self.head.weight)\n",
        "                nn.init.zeros_(self.head.bias)\n",
        "            else:\n",
        "                self.head.weight.copy_(np2th(weights[\"head/kernel\"]).t())\n",
        "                self.head.bias.copy_(np2th(weights[\"head/bias\"]).t())\n",
        "\n",
        "            self.transformer.embeddings.patch_embeddings.weight.copy_(np2th(weights[\"embedding/kernel\"], conv=True))\n",
        "            self.transformer.embeddings.patch_embeddings.bias.copy_(np2th(weights[\"embedding/bias\"]))\n",
        "            self.transformer.embeddings.cls_token.copy_(np2th(weights[\"cls\"]))\n",
        "            self.transformer.encoder.encoder_norm.weight.copy_(np2th(weights[\"Transformer/encoder_norm/scale\"]))\n",
        "            self.transformer.encoder.encoder_norm.bias.copy_(np2th(weights[\"Transformer/encoder_norm/bias\"]))\n",
        "\n",
        "            posemb = np2th(weights[\"Transformer/posembed_input/pos_embedding\"])\n",
        "            posemb_new = self.transformer.embeddings.position_embeddings\n",
        "            if posemb.size() == posemb_new.size():\n",
        "                self.transformer.embeddings.position_embeddings.copy_(posemb)\n",
        "            else:\n",
        "                logger.info(\"load_pretrained: resized variant: %s to %s\" % (posemb.size(), posemb_new.size()))\n",
        "                ntok_new = posemb_new.size(1)\n",
        "\n",
        "                if self.classifier == \"token\":\n",
        "                    posemb_tok, posemb_grid = posemb[:, :1], posemb[0, 1:]\n",
        "                    ntok_new -= 1\n",
        "                else:\n",
        "                    posemb_tok, posemb_grid = posemb[:, :0], posemb[0]\n",
        "\n",
        "                gs_old = int(np.sqrt(len(posemb_grid)))\n",
        "                gs_new = int(np.sqrt(ntok_new))\n",
        "                print('load_pretrained: grid-size from %s to %s' % (gs_old, gs_new))\n",
        "                posemb_grid = posemb_grid.reshape(gs_old, gs_old, -1)\n",
        "\n",
        "                zoom = (gs_new / gs_old, gs_new / gs_old, 1)\n",
        "                posemb_grid = ndimage.zoom(posemb_grid, zoom, order=1)\n",
        "                posemb_grid = posemb_grid.reshape(1, gs_new * gs_new, -1)\n",
        "                posemb = np.concatenate([posemb_tok, posemb_grid], axis=1)\n",
        "                self.transformer.embeddings.position_embeddings.copy_(np2th(posemb))\n",
        "\n",
        "            for bname, block in self.transformer.encoder.named_children():\n",
        "                for uname, unit in block.named_children():\n",
        "                    unit.load_from(weights, n_block=uname)\n",
        "\n",
        "            if self.transformer.embeddings.hybrid:\n",
        "                self.transformer.embeddings.hybrid_model.root.conv.weight.copy_(np2th(weights[\"conv_root/kernel\"], conv=True))\n",
        "                gn_weight = np2th(weights[\"gn_root/scale\"]).view(-1)\n",
        "                gn_bias = np2th(weights[\"gn_root/bias\"]).view(-1)\n",
        "                self.transformer.embeddings.hybrid_model.root.gn.weight.copy_(gn_weight)\n",
        "                self.transformer.embeddings.hybrid_model.root.gn.bias.copy_(gn_bias)\n",
        "\n",
        "                for bname, block in self.transformer.embeddings.hybrid_model.body.named_children():\n",
        "                    for uname, unit in block.named_children():\n",
        "                        unit.load_from(weights, n_block=bname, n_unit=uname)\n",
        "\n",
        "\n",
        "\n",
        "import ml_collections\n",
        "def get_b16_config():\n",
        "    \"\"\"Returns the ViT-B/16 configuration.\"\"\"\n",
        "    config = ml_collections.ConfigDict()\n",
        "    config.patches = ml_collections.ConfigDict({'size': (16, 16)})\n",
        "    config.hidden_size = 768\n",
        "    config.transformer = ml_collections.ConfigDict()\n",
        "    config.transformer.mlp_dim = 3072\n",
        "    config.transformer.num_heads = 12\n",
        "    config.transformer.num_layers = 12\n",
        "    config.transformer.attention_dropout_rate = 0.0\n",
        "    config.transformer.dropout_rate = 0.1\n",
        "    config.classifier = 'token'\n",
        "    config.representation_size = None\n",
        "    return config\n",
        "\n",
        "def get_r50_b16_config():\n",
        "    \"\"\"Returns the Resnet50 + ViT-B/16 configuration.\"\"\"\n",
        "    config = get_b16_config()\n",
        "    del config.patches.size\n",
        "    config.patches.grid = (14, 14)\n",
        "    config.resnet = ml_collections.ConfigDict()\n",
        "    config.resnet.num_layers = (3, 4, 9)\n",
        "    config.resnet.width_factor = 1\n",
        "    return config\n",
        "\n"
      ],
      "metadata": {
        "id": "4QsgoB4AjAf3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.transforms import ToTensor, Normalize, Resize, Compose\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import models\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "import argparse\n",
        "import copy\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "def seed_everything(seed=12):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "parser = argparse.ArgumentParser(description='CIFAR-10H Training')\n",
        "parser.add_argument('--lr', default=3e-2, type=float, help='learning rate')\n",
        "parser.add_argument('--lr_schedule', default=0, type=int, help='lr scheduler')\n",
        "parser.add_argument('--batch_size', default=48, type=int, help='batch size')\n",
        "parser.add_argument('--test_batch_size', default=64, type=int, help='batch size')\n",
        "parser.add_argument('--num_epoch', default=10, type=int, help='epoch number')\n",
        "parser.add_argument('--num_classes', type=int, default=100, help='number classes')\n",
        "args = parser.parse_args(args=[])\n",
        "\n",
        "def train(model, trainloader, criterion, optimizer):\n",
        "    model.train()\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 300 == 0:\n",
        "            print('Iter:',batch_idx,'/',len(trainloader), ' Loss:',loss.item())\n",
        "\n",
        "def test(model, testloader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "    return correct / total\n",
        "\n",
        "\n",
        "def main():\n",
        "    seed_everything()\n",
        "\n",
        "    preprocess = transforms.Compose([\n",
        "            transforms.Resize(224),\n",
        "            #transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225]\n",
        "        )])\n",
        "\n",
        "    train_dataset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=preprocess)\n",
        "    test_dataset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=preprocess)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True,num_workers=2)\n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.test_batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    config = get_r50_b16_config()\n",
        "    num_classes = 100\n",
        "    img_size = 224\n",
        "    pretrained_dir = 'R50+ViT-B_16.npz'\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    model = VisionTransformer(config, img_size, zero_head=True, num_classes=num_classes)\n",
        "    model.load_from(np.load(pretrained_dir))\n",
        "    model.to(device);\n",
        "\n",
        "    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=0.9, nesterov=False, weight_decay=0.0001)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_epoch, best_acc = 0.0, 0\n",
        "    for epoch in range(args.num_epoch):\n",
        "        train(model, train_loader, criterion, optimizer)\n",
        "        accuracy = test(model, test_loader)\n",
        "        if accuracy > best_acc:\n",
        "            patience = 0\n",
        "            best_acc = accuracy\n",
        "            best_epoch = epoch\n",
        "            best_model = copy.deepcopy(model)\n",
        "            torch.save(best_model.state_dict(), 'best_model_cifar10h_vit.pth.tar')\n",
        "        print('epoch: {}  acc: {:.4f}  best epoch: {}  best acc: {:.4f}'.format(\n",
        "                epoch, accuracy, best_epoch, best_acc, optimizer.param_groups[0]['lr']))\n",
        "        \n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "id": "ADQ9SlUUqEeG",
        "outputId": "583ad4f4-c68a-4f07-d3a3-adcaeeae8fdf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "load_pretrained: grid-size from 24 to 14\n",
            "Iter: 0 / 1042  Loss: 4.605168342590332\n",
            "Iter: 300 / 1042  Loss: 1.268534779548645\n",
            "Iter: 600 / 1042  Loss: 1.547910213470459\n",
            "Iter: 900 / 1042  Loss: 0.961249828338623\n",
            "epoch: 0  acc: 0.7256  best epoch: 0  best acc: 0.7256\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-7795de9d0e10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m                 epoch, accuracy, best_epoch, best_acc, optimizer.param_groups[0]['lr']))\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-7795de9d0e10>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mbest_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_acc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-7795de9d0e10>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, trainloader, criterion, optimizer)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m300\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 112.00 MiB (GPU 0; 14.76 GiB total capacity; 13.31 GiB already allocated; 63.75 MiB free; 13.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PI2byedHZkKs"
      },
      "outputs": [],
      "source": [
        "from torchvision.models import vision_transformer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip -q install --upgrade torch torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 661
        },
        "id": "q1cx4PLddDv_",
        "outputId": "6ce047be-9df3-40f5-e293-0c43d31e6431"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.12.0+cu113)\n",
            "Collecting torch\n",
            "  Downloading torch-1.12.1-cp37-cp37m-manylinux1_x86_64.whl (776.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 776.3 MB 11 kB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.13.0+cu113)\n",
            "Collecting torchvision\n",
            "  Downloading torchvision-0.13.1-cp37-cp37m-manylinux1_x86_64.whl (19.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.1 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2.10)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.12.0+cu113\n",
            "    Uninstalling torch-1.12.0+cu113:\n",
            "      Successfully uninstalled torch-1.12.0+cu113\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.13.0+cu113\n",
            "    Uninstalling torchvision-0.13.0+cu113:\n",
            "      Successfully uninstalled torchvision-0.13.0+cu113\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.13.0 requires torch==1.12.0, but you have torch 1.12.1 which is incompatible.\n",
            "torchaudio 0.12.0+cu113 requires torch==1.12.0, but you have torch 1.12.1 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.12.1 torchvision-0.13.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchvision"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JBdbcmC5dODt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}