{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/tutorial_notebooks/blob/main/VisualBert_EndoVis18_VQA_Sentence.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download Dataset"
      ],
      "metadata": {
        "id": "wevyY60BkqzM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0lkpVfSHTQ8",
        "outputId": "05f0e4cd-e076-4888-87d9-5997f6c827ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:138: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1WGdztykX3nW6pi_BKp4rO8nA7ESNRfVN\n",
            "From (redirected): https://drive.google.com/uc?id=1WGdztykX3nW6pi_BKp4rO8nA7ESNRfVN&confirm=t&uuid=75b306d1-cd82-44ee-85dd-a88fd336fa95\n",
            "To: /content/EndoVis-18-VQA.zip\n",
            "100% 2.70G/2.70G [00:23<00:00, 115MB/s] \n"
          ]
        }
      ],
      "source": [
        "# Downloading the VQA EndoVis18 Dataset https://drive.google.com/file/d/1WGdztykX3nW6pi_BKp4rO8nA7ESNRfVN/view?usp=sharing\n",
        "!gdown --id 1WGdztykX3nW6pi_BKp4rO8nA7ESNRfVN\n",
        "\n",
        "# Unzipping the VQA EndoVis18 Dataset\\\n",
        "!unzip -q EndoVis-18-VQA.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "VisualBert Sentence Generation Model:"
      ],
      "metadata": {
        "id": "uyhruA3SozDM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Description     : VisualBert + Transformer based sentence generation model.\n",
        "Paper           : Surgical-VQA: Visual Question Answering in Surgical Scenes Using Transformers\n",
        "Author          : Lalithkumar Seenivasan, Mobarakol Islam, Adithya Krishna, Hongliang Ren\n",
        "Lab             : MMLAB, National University of Singapore\n",
        "Acknowledgement : Code adopted from the official implementation of VisualBertModel from\n",
        "                  huggingface/transformers (https://github.com/huggingface/transformers.git) and modified.\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import models\n",
        "from transformers import VisualBertModel, VisualBertConfig\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "channel_number = 512\n",
        "\n",
        "\n",
        "'''\n",
        "Encoder transformer : visualBert Encoder\n",
        "'''\n",
        "class VisualBertEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, decoder_layers, n_heads):\n",
        "        '''\n",
        "        VisualBert encoder\n",
        "        vocab_size = tokenizer length\n",
        "        decoder_layers = 6\n",
        "        n_heads = 6\n",
        "        '''\n",
        "        super(VisualBertEncoder, self).__init__()\n",
        "        VBconfig = VisualBertConfig(vocab_size= vocab_size, visual_embedding_dim = 512, num_hidden_layers = decoder_layers, num_attention_heads = n_heads, hidden_size = 2048)\n",
        "        self.VisualBertEncoder = VisualBertModel(VBconfig)\n",
        "\n",
        "    def forward(self, inputs, visual_embeds):\n",
        "        # print(visual_embeds.shape)\n",
        "        # prepare visual embedding\n",
        "        visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long).to(device)\n",
        "        visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float).to(device)\n",
        "\n",
        "        # append visual features to text\n",
        "        inputs.update({\n",
        "        \"visual_embeds\": visual_embeds,\n",
        "        \"visual_token_type_ids\": visual_token_type_ids,\n",
        "        \"visual_attention_mask\": visual_attention_mask,\n",
        "        \"output_attentions\": True\n",
        "        })\n",
        "\n",
        "        inputs['input_ids'] = inputs['input_ids'].to(device)\n",
        "        inputs['token_type_ids'] = inputs['token_type_ids'].to(device)\n",
        "        inputs['attention_mask'] = inputs['attention_mask'].to(device)\n",
        "        inputs['visual_token_type_ids'] = inputs['visual_token_type_ids'].to(device)\n",
        "        inputs['visual_attention_mask'] = inputs['visual_attention_mask'].to(device)\n",
        "\n",
        "        outputs = self.VisualBertEncoder(**inputs)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "'''\n",
        "Decoder transformer\n",
        "'''\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, QKVdim):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "        self.QKVdim = QKVdim\n",
        "\n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        \"\"\"\n",
        "        :param Q: [batch_size, n_heads, -1(len_q), QKVdim]\n",
        "        :param K, V: [batch_size, n_heads, -1(len_k=len_v), QKVdim]\n",
        "        :param attn_mask: [batch_size, n_heads, len_q, len_k]\n",
        "        \"\"\"\n",
        "        # scores: [batch_size, n_heads, len_q, len_k]\n",
        "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(self.QKVdim)\n",
        "        # Fills elements of self tensor with value where mask is True.\n",
        "        scores.to(device).masked_fill_(attn_mask, -1e9)\n",
        "        attn = nn.Softmax(dim=-1)(scores)  # [batch_size, n_heads, len_q, len_k]\n",
        "        context = torch.matmul(attn, V).to(device)  # [batch_size, n_heads, len_q, QKVdim]\n",
        "        return context, attn\n",
        "\n",
        "\n",
        "class Multi_Head_Attention(nn.Module):\n",
        "    def __init__(self, Q_dim, K_dim, QKVdim, n_heads=8, dropout=0.1):\n",
        "        super(Multi_Head_Attention, self).__init__()\n",
        "        self.W_Q = nn.Linear(Q_dim, QKVdim * n_heads)\n",
        "        self.W_K = nn.Linear(K_dim, QKVdim * n_heads)\n",
        "        self.W_V = nn.Linear(K_dim, QKVdim * n_heads)\n",
        "        self.n_heads = n_heads\n",
        "        self.QKVdim = QKVdim\n",
        "        self.embed_dim = Q_dim\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.W_O = nn.Linear(self.n_heads * self.QKVdim, self.embed_dim)\n",
        "\n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        \"\"\"\n",
        "        In self-encoder attention:\n",
        "                Q = K = V: [batch_size, num_pixels=26, encoder_dim=2048]\n",
        "                attn_mask: [batch_size, len_q=26, len_k=26]\n",
        "        In self-decoder attention:\n",
        "                Q = K = V: [batch_size, max_len=20, embed_dim=512]\n",
        "                attn_mask: [batch_size, len_q=20, len_k=20]\n",
        "        encoder-decoder attention:\n",
        "                Q: [batch_size, 20, 512] from decoder\n",
        "                K, V: [batch_size, 26, 2048] from encoder\n",
        "                attn_mask: [batch_size, len_q=20, len_k=26]\n",
        "        return _, attn: [batch_size, n_heads, len_q, len_k]\n",
        "        \"\"\"\n",
        "        residual, batch_size = Q, Q.size(0)\n",
        "        # q_s: [batch_size, n_heads=8, len_q, QKVdim] k_s/v_s: [batch_size, n_heads=8, len_k, QKVdim]\n",
        "        q_s = self.W_Q(Q).view(batch_size, -1, self.n_heads, self.QKVdim).transpose(1, 2)\n",
        "        k_s = self.W_K(K).view(batch_size, -1, self.n_heads, self.QKVdim).transpose(1, 2)\n",
        "        v_s = self.W_V(V).view(batch_size, -1, self.n_heads, self.QKVdim).transpose(1, 2)\n",
        "        # attn_mask: [batch_size, self.n_heads, len_q, len_k]\n",
        "        attn_mask = attn_mask.unsqueeze(1).repeat(1, self.n_heads, 1, 1)\n",
        "        # attn: [batch_size, n_heads, len_q, len_k]\n",
        "        # context: [batch_size, n_heads, len_q, QKVdim]\n",
        "        context, attn = ScaledDotProductAttention(self.QKVdim)(q_s, k_s, v_s, attn_mask)\n",
        "        # context: [batch_size, n_heads, len_q, QKVdim] -> [batch_size, len_q, n_heads * QKVdim]\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.n_heads * self.QKVdim).to(device)\n",
        "        # output: [batch_size, len_q, embed_dim]\n",
        "        output = self.W_O(context)\n",
        "        output = self.dropout(output)\n",
        "        return nn.LayerNorm(self.embed_dim).to(device)(output + residual), attn\n",
        "\n",
        "\n",
        "class PoswiseFeedForwardNet(nn.Module):\n",
        "    def __init__(self, embed_dim, d_ff, dropout):\n",
        "        '''\n",
        "        PosewiseFeedForwardNet\n",
        "        embed_dim = 300\n",
        "        d_ff      = dim_size\n",
        "        dropout`  = 0.1\n",
        "        '''\n",
        "        super(PoswiseFeedForwardNet, self).__init__()\n",
        "        \"\"\"\n",
        "        Two fc layers can also be described by two cnn with kernel_size=1.\n",
        "        \"\"\"\n",
        "        self.conv1 = nn.Conv1d(in_channels=embed_dim, out_channels=d_ff, kernel_size=1)\n",
        "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=embed_dim, kernel_size=1)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        encoder: inputs: [batch_size, len_q=26, embed_dim=2048]\n",
        "        decoder: inputs: [batch_size, max_len=20, embed_dim=512]\n",
        "        \"\"\"\n",
        "        residual = inputs\n",
        "        output = nn.ReLU()(self.conv1(inputs.transpose(1, 2)))\n",
        "        output = self.conv2(output).transpose(1, 2)\n",
        "        output = self.dropout(output)\n",
        "        return nn.LayerNorm(self.embed_dim).to(device)(output + residual)\n",
        "\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, embed_dim, dropout, n_heads):\n",
        "        '''\n",
        "        Decoder layer\n",
        "        embed_dim   = 300\n",
        "        droput      = 0.1\n",
        "        n_heads     = 6\n",
        "        '''\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.dec_self_attn = Multi_Head_Attention(Q_dim=embed_dim, K_dim=embed_dim, QKVdim=64, n_heads=n_heads, dropout=dropout)\n",
        "        self.dec_enc_attn = Multi_Head_Attention(Q_dim=embed_dim, K_dim=2048, QKVdim=64, n_heads=n_heads, dropout=dropout)\n",
        "        self.pos_ffn = PoswiseFeedForwardNet(embed_dim=embed_dim, d_ff=2048, dropout=dropout)\n",
        "\n",
        "    def forward(self, dec_inputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask):\n",
        "        \"\"\"\n",
        "        :param dec_inputs: [batch_size, max_len=20, embed_dim=512]\n",
        "        :param enc_outputs: [batch_size, num_pixels=26, 2048]\n",
        "        :param dec_self_attn_mask: [batch_size, 20, 20]\n",
        "        :param dec_enc_attn_mask: [batch_size, 20, 26]\n",
        "        \"\"\"\n",
        "        # print(dec_inputs.shape, enc_outputs.shape, dec_self_attn_mask.shape, dec_enc_attn_mask.shape)\n",
        "\n",
        "        dec_outputs, dec_self_attn = self.dec_self_attn(dec_inputs, dec_inputs, dec_inputs, dec_self_attn_mask)\n",
        "        dec_outputs, dec_enc_attn = self.dec_enc_attn(dec_outputs, enc_outputs, enc_outputs, dec_enc_attn_mask)\n",
        "        dec_outputs = self.pos_ffn(dec_outputs)\n",
        "        return dec_outputs, dec_self_attn, dec_enc_attn\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, n_layers, vocab_size, embed_dim, dropout, n_heads, answer_len):\n",
        "        '''\n",
        "        Transformer decoder\n",
        "        n_layers    = 6\n",
        "        vocab_size  = tokenizer length\n",
        "        embed_fim   = 300\n",
        "        dropout     = 0.1\n",
        "        n_heads     = 6\n",
        "        answer_len  = 20\n",
        "        '''\n",
        "        super(Decoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.anwer_len = answer_len\n",
        "        self.tgt_emb = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "        self.pos_emb = nn.Embedding.from_pretrained(self.get_position_embedding_table(embed_dim), freeze=True)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.layers = nn.ModuleList([DecoderLayer(embed_dim, dropout, n_heads) for _ in range(n_layers)])\n",
        "        self.projection = nn.Linear(embed_dim, vocab_size, bias=False)\n",
        "\n",
        "    def get_position_embedding_table(self, embed_dim):\n",
        "        def cal_angle(position, hid_idx):\n",
        "            return position / np.power(10000, 2 * (hid_idx // 2) / embed_dim)\n",
        "        def get_posi_angle_vec(position):\n",
        "            return [cal_angle(position, hid_idx) for hid_idx in range(embed_dim)]\n",
        "\n",
        "        embedding_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(self.anwer_len)])\n",
        "        embedding_table[:, 0::2] = np.sin(embedding_table[:, 0::2])  # dim 2i\n",
        "        embedding_table[:, 1::2] = np.cos(embedding_table[:, 1::2])  # dim 2i+1\n",
        "        return torch.FloatTensor(embedding_table).to(device)\n",
        "\n",
        "    def get_attn_pad_mask(self, seq_q, seq_k):\n",
        "        batch_size, len_q = seq_q.size()\n",
        "        batch_size, len_k = seq_k.size()\n",
        "        # In wordmap, <pad>:0\n",
        "        # pad_attn_mask: [batch_size, 1, len_k], one is masking\n",
        "        pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)\n",
        "        return pad_attn_mask.expand(batch_size, len_q, len_k)  # [batch_size, len_q, len_k]\n",
        "\n",
        "    def get_attn_subsequent_mask(self, seq):\n",
        "        attn_shape = [seq.size(0), seq.size(1), seq.size(1)]\n",
        "        subsequent_mask = np.triu(np.ones(attn_shape), k=1)\n",
        "        subsequent_mask = torch.from_numpy(subsequent_mask).byte().to(device)\n",
        "        return subsequent_mask\n",
        "\n",
        "    def forward(self, encoder_out, encoded_captions, caption_lengths):\n",
        "        \"\"\"\n",
        "        :param encoder_out: [batch_size, num_pixels=26, 2048]\n",
        "        :param encoded_captions: [batch_size, 20]\n",
        "        :param caption_lengths: [batch_size, 1]\n",
        "        \"\"\"\n",
        "        batch_size = encoder_out.size(0)\n",
        "        token_size = encoder_out.size(1)\n",
        "        # Sort input data by decreasing lengths.\n",
        "        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim=0, descending=True)\n",
        "        encoder_out = encoder_out[sort_ind]\n",
        "        encoded_captions = encoded_captions[sort_ind]\n",
        "        # We won't decode at the <end> position, since we've finished generating as soon as we generate <end>\n",
        "        # So, decoding lengths are actual lengths - 1\n",
        "        decode_lengths = (caption_lengths - 1).tolist()\n",
        "\n",
        "        '''# dec_outputs: [batch_size, max_len=20, embed_dim=512]\n",
        "        # dec_self_attn_pad_mask: [batch_size, len_q=20, len_k=20], 1 if id=0(<pad>)\n",
        "        # dec_self_attn_subsequent_mask: [batch_size, 20, 20], Upper triangle of an array with 1.\n",
        "        # dec_self_attn_mask for self-decoder attention, the position whose val > 0 will be masked.\n",
        "        # dec_enc_attn_mask for encoder-decoder attention.\n",
        "        # e.g. 9488, 23, 53, 74, 0, 0  |  dec_self_attn_mask:\n",
        "        # 0 1 1 1 2 2\n",
        "        # 0 0 1 1 2 2\n",
        "        # 0 0 0 1 2 2\n",
        "        # 0 0 0 0 2 2\n",
        "        # 0 0 0 0 1 2\n",
        "        # 0 0 0 0 1 1'''\n",
        "        dec_outputs = self.tgt_emb(encoded_captions) + self.pos_emb(torch.LongTensor([list(range(self.anwer_len))]*batch_size).to(device))\n",
        "        dec_outputs = self.dropout(dec_outputs)\n",
        "        dec_self_attn_pad_mask = self.get_attn_pad_mask(encoded_captions, encoded_captions)\n",
        "        dec_self_attn_subsequent_mask = self.get_attn_subsequent_mask(encoded_captions)\n",
        "        dec_self_attn_mask = torch.gt((dec_self_attn_pad_mask + dec_self_attn_subsequent_mask), 0)\n",
        "        dec_enc_attn_mask = (torch.tensor(np.zeros((batch_size, self.anwer_len, token_size))).to(device) == torch.tensor(np.ones((batch_size, self.anwer_len, token_size))).to(device))\n",
        "\n",
        "        dec_self_attns, dec_enc_attns = [], []\n",
        "        for layer in self.layers:\n",
        "            # attn: [batch_size, n_heads, len_q, len_k]\n",
        "            dec_outputs, dec_self_attn, dec_enc_attn = layer(dec_outputs, encoder_out, dec_self_attn_mask, dec_enc_attn_mask)\n",
        "            dec_self_attns.append(dec_self_attn)\n",
        "            dec_enc_attns.append(dec_enc_attn)\n",
        "        predictions = self.projection(dec_outputs)\n",
        "        return predictions, encoded_captions, decode_lengths, sort_ind, dec_self_attns, dec_enc_attns\n",
        "\n",
        "\n",
        "'''\n",
        "VisualBert Encoder + Transformer decoder\n",
        "'''\n",
        "class VisualBertSentence(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embed_dim, encoder_layers, decoder_layers, dropout=0.1, n_heads=8, answer_len = 20):\n",
        "        '''\n",
        "        VisualBert Encoder + Transformer decoder\n",
        "        vocab_size     = tokenizer length\n",
        "        embed_dim      = 300\n",
        "        encoder_layers = 6\n",
        "        decoder_layers = 6\n",
        "        dropout        = 0.1\n",
        "        n_heads        = 6\n",
        "        answer_len     = 20\n",
        "        '''\n",
        "        super(VisualBertSentence, self).__init__()\n",
        "        self.encoder = VisualBertEncoder(vocab_size, encoder_layers, n_heads)\n",
        "        self.decoder = Decoder(decoder_layers, vocab_size, embed_dim, dropout, n_heads, answer_len)\n",
        "        self.embedding = self.decoder.tgt_emb\n",
        "\n",
        "    def load_pretrained_embeddings(self, embeddings):\n",
        "        self.embedding.weight = nn.Parameter(embeddings)\n",
        "\n",
        "    def fine_tune_embeddings(self, fine_tune=True):\n",
        "        for p in self.embedding.parameters():\n",
        "            p.requires_grad = fine_tune\n",
        "\n",
        "    def forward(self, inputs, visual_embeds, encoded_captions, caption_lengths):\n",
        "        # Vision and text encoder output\n",
        "        encoder_outputs = self.encoder(inputs, visual_embeds)\n",
        "\n",
        "        # predict answer using decoder model\n",
        "        predictions, encoded_captions, decode_lengths, sort_ind, dec_self_attns, dec_enc_attns = self.decoder(encoder_outputs['last_hidden_state'], encoded_captions, caption_lengths)\n",
        "        alphas = {\"enc_self_attns\": encoder_outputs['attentions'], \"dec_self_attns\": dec_self_attns, \"dec_enc_attns\": dec_enc_attns}\n",
        "        return predictions, encoded_captions, decode_lengths, alphas, sort_ind"
      ],
      "metadata": {
        "id": "Zv8MXL9mo4MQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utils"
      ],
      "metadata": {
        "id": "alIq7g24kpkp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"\n",
        "    Keeps track of most recent, average, sum, and count of a metric.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "def adjust_learning_rate(optimizer, shrink_factor):\n",
        "    \"\"\"\n",
        "    Shrinks learning rate by a specified factor.\n",
        "\n",
        "    :param optimizer: optimizer whose learning rate must be shrunk.\n",
        "    :param shrink_factor: factor in interval (0, 1) to multiply learning rate with.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\nDECAYING learning rate.\")\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = param_group['lr'] * shrink_factor\n",
        "    print(\"The new learning rate is %f\\n\" % (optimizer.param_groups[0]['lr'],))\n",
        "\n",
        "\n",
        "def save_clf_checkpoint(checkpoint_dir, epoch, epochs_since_improvement, model, optimizer, Acc, final_args):\n",
        "    \"\"\"\n",
        "    Saves model checkpoint.\n",
        "    \"\"\"\n",
        "    state = {'epoch': epoch,\n",
        "             'epochs_since_improvement': epochs_since_improvement,\n",
        "             'Acc': Acc,\n",
        "             'model': model,\n",
        "             'optimizer': optimizer,\n",
        "             'final_args': final_args}\n",
        "    filename = checkpoint_dir + 'Best.pth.tar'\n",
        "    torch.save(state, filename)\n",
        "\n",
        "def calc_acc(y_true, y_pred):\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    return acc\n",
        "\n",
        "def calc_classwise_acc(y_true, y_pred):\n",
        "    matrix = confusion_matrix(y_true, y_pred)\n",
        "    classwise_acc = matrix.diagonal()/matrix.sum(axis=1)\n",
        "    return classwise_acc\n",
        "\n",
        "def calc_map(y_true, y_scores):\n",
        "    mAP = average_precision_score(y_true, y_scores,average=None)\n",
        "    return mAP\n",
        "\n",
        "def calc_precision_recall_fscore(y_true, y_pred):\n",
        "    precision, recall, fscore, _ = precision_recall_fscore_support(y_true, y_pred, average='macro', zero_division = 1)\n",
        "    return(precision, recall, fscore)\n",
        "\n",
        "\n",
        "def seed_everything(seed=27):\n",
        "    '''\n",
        "    Set random seed for reproducible experiments\n",
        "    Inputs: seed number\n",
        "    '''\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "gHVYjRD1Bi7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "class EndoVis18VQAGPTSentence(Dataset):\n",
        "    '''\n",
        "    \tseq: train_seq  = [2, 3, 4, 6, 7, 9, 10, 11, 12, 14, 15]\n",
        "    \t     val_seq    = [1, 5, 16]\n",
        "    \tfolder_head = '../dataset/EndoVis-18-VQA/seq_'\n",
        "        folder_tail = '/vqa2/Sentence/*.txt'\n",
        "    '''\n",
        "    def __init__(self, seq, folder_head, folder_tail, model_ver = None, transform=None):\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "                                transforms.Resize((300,256)),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
        "                                ])\n",
        "\n",
        "        filenames = []\n",
        "        for curr_seq in seq: filenames = filenames + glob.glob(folder_head + str(curr_seq) + folder_tail)\n",
        "\n",
        "        self.vqas = []\n",
        "        for file in filenames:\n",
        "            file_data = open(file, \"r\")\n",
        "            lines = [line.strip(\"\\n\") for line in file_data if line != \"\\n\"]\n",
        "            file_data.close()\n",
        "            for line in lines:\n",
        "                q_s, an_s = line.split('|')\n",
        "                q_s = q_s.split('&')\n",
        "                an_s = an_s.split(('&'))\n",
        "                for i in range(len(q_s)):\n",
        "                    q_a = q_s[i]+'|'+an_s[i]\n",
        "                    # print(file, q_a)\n",
        "                    self.vqas.append([file, q_a])\n",
        "        print('Total files: %d | Total question: %.d' %(len(filenames), len(self.vqas)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.vqas)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        loc = self.vqas[idx][0].split('/')\n",
        "\n",
        "        # img loc[3],\n",
        "        img_loc = os.path.join(loc[0],loc[1],loc[2], 'left_frames',loc[-1].split('_')[0]+'.png')\n",
        "        if self.transform:\n",
        "            img = Image.open(img_loc)\n",
        "            img = self.transform(img)\n",
        "\n",
        "        # question and answer\n",
        "        question, answer = self.vqas[idx][1].split('|')\n",
        "        answer = '<|sep|> '+answer\n",
        "\n",
        "\n",
        "        return img, question, answer\n",
        "\n",
        "\n",
        "# data location\n",
        "train_seq = [2, 3, 4, 6, 7, 9, 10, 11, 12, 14, 15]\n",
        "val_seq = [1, 5, 16]\n",
        "\n",
        "folder_head = 'datasets/EndoVis-18-VQA/seq_'\n",
        "folder_tail = '/vqa/Sentence/*.txt'\n",
        "\n",
        "train_dataset = EndoVis18VQAGPTSentence(train_seq, folder_head, folder_tail, model_ver=args.model_ver)\n",
        "# train_dataloader = DataLoader(dataset=train_dataset, batch_size= args.batch_size, shuffle=True, num_workers=8)\n",
        "# val_dataset = EndoVis18VQAGPTSentence(val_seq, folder_head, folder_tail, model_ver=args.model_ver)\n",
        "# val_dataloader = DataLoader(dataset=val_dataset, batch_size= args.batch_size, shuffle=False, num_workers=8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "NslIOHMTkEWw",
        "outputId": "0d1b70ae-964d-4c25-c288-6f588443f3f8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'args' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-fe0078bea4cc>\u001b[0m in \u001b[0;36m<cell line: 66>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0mfolder_tail\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/vqa/Sentence/*.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEndoVis18VQAGPTSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder_head\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder_tail\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_ver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_ver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;31m# train_dataloader = DataLoader(dataset=train_dataset, batch_size= args.batch_size, shuffle=True, num_workers=8)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;31m# val_dataset = EndoVis18VQAGPTSentence(val_seq, folder_head, folder_tail, model_ver=args.model_ver)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Model"
      ],
      "metadata": {
        "id": "DUy9LwOxkuhX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#############Dataloader###############\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "import glob\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class EndoVis18VQAGPTClassification(Dataset):\n",
        "    '''\n",
        "    \tseq: train_seq  = [2, 3, 4, 6, 7, 9, 10, 11, 12, 14, 15]\n",
        "    \t     val_seq    = [1, 5, 16]\n",
        "    \tfolder_head     = 'dataset/EndoVis-18-VQA/seq_'\n",
        "    \tfolder_tail     = '/vqa/Classification/*.txt'\n",
        "    '''\n",
        "    def __init__(self, seq, folder_head, folder_tail, transform=None):\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "                    transforms.Resize((224,224)),\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
        "                    ])\n",
        "\n",
        "\n",
        "        # files, question and answers\n",
        "        filenames = []\n",
        "        for curr_seq in seq: filenames = filenames + glob.glob(folder_head + str(curr_seq) + folder_tail)\n",
        "        self.vqas = []\n",
        "        for file in filenames:\n",
        "            file_data = open(file, \"r\")\n",
        "            lines = [line.strip(\"\\n\") for line in file_data if line != \"\\n\"]\n",
        "            file_data.close()\n",
        "            for line in lines: self.vqas.append([file, line])\n",
        "        print('Total files: %d | Total question: %.d' %(len(filenames), len(self.vqas)))\n",
        "\n",
        "        # Labels\n",
        "        self.labels = ['kidney', 'Idle', 'Grasping', 'Retraction', 'Tissue_Manipulation',\n",
        "                        'Tool_Manipulation', 'Cutting', 'Cauterization', 'Suction',\n",
        "                        'Looping', 'Suturing', 'Clipping', 'Staple', 'Ultrasound_Sensing',\n",
        "                        'left-top', 'right-top', 'left-bottom', 'right-bottom']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.vqas)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        loc = self.vqas[idx][0].split('/')\n",
        "\n",
        "        # img\n",
        "        img_loc = os.path.join(loc[0],loc[1],'left_frames',loc[-1].split('_')[0]+'.png')\n",
        "        if self.transform:\n",
        "            img = Image.open(img_loc)\n",
        "            img = self.transform(img)\n",
        "\n",
        "        # question and answer\n",
        "        question = self.vqas[idx][1].split('|')[0]\n",
        "        label = self.labels.index(str(self.vqas[idx][1].split('|')[1]))\n",
        "\n",
        "        return img, question, label\n",
        "\n",
        "\n",
        "#############Model Architecture###############\n",
        "import torch\n",
        "from torch import nn\n",
        "from transformers import VisualBertModel, VisualBertConfig\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "class VisualBertClassification(nn.Module):\n",
        "    '''\n",
        "    VisualBert Classification Model\n",
        "    vocab_size    = tokenizer length\n",
        "    encoder_layer = 6\n",
        "    n_heads       = 8\n",
        "    num_class     = number of class in dataset\n",
        "    '''\n",
        "    def __init__(self, vocab_size, layers, n_heads, num_class = 10):\n",
        "        super(VisualBertClassification, self).__init__()\n",
        "        self.num_labels = num_class\n",
        "        self.VisualBertEncoder = VisualBertModel.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n",
        "        self.visual_projection = nn.Linear(512, self.VisualBertEncoder.embeddings.visual_projection.in_features)\n",
        "        self.classifier = nn.Linear(768, self.num_labels)\n",
        "\n",
        "        ## image processing\n",
        "        self.img_feature_extractor = models.resnet18(weights=True)\n",
        "        self.img_feature_extractor.fc = nn.Sequential(*list(self.img_feature_extractor.fc.children())[:-1])\n",
        "\n",
        "    def forward(self, inputs, img):\n",
        "        # prepare visual embedding\n",
        "        visual_embeds = self.img_feature_extractor(img)\n",
        "        visual_embeds = self.visual_projection(visual_embeds)\n",
        "        visual_embeds = torch.unsqueeze(visual_embeds, dim=1)\n",
        "        visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long).to(device)\n",
        "        visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float).to(device)\n",
        "\n",
        "        # append visual features to text\n",
        "        inputs.update({\n",
        "                        \"visual_embeds\": visual_embeds,\n",
        "                        \"visual_token_type_ids\": visual_token_type_ids,\n",
        "                        \"visual_attention_mask\": visual_attention_mask,\n",
        "                        \"output_attentions\": True\n",
        "                        })\n",
        "\n",
        "        inputs['input_ids'] = inputs['input_ids'].to(device)\n",
        "        inputs['token_type_ids'] = inputs['token_type_ids'].to(device)\n",
        "        inputs['attention_mask'] = inputs['attention_mask'].to(device)\n",
        "        inputs['visual_token_type_ids'] = inputs['visual_token_type_ids'].to(device)\n",
        "        inputs['visual_attention_mask'] = inputs['visual_attention_mask'].to(device)\n",
        "\n",
        "        '----------------- VQA -----------------'\n",
        "        index_to_gather = inputs['attention_mask'].sum(1) - 2  # as in original code # 6\n",
        "        outputs = self.VisualBertEncoder(**inputs)\n",
        "        sequence_output = outputs[0] # [1, 33, 2048]\n",
        "\n",
        "        # TO-CHECK: From the original code\n",
        "        index_to_gather = (index_to_gather.unsqueeze(-1).unsqueeze(-1).expand(index_to_gather.size(0), 1, sequence_output.size(-1))) #  [1, 1, 2048]\n",
        "\n",
        "        pooled_output = torch.gather(sequence_output, 1, index_to_gather) # [1, 33, 2048]\n",
        "\n",
        "        # pooled_output = self.dropout(pooled_output) # [1, 1, 2048]\n",
        "        logits = self.classifier(pooled_output) # [1, 1, 8]\n",
        "        reshaped_logits = logits.view(-1, self.num_labels) # [1, 8]\n",
        "        return reshaped_logits\n",
        "\n",
        "\n",
        "#############Training Script###############\n",
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.utils.data\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data  import DataLoader\n",
        "from transformers import BertTokenizer\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "def get_arg():\n",
        "    parser = argparse.ArgumentParser(description='VisualQuestionAnswerClassification')\n",
        "\n",
        "    # VB Model parameters\n",
        "    parser.add_argument('--n_heads',        type=int,   default=8,                                  help='Multi-head attention.')\n",
        "    parser.add_argument('--encoder_layers', type=int,   default=6,                                  help='the number of layers of encoder in Transformer.')\n",
        "\n",
        "    # Training parameters\n",
        "    parser.add_argument('--epochs',         type=int,   default=2,                                 help='number of epochs to train for (if early stopping is not triggered).') #80, 26\n",
        "    parser.add_argument('--batch_size',     type=int,   default=64,                                 help='batch_size')\n",
        "    parser.add_argument('--workers',        type=int,   default=1,                                  help='for data-loading; right now, only 1 works with h5pys.')\n",
        "    parser.add_argument('--lr',             type=float, default=0.00001,                           help='0.000005, 0.00001, 0.000005')\n",
        "    parser.add_argument('--checkpoint_dir', default= 'checkpoints/VB_RN18',            help='med_vqa_c/m18/c80/m18_vid/c80_vid') #clf_v1_2_1x1/med_vqa_c3\n",
        "    parser.add_argument('--question_len',   default= 25,                                            help='25')\n",
        "    parser.add_argument('--num_class',      default= 2,                                             help='25')\n",
        "    parser.add_argument('--validate',       default=False,                                          help='When only validation required False/True')\n",
        "\n",
        "    if 'ipykernel' in sys.modules:\n",
        "        args = parser.parse_args([])\n",
        "    else:\n",
        "        args = parser.parse_args()\n",
        "    return args\n",
        "\n",
        "def train(args, train_dataloader, model, criterion, optimizer, epoch, tokenizer, device):\n",
        "\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    label_true = None\n",
        "    label_pred = None\n",
        "    label_score = None\n",
        "\n",
        "    for i, (imgs, q, labels) in enumerate(train_dataloader,0):\n",
        "        questions = []\n",
        "        for question in q: questions.append(question)\n",
        "        inputs = tokenizer(questions, padding=\"max_length\",max_length= args.question_len, return_tensors=\"pt\")\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        outputs = model(inputs, imgs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        scores, predicted = torch.max(F.softmax(outputs, dim=1).data, 1)\n",
        "        label_true = labels.data.cpu() if label_true == None else torch.cat((label_true, labels.data.cpu()), 0)\n",
        "        label_pred = predicted.data.cpu() if label_pred == None else torch.cat((label_pred, predicted.data.cpu()), 0)\n",
        "        label_score = scores.data.cpu() if label_score == None else torch.cat((label_score, scores.data.cpu()), 0)\n",
        "\n",
        "    # loss and acc\n",
        "    acc, c_acc = calc_acc(label_true, label_pred), calc_classwise_acc(label_true, label_pred)\n",
        "    precision, recall, fscore = calc_precision_recall_fscore(label_true, label_pred)\n",
        "    print('Train: epoch: %d loss: %.6f | Acc: %.6f | Precision: %.6f | Recall: %.6f | FScore: %.6f' %(epoch, total_loss, acc, precision, recall, fscore))\n",
        "    return acc\n",
        "\n",
        "\n",
        "def validate(args, val_loader, model, criterion, epoch, tokenizer, device, save_output = False):\n",
        "\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    label_true = None\n",
        "    label_pred = None\n",
        "    label_score = None\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, ( imgs, q, labels) in enumerate(val_loader,0):\n",
        "            questions = []\n",
        "            for question in q: questions.append(question)\n",
        "            inputs = tokenizer(questions, padding=\"max_length\",max_length=args.question_len, return_tensors=\"pt\")\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "            # model forward pass\n",
        "            outputs = model(inputs, imgs)\n",
        "\n",
        "            # loss\n",
        "            loss = criterion(outputs,labels)\n",
        "            total_loss += loss.item()\n",
        "            scores, predicted = torch.max(F.softmax(outputs, dim=1).data, 1)\n",
        "            label_true = labels.data.cpu() if label_true == None else torch.cat((label_true, labels.data.cpu()), 0)\n",
        "            label_pred = predicted.data.cpu() if label_pred == None else torch.cat((label_pred, predicted.data.cpu()), 0)\n",
        "            label_score = scores.data.cpu() if label_score == None else torch.cat((label_score, scores.data.cpu()), 0)\n",
        "\n",
        "    acc = calc_acc(label_true, label_pred)\n",
        "    c_acc = 0.0\n",
        "    precision, recall, fscore = calc_precision_recall_fscore(label_true, label_pred)\n",
        "    print('Test: epoch: %d loss: %.6f | Acc: %.6f | Precision: %.6f | Recall: %.6f | FScore: %.6f' %(epoch, total_loss, acc, precision, recall, fscore))\n",
        "\n",
        "    return (acc, c_acc, precision, recall, fscore)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    args = get_arg()\n",
        "    args.checkpoint_dir = 'checkpoints/VB_RN18'\n",
        "    os.makedirs(args.checkpoint_dir, exist_ok=True)\n",
        "    seed_everything()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    start_epoch = 1\n",
        "    best_epoch = [0]\n",
        "    best_results = [0.0]\n",
        "    epochs_since_improvement = 0\n",
        "    final_args = { \"n_heads\": args.n_heads, \"encoder_layers\": args.encoder_layers}\n",
        "    train_seq = [2, 3, 4, 6, 7, 9, 10, 11, 12, 14, 15]\n",
        "    val_seq = [1, 5, 16]\n",
        "    args.num_class = 18\n",
        "\n",
        "    folder_head = 'EndoVis-18-VQA/seq_'\n",
        "    folder_tail = '/vqa/Classification/*.txt'\n",
        "\n",
        "    train_dataset = EndoVis18VQAGPTClassification(train_seq, folder_head, folder_tail)\n",
        "    train_dataloader = DataLoader(dataset=train_dataset, batch_size= args.batch_size, shuffle=True, num_workers=2)\n",
        "    val_dataset = EndoVis18VQAGPTClassification(val_seq, folder_head, folder_tail)\n",
        "    val_dataloader = DataLoader(dataset=val_dataset, batch_size= args.batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    # model = VisualBertClassification(vocab_size=len(tokenizer), layers=args.encoder_layers, n_heads=args.n_heads, num_class = args.num_class)\n",
        "    model = VisualBertClassification_V3(vocab_size=len(tokenizer), layers=args.encoder_layers, n_heads=args.n_heads, num_class = args.num_class)\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss().to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
        "\n",
        "    for epoch in range(start_epoch, args.epochs):\n",
        "\n",
        "            if epochs_since_improvement > 0 and epochs_since_improvement % 5 == 0:\n",
        "                adjust_learning_rate(optimizer, 0.8)\n",
        "\n",
        "            train_acc = train(args, train_dataloader=train_dataloader, model = model, criterion=criterion, optimizer=optimizer, epoch=epoch, tokenizer = tokenizer, device = device)\n",
        "            test_acc, test_c_acc, test_precision, test_recall, test_fscore = validate(args, val_loader=val_dataloader, model = model, criterion=criterion, epoch=epoch, tokenizer = tokenizer, device = device)\n",
        "\n",
        "            if test_acc >= best_results[0]:\n",
        "                print('Best Epoch:', epoch)\n",
        "                epochs_since_improvement = 0\n",
        "                best_results[0] = test_acc\n",
        "                best_epoch[0] = epoch\n",
        "                save_clf_checkpoint(args.checkpoint_dir, epoch, epochs_since_improvement, model, optimizer, best_results[0], final_args)\n",
        "            else:\n",
        "                epochs_since_improvement += 1\n",
        "                print(\"\\nEpochs since last improvement: %d\\n\" % (epochs_since_improvement,))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1TQaWXz4D0H",
        "outputId": "5098ae09-c15b-4893-b19b-5494046cfc76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total files: 1560 | Total question: 9014\n",
            "Total files: 447 | Total question: 2769\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    }
  ]
}