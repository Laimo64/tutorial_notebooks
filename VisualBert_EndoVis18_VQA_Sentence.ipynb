{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/tutorial_notebooks/blob/main/VisualBert_EndoVis18_VQA_Sentence.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download Dataset"
      ],
      "metadata": {
        "id": "wevyY60BkqzM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0lkpVfSHTQ8",
        "outputId": "94c0775b-ff78-4f1f-f106-cd258ec4bf26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:138: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1WGdztykX3nW6pi_BKp4rO8nA7ESNRfVN\n",
            "From (redirected): https://drive.google.com/uc?id=1WGdztykX3nW6pi_BKp4rO8nA7ESNRfVN&confirm=t&uuid=1cf8155d-61d9-4d9f-a2d6-f4de37d62da8\n",
            "To: /content/EndoVis-18-VQA.zip\n",
            "100% 2.70G/2.70G [00:32<00:00, 83.1MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Downloading the VQA EndoVis18 Dataset https://drive.google.com/file/d/1WGdztykX3nW6pi_BKp4rO8nA7ESNRfVN/view?usp=sharing\n",
        "!gdown --id 1WGdztykX3nW6pi_BKp4rO8nA7ESNRfVN\n",
        "\n",
        "# Unzipping the VQA EndoVis18 Dataset\\\n",
        "!unzip -q EndoVis-18-VQA.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "VisualBert Sentence Generation Model:"
      ],
      "metadata": {
        "id": "uyhruA3SozDM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Description     : VisualBert + Transformer based sentence generation model.\n",
        "Paper           : Surgical-VQA: Visual Question Answering in Surgical Scenes Using Transformers\n",
        "Author          : Lalithkumar Seenivasan, Mobarakol Islam, Adithya Krishna, Hongliang Ren\n",
        "Lab             : MMLAB, National University of Singapore\n",
        "Acknowledgement : Code adopted from the official implementation of VisualBertModel from\n",
        "                  huggingface/transformers (https://github.com/huggingface/transformers.git) and modified.\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import models\n",
        "from transformers import VisualBertModel, VisualBertConfig\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "channel_number = 512\n",
        "\n",
        "\n",
        "'''\n",
        "Encoder transformer : visualBert Encoder\n",
        "'''\n",
        "class VisualBertEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, decoder_layers, n_heads):\n",
        "        '''\n",
        "        VisualBert encoder\n",
        "        vocab_size = tokenizer length\n",
        "        decoder_layers = 6\n",
        "        n_heads = 6\n",
        "        '''\n",
        "        super(VisualBertEncoder, self).__init__()\n",
        "        VBconfig = VisualBertConfig(vocab_size= vocab_size, visual_embedding_dim = 512, num_hidden_layers = decoder_layers, num_attention_heads = n_heads, hidden_size = 2048)\n",
        "        self.VisualBertEncoder = VisualBertModel(VBconfig)\n",
        "\n",
        "        ## image processing\n",
        "        self.img_feature_extractor = models.resnet18(weights=True)\n",
        "        self.img_feature_extractor.fc = nn.Sequential(*list(self.img_feature_extractor.fc.children())[:-1])\n",
        "\n",
        "    def forward(self, inputs, imgs):\n",
        "        # print(visual_embeds.shape)\n",
        "        # prepare visual embedding\n",
        "        visual_embeds = self.img_feature_extractor(imgs)\n",
        "        visual_embeds = torch.unsqueeze(visual_embeds, dim=1)\n",
        "        visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long).to(device)\n",
        "        visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float).to(device)\n",
        "\n",
        "        # append visual features to text\n",
        "        inputs.update({\n",
        "        \"visual_embeds\": visual_embeds,\n",
        "        \"visual_token_type_ids\": visual_token_type_ids,\n",
        "        \"visual_attention_mask\": visual_attention_mask,\n",
        "        \"output_attentions\": True\n",
        "        })\n",
        "\n",
        "        inputs['input_ids'] = inputs['input_ids'].to(device)\n",
        "        inputs['token_type_ids'] = inputs['token_type_ids'].to(device)\n",
        "        inputs['attention_mask'] = inputs['attention_mask'].to(device)\n",
        "        inputs['visual_token_type_ids'] = inputs['visual_token_type_ids'].to(device)\n",
        "        inputs['visual_attention_mask'] = inputs['visual_attention_mask'].to(device)\n",
        "\n",
        "        outputs = self.VisualBertEncoder(**inputs)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "'''\n",
        "Decoder transformer\n",
        "'''\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, QKVdim):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "        self.QKVdim = QKVdim\n",
        "\n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        \"\"\"\n",
        "        :param Q: [batch_size, n_heads, -1(len_q), QKVdim]\n",
        "        :param K, V: [batch_size, n_heads, -1(len_k=len_v), QKVdim]\n",
        "        :param attn_mask: [batch_size, n_heads, len_q, len_k]\n",
        "        \"\"\"\n",
        "        # scores: [batch_size, n_heads, len_q, len_k]\n",
        "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(self.QKVdim)\n",
        "        # Fills elements of self tensor with value where mask is True.\n",
        "        scores.to(device).masked_fill_(attn_mask, -1e9)\n",
        "        attn = nn.Softmax(dim=-1)(scores)  # [batch_size, n_heads, len_q, len_k]\n",
        "        context = torch.matmul(attn, V).to(device)  # [batch_size, n_heads, len_q, QKVdim]\n",
        "        return context, attn\n",
        "\n",
        "\n",
        "class Multi_Head_Attention(nn.Module):\n",
        "    def __init__(self, Q_dim, K_dim, QKVdim, n_heads=8, dropout=0.1):\n",
        "        super(Multi_Head_Attention, self).__init__()\n",
        "        self.W_Q = nn.Linear(Q_dim, QKVdim * n_heads)\n",
        "        self.W_K = nn.Linear(K_dim, QKVdim * n_heads)\n",
        "        self.W_V = nn.Linear(K_dim, QKVdim * n_heads)\n",
        "        self.n_heads = n_heads\n",
        "        self.QKVdim = QKVdim\n",
        "        self.embed_dim = Q_dim\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.W_O = nn.Linear(self.n_heads * self.QKVdim, self.embed_dim)\n",
        "\n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        \"\"\"\n",
        "        In self-encoder attention:\n",
        "                Q = K = V: [batch_size, num_pixels=26, encoder_dim=2048]\n",
        "                attn_mask: [batch_size, len_q=26, len_k=26]\n",
        "        In self-decoder attention:\n",
        "                Q = K = V: [batch_size, max_len=20, embed_dim=512]\n",
        "                attn_mask: [batch_size, len_q=20, len_k=20]\n",
        "        encoder-decoder attention:\n",
        "                Q: [batch_size, 20, 512] from decoder\n",
        "                K, V: [batch_size, 26, 2048] from encoder\n",
        "                attn_mask: [batch_size, len_q=20, len_k=26]\n",
        "        return _, attn: [batch_size, n_heads, len_q, len_k]\n",
        "        \"\"\"\n",
        "        residual, batch_size = Q, Q.size(0)\n",
        "        # q_s: [batch_size, n_heads=8, len_q, QKVdim] k_s/v_s: [batch_size, n_heads=8, len_k, QKVdim]\n",
        "        q_s = self.W_Q(Q).view(batch_size, -1, self.n_heads, self.QKVdim).transpose(1, 2)\n",
        "        k_s = self.W_K(K).view(batch_size, -1, self.n_heads, self.QKVdim).transpose(1, 2)\n",
        "        v_s = self.W_V(V).view(batch_size, -1, self.n_heads, self.QKVdim).transpose(1, 2)\n",
        "        # attn_mask: [batch_size, self.n_heads, len_q, len_k]\n",
        "        attn_mask = attn_mask.unsqueeze(1).repeat(1, self.n_heads, 1, 1)\n",
        "        # attn: [batch_size, n_heads, len_q, len_k]\n",
        "        # context: [batch_size, n_heads, len_q, QKVdim]\n",
        "        context, attn = ScaledDotProductAttention(self.QKVdim)(q_s, k_s, v_s, attn_mask)\n",
        "        # context: [batch_size, n_heads, len_q, QKVdim] -> [batch_size, len_q, n_heads * QKVdim]\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.n_heads * self.QKVdim).to(device)\n",
        "        # output: [batch_size, len_q, embed_dim]\n",
        "        output = self.W_O(context)\n",
        "        output = self.dropout(output)\n",
        "        return nn.LayerNorm(self.embed_dim).to(device)(output + residual), attn\n",
        "\n",
        "\n",
        "class PoswiseFeedForwardNet(nn.Module):\n",
        "    def __init__(self, embed_dim, d_ff, dropout):\n",
        "        '''\n",
        "        PosewiseFeedForwardNet\n",
        "        embed_dim = 300\n",
        "        d_ff      = dim_size\n",
        "        dropout`  = 0.1\n",
        "        '''\n",
        "        super(PoswiseFeedForwardNet, self).__init__()\n",
        "        \"\"\"\n",
        "        Two fc layers can also be described by two cnn with kernel_size=1.\n",
        "        \"\"\"\n",
        "        self.conv1 = nn.Conv1d(in_channels=embed_dim, out_channels=d_ff, kernel_size=1)\n",
        "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=embed_dim, kernel_size=1)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        encoder: inputs: [batch_size, len_q=26, embed_dim=2048]\n",
        "        decoder: inputs: [batch_size, max_len=20, embed_dim=512]\n",
        "        \"\"\"\n",
        "        residual = inputs\n",
        "        output = nn.ReLU()(self.conv1(inputs.transpose(1, 2)))\n",
        "        output = self.conv2(output).transpose(1, 2)\n",
        "        output = self.dropout(output)\n",
        "        return nn.LayerNorm(self.embed_dim).to(device)(output + residual)\n",
        "\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, embed_dim, dropout, n_heads):\n",
        "        '''\n",
        "        Decoder layer\n",
        "        embed_dim   = 300\n",
        "        droput      = 0.1\n",
        "        n_heads     = 6\n",
        "        '''\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.dec_self_attn = Multi_Head_Attention(Q_dim=embed_dim, K_dim=embed_dim, QKVdim=64, n_heads=n_heads, dropout=dropout)\n",
        "        self.dec_enc_attn = Multi_Head_Attention(Q_dim=embed_dim, K_dim=2048, QKVdim=64, n_heads=n_heads, dropout=dropout)\n",
        "        self.pos_ffn = PoswiseFeedForwardNet(embed_dim=embed_dim, d_ff=2048, dropout=dropout)\n",
        "\n",
        "    def forward(self, dec_inputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask):\n",
        "        \"\"\"\n",
        "        :param dec_inputs: [batch_size, max_len=20, embed_dim=512]\n",
        "        :param enc_outputs: [batch_size, num_pixels=26, 2048]\n",
        "        :param dec_self_attn_mask: [batch_size, 20, 20]\n",
        "        :param dec_enc_attn_mask: [batch_size, 20, 26]\n",
        "        \"\"\"\n",
        "        # print(dec_inputs.shape, enc_outputs.shape, dec_self_attn_mask.shape, dec_enc_attn_mask.shape)\n",
        "\n",
        "        dec_outputs, dec_self_attn = self.dec_self_attn(dec_inputs, dec_inputs, dec_inputs, dec_self_attn_mask)\n",
        "        dec_outputs, dec_enc_attn = self.dec_enc_attn(dec_outputs, enc_outputs, enc_outputs, dec_enc_attn_mask)\n",
        "        dec_outputs = self.pos_ffn(dec_outputs)\n",
        "        return dec_outputs, dec_self_attn, dec_enc_attn\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, n_layers, vocab_size, embed_dim, dropout, n_heads, answer_len):\n",
        "        '''\n",
        "        Transformer decoder\n",
        "        n_layers    = 6\n",
        "        vocab_size  = tokenizer length\n",
        "        embed_fim   = 300\n",
        "        dropout     = 0.1\n",
        "        n_heads     = 6\n",
        "        answer_len  = 20\n",
        "        '''\n",
        "        super(Decoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.anwer_len = answer_len\n",
        "        self.tgt_emb = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "        self.pos_emb = nn.Embedding.from_pretrained(self.get_position_embedding_table(embed_dim), freeze=True)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.layers = nn.ModuleList([DecoderLayer(embed_dim, dropout, n_heads) for _ in range(n_layers)])\n",
        "        self.projection = nn.Linear(embed_dim, vocab_size, bias=False)\n",
        "\n",
        "    def get_position_embedding_table(self, embed_dim):\n",
        "        def cal_angle(position, hid_idx):\n",
        "            return position / np.power(10000, 2 * (hid_idx // 2) / embed_dim)\n",
        "        def get_posi_angle_vec(position):\n",
        "            return [cal_angle(position, hid_idx) for hid_idx in range(embed_dim)]\n",
        "\n",
        "        embedding_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(self.anwer_len)])\n",
        "        embedding_table[:, 0::2] = np.sin(embedding_table[:, 0::2])  # dim 2i\n",
        "        embedding_table[:, 1::2] = np.cos(embedding_table[:, 1::2])  # dim 2i+1\n",
        "        return torch.FloatTensor(embedding_table).to(device)\n",
        "\n",
        "    def get_attn_pad_mask(self, seq_q, seq_k):\n",
        "        batch_size, len_q = seq_q.size()\n",
        "        batch_size, len_k = seq_k.size()\n",
        "        # In wordmap, <pad>:0\n",
        "        # pad_attn_mask: [batch_size, 1, len_k], one is masking\n",
        "        pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)\n",
        "        return pad_attn_mask.expand(batch_size, len_q, len_k)  # [batch_size, len_q, len_k]\n",
        "\n",
        "    def get_attn_subsequent_mask(self, seq):\n",
        "        attn_shape = [seq.size(0), seq.size(1), seq.size(1)]\n",
        "        subsequent_mask = np.triu(np.ones(attn_shape), k=1)\n",
        "        subsequent_mask = torch.from_numpy(subsequent_mask).byte().to(device)\n",
        "        return subsequent_mask\n",
        "\n",
        "    def forward(self, encoder_out, encoded_captions, caption_lengths):\n",
        "        \"\"\"\n",
        "        :param encoder_out: [batch_size, num_pixels=26, 2048]\n",
        "        :param encoded_captions: [batch_size, 20]\n",
        "        :param caption_lengths: [batch_size, 1]\n",
        "        \"\"\"\n",
        "        batch_size = encoder_out.size(0)\n",
        "        token_size = encoder_out.size(1)\n",
        "        # Sort input data by decreasing lengths.\n",
        "        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim=0, descending=True)\n",
        "        encoder_out = encoder_out[sort_ind]\n",
        "        encoded_captions = encoded_captions[sort_ind]\n",
        "        # We won't decode at the <end> position, since we've finished generating as soon as we generate <end>\n",
        "        # So, decoding lengths are actual lengths - 1\n",
        "        decode_lengths = (caption_lengths - 1).tolist()\n",
        "\n",
        "        '''# dec_outputs: [batch_size, max_len=20, embed_dim=512]\n",
        "        # dec_self_attn_pad_mask: [batch_size, len_q=20, len_k=20], 1 if id=0(<pad>)\n",
        "        # dec_self_attn_subsequent_mask: [batch_size, 20, 20], Upper triangle of an array with 1.\n",
        "        # dec_self_attn_mask for self-decoder attention, the position whose val > 0 will be masked.\n",
        "        # dec_enc_attn_mask for encoder-decoder attention.\n",
        "        # e.g. 9488, 23, 53, 74, 0, 0  |  dec_self_attn_mask:\n",
        "        # 0 1 1 1 2 2\n",
        "        # 0 0 1 1 2 2\n",
        "        # 0 0 0 1 2 2\n",
        "        # 0 0 0 0 2 2\n",
        "        # 0 0 0 0 1 2\n",
        "        # 0 0 0 0 1 1'''\n",
        "        dec_outputs = self.tgt_emb(encoded_captions) + self.pos_emb(torch.LongTensor([list(range(self.anwer_len))]*batch_size).to(device))\n",
        "        dec_outputs = self.dropout(dec_outputs)\n",
        "        dec_self_attn_pad_mask = self.get_attn_pad_mask(encoded_captions, encoded_captions)\n",
        "        dec_self_attn_subsequent_mask = self.get_attn_subsequent_mask(encoded_captions)\n",
        "        dec_self_attn_mask = torch.gt((dec_self_attn_pad_mask + dec_self_attn_subsequent_mask), 0)\n",
        "        dec_enc_attn_mask = (torch.tensor(np.zeros((batch_size, self.anwer_len, token_size))).to(device) == torch.tensor(np.ones((batch_size, self.anwer_len, token_size))).to(device))\n",
        "\n",
        "        dec_self_attns, dec_enc_attns = [], []\n",
        "        for layer in self.layers:\n",
        "            # attn: [batch_size, n_heads, len_q, len_k]\n",
        "            dec_outputs, dec_self_attn, dec_enc_attn = layer(dec_outputs, encoder_out, dec_self_attn_mask, dec_enc_attn_mask)\n",
        "            dec_self_attns.append(dec_self_attn)\n",
        "            dec_enc_attns.append(dec_enc_attn)\n",
        "        predictions = self.projection(dec_outputs)\n",
        "        return predictions, encoded_captions, decode_lengths, sort_ind, dec_self_attns, dec_enc_attns\n",
        "\n",
        "\n",
        "'''\n",
        "VisualBert Encoder + Transformer decoder\n",
        "'''\n",
        "class VisualBertSentence(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embed_dim, encoder_layers, decoder_layers, dropout=0.1, n_heads=8, answer_len = 20):\n",
        "        '''\n",
        "        VisualBert Encoder + Transformer decoder\n",
        "        vocab_size     = tokenizer length\n",
        "        embed_dim      = 300\n",
        "        encoder_layers = 6\n",
        "        decoder_layers = 6\n",
        "        dropout        = 0.1\n",
        "        n_heads        = 6\n",
        "        answer_len     = 20\n",
        "        '''\n",
        "        super(VisualBertSentence, self).__init__()\n",
        "        self.encoder = VisualBertEncoder(vocab_size, encoder_layers, n_heads)\n",
        "        self.decoder = Decoder(decoder_layers, vocab_size, embed_dim, dropout, n_heads, answer_len)\n",
        "        self.embedding = self.decoder.tgt_emb\n",
        "\n",
        "    def load_pretrained_embeddings(self, embeddings):\n",
        "        self.embedding.weight = nn.Parameter(embeddings)\n",
        "\n",
        "    def fine_tune_embeddings(self, fine_tune=True):\n",
        "        for p in self.embedding.parameters():\n",
        "            p.requires_grad = fine_tune\n",
        "\n",
        "    def forward(self, inputs, imgs, encoded_captions, caption_lengths):\n",
        "        # Vision and text encoder output\n",
        "        encoder_outputs = self.encoder(inputs, imgs)\n",
        "\n",
        "        # predict answer using decoder model\n",
        "        predictions, encoded_captions, decode_lengths, sort_ind, dec_self_attns, dec_enc_attns = self.decoder(encoder_outputs['last_hidden_state'], encoded_captions, caption_lengths)\n",
        "        alphas = {\"enc_self_attns\": encoder_outputs['attentions'], \"dec_self_attns\": dec_self_attns, \"dec_enc_attns\": dec_enc_attns}\n",
        "        return predictions, encoded_captions, decode_lengths, alphas, sort_ind"
      ],
      "metadata": {
        "id": "Zv8MXL9mo4MQ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utils"
      ],
      "metadata": {
        "id": "alIq7g24kpkp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"\n",
        "    Keeps track of most recent, average, sum, and count of a metric.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "def adjust_learning_rate(optimizer, shrink_factor):\n",
        "    \"\"\"\n",
        "    Shrinks learning rate by a specified factor.\n",
        "\n",
        "    :param optimizer: optimizer whose learning rate must be shrunk.\n",
        "    :param shrink_factor: factor in interval (0, 1) to multiply learning rate with.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\nDECAYING learning rate.\")\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = param_group['lr'] * shrink_factor\n",
        "    print(\"The new learning rate is %f\\n\" % (optimizer.param_groups[0]['lr'],))\n",
        "\n",
        "\n",
        "def save_clf_checkpoint(checkpoint_dir, epoch, epochs_since_improvement, model, optimizer, Acc):\n",
        "    \"\"\"\n",
        "    Saves model checkpoint.\n",
        "    \"\"\"\n",
        "    state = {'epoch': epoch,\n",
        "             'epochs_since_improvement': epochs_since_improvement,\n",
        "             'Acc': Acc,\n",
        "             'model': model,\n",
        "             'optimizer': optimizer}\n",
        "    filename = checkpoint_dir + 'Best.pth.tar'\n",
        "    torch.save(state, filename)\n",
        "\n",
        "def calc_acc(y_true, y_pred):\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    return acc\n",
        "\n",
        "def calc_classwise_acc(y_true, y_pred):\n",
        "    matrix = confusion_matrix(y_true, y_pred)\n",
        "    classwise_acc = matrix.diagonal()/matrix.sum(axis=1)\n",
        "    return classwise_acc\n",
        "\n",
        "def calc_map(y_true, y_scores):\n",
        "    mAP = average_precision_score(y_true, y_scores,average=None)\n",
        "    return mAP\n",
        "\n",
        "def calc_precision_recall_fscore(y_true, y_pred):\n",
        "    precision, recall, fscore, _ = precision_recall_fscore_support(y_true, y_pred, average='macro', zero_division = 1)\n",
        "    return(precision, recall, fscore)\n",
        "\n",
        "\n",
        "def seed_everything(seed=27):\n",
        "    '''\n",
        "    Set random seed for reproducible experiments\n",
        "    Inputs: seed number\n",
        "    '''\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "gHVYjRD1Bi7a"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "class EndoVis18VQAGPTSentence(Dataset):\n",
        "    '''\n",
        "    \tseq: train_seq  = [2, 3, 4, 6, 7, 9, 10, 11, 12, 14, 15]\n",
        "    \t     val_seq    = [1, 5, 16]\n",
        "    \tfolder_head = '../dataset/EndoVis-18-VQA/seq_'\n",
        "        folder_tail = '/vqa2/Sentence/*.txt'\n",
        "    '''\n",
        "    def __init__(self, seq, folder_head, folder_tail, model_ver = None, transform=None):\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "                                transforms.Resize((300,256)),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
        "                                ])\n",
        "\n",
        "        filenames = []\n",
        "        for curr_seq in seq: filenames = filenames + glob.glob(folder_head + str(curr_seq) + folder_tail)\n",
        "\n",
        "        self.vqas = []\n",
        "        for file in filenames:\n",
        "            file_data = open(file, \"r\")\n",
        "            lines = [line.strip(\"\\n\") for line in file_data if line != \"\\n\"]\n",
        "            file_data.close()\n",
        "            for line in lines:\n",
        "                q_s, an_s = line.split('|')\n",
        "                q_s = q_s.split('&')\n",
        "                an_s = an_s.split(('&'))\n",
        "                for i in range(len(q_s)):\n",
        "                    q_a = q_s[i]+'|'+an_s[i]\n",
        "                    # print(file, q_a)\n",
        "                    self.vqas.append([file, q_a])\n",
        "        print('Total files: %d | Total question: %.d' %(len(filenames), len(self.vqas)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.vqas)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        loc = self.vqas[idx][0].split('/')\n",
        "\n",
        "        # img loc[3],\n",
        "        img_loc = os.path.join(loc[0],loc[1], 'left_frames',loc[-1].split('_')[0]+'.png')\n",
        "        if self.transform:\n",
        "            img = Image.open(img_loc)\n",
        "            img = self.transform(img)\n",
        "\n",
        "        # question and answer\n",
        "        question, answer = self.vqas[idx][1].split('|')\n",
        "        answer = '<|sep|> '+answer\n",
        "\n",
        "\n",
        "        return img, question, answer\n",
        "\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch import optim\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.utils.data  import DataLoader\n",
        "from transformers import GPT2Tokenizer\n",
        "from transformers import BertTokenizer\n",
        "import torch.backends.cudnn as cudnn\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "def train(args, train_dataloader, model, criterion, optimizer, epoch, tokenizer, device):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    total_loss = AverageMeter()\n",
        "\n",
        "    for i, ( imgs, questions, answers) in enumerate(tqdm(train_dataloader),0):\n",
        "\n",
        "        # prepare questions and answers\n",
        "        question_list = []\n",
        "        answer_list = []\n",
        "        for question in questions: question_list.append(question)\n",
        "        for answer in answers: answer_list.append(answer)\n",
        "\n",
        "        question_inputs = tokenizer(question_list, padding=\"max_length\", max_length= args.question_len, return_tensors=\"pt\")\n",
        "        answer_inputs = tokenizer(answer_list, padding=\"max_length\", max_length= args.answer_len, return_tensors=\"pt\")\n",
        "        answers_GT_ID = answer_inputs.input_ids.to(device)\n",
        "        answers_GT_len = torch.sum(answer_inputs.attention_mask, dim=1).unsqueeze(1).to(device)\n",
        "\n",
        "        # Visual features\n",
        "        imgs = imgs.to(device)\n",
        "        visual_len = 80\n",
        "\n",
        "        # model forward(question, img, answer)\n",
        "        # print('mob:', answers_GT_len.shape, answers_GT_len, len(answer_inputs))\n",
        "        logits, _, _, _, _ = model(question_inputs, imgs, answers_GT_ID, answers_GT_len)\n",
        "\n",
        "        # only consider loss on reference summary just like seq2seq models\n",
        "        # idx = args.answer_len + 1\n",
        "        shift_logits = logits[..., :-1, :].contiguous()\n",
        "        shift_labels = answer_inputs['input_ids'][..., 1:].contiguous() # 1 because answer has '<|sep|>' in front\n",
        "        shift_labels = shift_labels.to(device)\n",
        "\n",
        "        loss = criterion(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss.update(loss.item())\n",
        "\n",
        "    print(\"Epoch: {}/{} Loss: {:.6f} AVG_Loss: {:.6f}\".format(epoch, args.epochs, total_loss.val, total_loss.avg))\n",
        "\n",
        "def validate(args, val_loader, model, criterion, epoch, tokenizer, device, save_output = False):\n",
        "\n",
        "    references = []\n",
        "    hypotheses = []\n",
        "\n",
        "    model.eval()\n",
        "    total_loss = AverageMeter()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (imgs, questions, answers) in enumerate(tqdm(val_loader),0):\n",
        "\n",
        "            # prepare questions and answers\n",
        "            question_list = []\n",
        "            answer_list = []\n",
        "            for question in questions: question_list.append(question)\n",
        "            for answer in answers: answer_list.append(answer)\n",
        "\n",
        "            question_inputs = tokenizer(question_list, padding=\"max_length\",max_length= args.question_len, return_tensors=\"pt\")\n",
        "            answer_inputs = tokenizer(answer_list, padding=\"max_length\",max_length= args.answer_len, return_tensors=\"pt\")\n",
        "            answers_GT_ID = answer_inputs.input_ids.to(device)\n",
        "            answers_GT_len = torch.sum(answer_inputs.attention_mask, dim=1).unsqueeze(1).to(device)\n",
        "\n",
        "            # Visual features\n",
        "            imgs = imgs.to(device)\n",
        "            visual_len = 80\n",
        "\n",
        "            # model forward(question, img, answer)\n",
        "            logits, _, _, _, _ = model(question_inputs, imgs, answers_GT_ID, answers_GT_len)\n",
        "\n",
        "\n",
        "            # only consider loss on reference summary just like seq2seq models\n",
        "            # idx = args.question_len + 1\n",
        "            shift_logits = logits[..., 0:-1, :].contiguous()\n",
        "            shift_labels = answer_inputs['input_ids'][..., 1:].contiguous() # 1 because answer has '<|sep|>' in front\n",
        "\n",
        "            # copy for logits and labels for sentence decoding and blue-4 score calculation\n",
        "            logits_copy = logits.clone()\n",
        "            shift_labels_copy = shift_labels.clone()\n",
        "\n",
        "            # loss calculation\n",
        "            shift_labels = shift_labels.to(device)\n",
        "            loss = criterion(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "            total_loss.update(loss.item())\n",
        "\n",
        "            # references    - Ground truth answer\n",
        "            answer_GT_dec = tokenizer.batch_decode(shift_labels_copy, skip_special_tokens= True)\n",
        "            for answer_GT_dec_i in answer_GT_dec: references.append([answer_GT_dec_i.split()])\n",
        "            # print(references)\n",
        "\n",
        "            # Hypotheses - predicted answer\n",
        "            _, answer_Gen_id = torch.max(logits_copy, dim=2)\n",
        "            answer_Gen_dec = tokenizer.batch_decode(answer_Gen_id, skip_special_tokens= True)\n",
        "            for answer_Gen_dec_i in answer_Gen_dec: hypotheses.append(answer_Gen_dec_i.split())\n",
        "            # print(hypotheses)\n",
        "\n",
        "\n",
        "        # Calculate BLEU1~4\n",
        "        metrics = {}\n",
        "        metrics[\"Bleu_1\"] = corpus_bleu(references, hypotheses, weights=(1.00, 0.00, 0.00, 0.00))\n",
        "        metrics[\"Bleu_2\"] = corpus_bleu(references, hypotheses, weights=(0.50, 0.50, 0.00, 0.00))\n",
        "        metrics[\"Bleu_3\"] = corpus_bleu(references, hypotheses, weights=(0.33, 0.33, 0.33, 0.00))\n",
        "        metrics[\"Bleu_4\"] = corpus_bleu(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "\n",
        "        print(\"Epoch: {}/{} EVA LOSS: {:.6f} BLEU-1 {:.6f} BLEU2 {:.6f} BLEU3 {:.6f} BLEU-4 {:.6f}\".format\n",
        "          (epoch, args.epochs, total_loss.avg, metrics[\"Bleu_1\"],  metrics[\"Bleu_2\"],  metrics[\"Bleu_3\"],  metrics[\"Bleu_4\"]))\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def get_arg():\n",
        "    parser = argparse.ArgumentParser(description='VisualQuestionAnswerClassification')\n",
        "\n",
        "    # Model parameters\n",
        "    parser.add_argument('--emb_dim',        type=int,   default=300,                                help='dimension of word embeddings.')\n",
        "    parser.add_argument('--n_heads',        type=int,   default=8,                                  help='Multi-head attention.')\n",
        "    parser.add_argument('--dropout',        type=float, default=0.1,                                help='dropout')\n",
        "    parser.add_argument('--encoder_layers', type=int,   default=6,                                  help='the number of layers of encoder in Transformer.')\n",
        "    parser.add_argument('--decoder_layers', type=int,   default=6,                                  help='the number of layers of decoder in Transformer.')\n",
        "\n",
        "    # Training parameters\n",
        "    parser.add_argument('--epochs',         type=int,   default=80,                                 help='number of epochs to train for (if early stopping is not triggered).') #80, 26\n",
        "    parser.add_argument('--batch_size',     type=int,   default=50,                                 help='batch_size')\n",
        "    parser.add_argument('--workers',        type=int,   default=1,                                  help='for data-loading; right now, only 1 works with h5pys.')\n",
        "\n",
        "    # existing checkpoint\n",
        "    parser.add_argument('--checkpoint',     default=None,                                           help='path to checkpoint, None if none.')\n",
        "\n",
        "    parser.add_argument('--lr',             type=float, default=0.000001,                            help=' 0.00001, 0.000005')\n",
        "    parser.add_argument('--checkpoint_dir', default= 'checkpoints/efvlegpt2rs18/m18/v3_p_qf_',      help='m18/c80')\n",
        "    parser.add_argument('--dataset_type',   default= 'm18',                                         help='m18/c80')\n",
        "    parser.add_argument('--tokenizer_ver',  default= 'gpt2v1',                                      help='btv2/btv3/gpt2v1')\n",
        "    parser.add_argument('--model_subver',   default= 'v3',                                          help='V0,v1/v2/v3/v4')\n",
        "    parser.add_argument('--question_len',   default= 25,                                            help='25')\n",
        "    parser.add_argument('--answer_len',     default= 35,                                            help='25')\n",
        "    parser.add_argument('--model_ver',      default= 'efvlegpt2rs18',                               help='efvlegpt2rs18/efvlegpt2Swin/\"')  #vrvb/gpt2rs18/gpt2ViT/gpt2Swin/biogpt2rs18/vilgpt2vqa/efgpt2rs18gr/efvlegpt2Swingr\n",
        "    parser.add_argument('--vis_pos_emb',    default= 'pos',                                         help='None, zeroes, pos')\n",
        "    parser.add_argument('--patch_size',     default= 5,                                             help='1/2/3/4/5')\n",
        "\n",
        "    parser.add_argument('--validate',       default=False,                                          help='When only validation required False/True')\n",
        "\n",
        "    if 'ipykernel' in sys.modules:\n",
        "        args = parser.parse_args([])\n",
        "    else:\n",
        "        args = parser.parse_args()\n",
        "    return args\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    seed_everything()\n",
        "\n",
        "    args = get_arg()\n",
        "    args.lr = 0.00005\n",
        "    args.epochs = 2\n",
        "    args.checkpoint_dir='checkpoints/efvlegpt2rs18/m18_v1_z_qf_'\n",
        "    args.dataset_type='m18'\n",
        "    args.tokenizer_ver='gpt2v1'\n",
        "    args.model_ver='efvlegpt2rs18'\n",
        "    args.model_subver='v1'\n",
        "    args.vis_pos_emb='zeroes'\n",
        "    args.batch_size=40\n",
        "    os.makedirs('checkpoints/efvlegpt2rs18', exist_ok=True)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # sets device for model and PyTorch tensors\n",
        "    cudnn.benchmark = True  # set to true only if inputs to model are fixed size; otherwise lot of computational overhead\n",
        "    print('device =', device)\n",
        "\n",
        "    # best model initialize\n",
        "    start_epoch = 1\n",
        "    best_epoch = [0]\n",
        "    best_results = [0.0]\n",
        "    epochs_since_improvement = 0\n",
        "\n",
        "    # data location\n",
        "    train_seq = [2, 3, 4, 6, 7, 9, 10, 11, 12, 14, 15]\n",
        "    val_seq = [1, 5, 16]\n",
        "\n",
        "    folder_head = 'EndoVis-18-VQA/seq_'\n",
        "    folder_tail = '/vqa/Sentence/*.txt'\n",
        "\n",
        "    train_dataset = EndoVis18VQAGPTSentence(train_seq, folder_head, folder_tail, model_ver=args.model_ver)\n",
        "    train_dataloader = DataLoader(dataset=train_dataset, batch_size= args.batch_size, shuffle=True, num_workers=8)\n",
        "    val_dataset = EndoVis18VQAGPTSentence(val_seq, folder_head, folder_tail, model_ver=args.model_ver)\n",
        "    val_dataloader = DataLoader(dataset=val_dataset, batch_size= args.batch_size, shuffle=False, num_workers=8)\n",
        "\n",
        "    # tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "    # tokenizer.pad_token = tokenizer.eos_token\n",
        "    # tokenizer_length = len(tokenizer)\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    model = VisualBertSentence(vocab_size=len(tokenizer), embed_dim=args.emb_dim, encoder_layers=args.encoder_layers, decoder_layers=args.decoder_layers,\n",
        "                            dropout=args.dropout, n_heads=args.n_heads, answer_len=args.answer_len)\n",
        "    model = model.to(device)\n",
        "\n",
        "    pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
        "    print('model params: ', pytorch_total_params)\n",
        "\n",
        "    criterion = CrossEntropyLoss().to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
        "\n",
        "    for epoch in range(start_epoch, args.epochs):\n",
        "\n",
        "        if epochs_since_improvement > 0 and epochs_since_improvement % 5 == 0:\n",
        "            adjust_learning_rate(optimizer, 0.8)\n",
        "\n",
        "        # train\n",
        "        train(args, train_dataloader=train_dataloader, model = model, criterion=criterion, optimizer=optimizer, epoch=epoch, tokenizer = tokenizer, device = device)\n",
        "\n",
        "        # validation\n",
        "        metrics = validate(args, val_loader=val_dataloader, model = model, criterion=criterion, epoch=epoch, tokenizer = tokenizer, device = device)\n",
        "\n",
        "        if metrics[\"Bleu_4\"] >= best_results[0]:\n",
        "            epochs_since_improvement = 0\n",
        "\n",
        "            best_results[0] = metrics[\"Bleu_4\"]\n",
        "            best_epoch[0] = epoch\n",
        "            save_clf_checkpoint(args.checkpoint_dir, epoch, epochs_since_improvement, model, optimizer, best_results[0])\n",
        "        else:\n",
        "            epochs_since_improvement += 1\n",
        "            print(\"\\nEpochs since last improvement: %d\\n\" % (epochs_since_improvement,))\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "NslIOHMTkEWw",
        "outputId": "5ee1bc6f-1009-4684-8b1c-23584c6fa607"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device = cuda\n",
            "Total files: 1560 | Total question: 10574\n",
            "Total files: 447 | Total question: 3216\n",
            "model params:  301176908\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 265/265 [06:13<00:00,  1.41s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/2 Loss: 1.195600 AVG_Loss: 2.574225\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 81/81 [01:52<00:00,  1.38s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/2 EVA LOSS: 1.352161 BLEU-1 0.547054 BLEU2 0.491378 BLEU3 0.456378 BLEU-4 0.418068\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "save_clf_checkpoint() missing 2 required positional arguments: 'Acc' and 'final_args'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-dc4497636566>\u001b[0m in \u001b[0;36m<cell line: 222>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0mbest_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Bleu_4\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0mbest_epoch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m             \u001b[0msave_clf_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs_since_improvement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0mepochs_since_improvement\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: save_clf_checkpoint() missing 2 required positional arguments: 'Acc' and 'final_args'"
          ]
        }
      ]
    }
  ]
}