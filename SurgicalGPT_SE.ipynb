{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/tutorial_notebooks/blob/main/SurgicalGPT_SE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pU4iPZR0ucs",
        "outputId": "67f6cf93-399f-42b7-ec41-a7791dc7fc76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SurgicalGPT'...\n",
            "remote: Enumerating objects: 98, done.\u001b[K\n",
            "remote: Counting objects: 100% (98/98), done.\u001b[K\n",
            "remote: Compressing objects: 100% (61/61), done.\u001b[K\n",
            "remote: Total 98 (delta 48), reused 80 (delta 35), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (98/98), 752.31 KiB | 2.56 MiB/s, done.\n",
            "Resolving deltas: 100% (48/48), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/lalithjets/SurgicalGPT.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "google drive: https://drive.google.com/file/d/1K5YnSPMPvn2x1gtRAw2ZfxIqoIo2DX3I"
      ],
      "metadata": {
        "id": "hvfERkLBGAM8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7wl6PSm0xF9",
        "outputId": "d6606121-0a71-4c8b-d052-d081f1b135d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:138: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Failed to retrieve file url:\n",
            "\n",
            "\tCannot retrieve the public link of the file. You may need to change\n",
            "\tthe permission to 'Anyone with the link', or have had many accesses.\n",
            "\n",
            "You may still be able to access the file from the browser:\n",
            "\n",
            "\thttps://drive.google.com/uc?id=1WGdztykX3nW6pi_BKp4rO8nA7ESNRfVN\n",
            "\n",
            "but Gdown can't. Please check connections and permissions.\n",
            "unzip:  cannot find or open EndoVis-18-VQA.zip, EndoVis-18-VQA.zip.zip or EndoVis-18-VQA.zip.ZIP.\n"
          ]
        }
      ],
      "source": [
        "# Downloading the VQA EndoVis18 Dataset https://drive.google.com/file/d/1WGdztykX3nW6pi_BKp4rO8nA7ESNRfVN/view?usp=sharing\n",
        "!gdown --id 1WGdztykX3nW6pi_BKp4rO8nA7ESNRfVN\n",
        "\n",
        "# Unzipping the VQA EndoVis18 Dataset\\\n",
        "!unzip -q EndoVis-18-VQA.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import os\n",
        "\n",
        "download_with_pydrive = True\n",
        "\n",
        "class Downloader(object):\n",
        "  def __init__(self, use_pydrive):\n",
        "    self.use_pydrive = use_pydrive\n",
        "    current_directory = os.getcwd()\n",
        "    self.save_dir = '.'\n",
        "    if self.use_pydrive:\n",
        "      self.authenticate()\n",
        "\n",
        "  def authenticate(self):\n",
        "    auth.authenticate_user()\n",
        "    gauth = GoogleAuth()\n",
        "    gauth.credentials = GoogleCredentials.get_application_default()\n",
        "    self.drive = GoogleDrive(gauth)\n",
        "\n",
        "  def download_file(self, file_id, file_name):\n",
        "    file_dst = f'{self.save_dir}/{file_name}'\n",
        "    if os.path.exists(file_dst):\n",
        "      print(f'{file_name} already exists')\n",
        "      return\n",
        "    downloaded = self.drive.CreateFile({'id': file_id})\n",
        "    downloaded.FetchMetadata(fetch_all=True)\n",
        "    downloaded.GetContentFile(file_dst)\n",
        "\n",
        "downloader = Downloader(download_with_pydrive)\n",
        "samed_model = {'id': '1WGdztykX3nW6pi_BKp4rO8nA7ESNRfVN', 'name': 'EndoVis-18-VQA.zip'}\n",
        "downloader.download_file(file_id=samed_model['id'], file_name=samed_model['name'])\n",
        "\n",
        "!unzip -q EndoVis-18-VQA.zip"
      ],
      "metadata": {
        "id": "o09I9-n1_iuI",
        "outputId": "0d4b8475-f40d-482e-e319-630c4e37988c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:pydrive is deprecated and no longer maintained. We recommend that you migrate your projects to pydrive2, the maintained fork of pydrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q EndoVis-18-VQA.zip"
      ],
      "metadata": {
        "id": "cRfny44Z_QSl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset path correction as repo\n",
        "!mv -f EndoVis-18-VQA /content/SurgicalGPT/dataset"
      ],
      "metadata": {
        "id": "NtzbQyOoNypt"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training:"
      ],
      "metadata": {
        "id": "F2-FwQY6WXJ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/SurgicalGPT\n",
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.utils.data\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data  import DataLoader\n",
        "from transformers import BertTokenizer, GPT2Tokenizer\n",
        "from train import seed_everything\n",
        "from dataloaders.dataloaderGPT2Classification import EndoVis18VQAGPTClassification\n",
        "# from models.EFGPT2Classification import EFVLEGPT2RS18Classification, EFVLEGPT2SwinClassification\n",
        "from utils import save_clf_checkpoint, adjust_learning_rate, calc_acc, calc_precision_recall_fscore, calc_classwise_acc\n",
        "\n",
        "from transformers import  VisualBertConfig, GPT2Config\n",
        "from transformers import VisualBertModel, GPT2Model, ViTModel, SwinModel\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "def get_arg():\n",
        "    parser = argparse.ArgumentParser(description='VisualQuestionAnswerClassification')\n",
        "\n",
        "    # VB Model parameters\n",
        "    parser.add_argument('--emb_dim',        type=int,   default=300,                                help='dimension of word embeddings.')\n",
        "    parser.add_argument('--n_heads',        type=int,   default=8,                                  help='Multi-head attention.')\n",
        "    parser.add_argument('--dropout',        type=float, default=0.1,                                help='dropout')\n",
        "    parser.add_argument('--encoder_layers', type=int,   default=6,                                  help='the number of layers of encoder in Transformer.')\n",
        "\n",
        "    # Training parameters\n",
        "    parser.add_argument('--epochs',         type=int,   default=2,                                 help='number of epochs to train for (if early stopping is not triggered).') #80, 26\n",
        "    parser.add_argument('--batch_size',     type=int,   default=40,                                 help='batch_size')\n",
        "    parser.add_argument('--workers',        type=int,   default=1,                                  help='for data-loading; right now, only 1 works with h5pys.')\n",
        "    parser.add_argument('--print_freq',     type=int,   default=100,                                help='print training/validation stats every __ batches.')\n",
        "\n",
        "    # existing checkpoint\n",
        "    parser.add_argument('--checkpoint',     default=None,                                           help='path to checkpoint, None if none.')\n",
        "\n",
        "    parser.add_argument('--lr',             type=float, default=0.00001,                           help='0.000005, 0.00001, 0.000005')\n",
        "    parser.add_argument('--checkpoint_dir', default= 'checkpoints/efvlegpt2Swin/m18_v1_z_qf_',            help='med_vqa_c$version$/m18/c80/m18_vid$temporal_size$/c80_vid$temporal_size$') #clf_v1_2_1x1/med_vqa_c3\n",
        "    parser.add_argument('--dataset_type',   default= 'm18',                                          help='med_vqa/m18/c80/m18_vid/c80_vid')\n",
        "    parser.add_argument('--dataset_cat',    default= 'cat1',                                        help='cat1/cat2/cat3')\n",
        "    parser.add_argument('--tokenizer_ver',  default= 'gpt2v1',                                      help='btv2/btv3/gpt2v1')\n",
        "    parser.add_argument('--question_len',   default= 25,                                            help='25')\n",
        "    parser.add_argument('--model_ver',      default= 'efvlegpt2Swin',                                          help='vb/vbrm/efvlegpt2rs18/efvlegpt2Swin/\"')  #vrvb/gpt2rs18/gpt2ViT/gpt2Swin/biogpt2rs18/vilgpt2vqa/efgpt2rs18gr/efvlegpt2Swingr\n",
        "    parser.add_argument('--model_subver',   default= 'v1',                                          help='V0,v1/v2/v3/v4')\n",
        "    parser.add_argument('--vis_pos_emb',   default= 'zeroes',                                           help='None, zeroes, pos')\n",
        "    parser.add_argument('--patch_size',     default= 5,                                             help='1/2/3/4/5')\n",
        "\n",
        "    parser.add_argument('--num_class',      default= 2,                                             help='25')\n",
        "    # parser.add_argument('--temporal_size',  default= 1,                                             help='1/2/3/4/5')\n",
        "    parser.add_argument('--validate',       default=False,                                          help='When only validation required False/True')\n",
        "\n",
        "    if 'ipykernel' in sys.modules:\n",
        "        args = parser.parse_args([])\n",
        "    else:\n",
        "        args = parser.parse_args()\n",
        "    return args\n",
        "\n",
        "class EFVLEGPT2SwinClassification(nn.Module):\n",
        "    def __init__(self, num_class = 12, model_subver = 'v0', vis_pos_emb = None):\n",
        "        super(EFVLEGPT2SwinClassification, self).__init__()\n",
        "        '''\n",
        "        v0: visual embedding : Default patch1 + embedding form VB + GPT2 decoder\n",
        "        v1: visual embedding : Default patch1 + GPT2 decoder\n",
        "        '''\n",
        "\n",
        "        self.sub_ver = model_subver\n",
        "        self.vis_pos_emb = vis_pos_emb\n",
        "\n",
        "        ## image processing\n",
        "        self.img_feature_extractor = SwinModel.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\")\n",
        "\n",
        "\n",
        "        ## Question_embedding\n",
        "        question_embedder = GPT2Model.from_pretrained('gpt2')\n",
        "        self.question_embedder = question_embedder.wte\n",
        "\n",
        "        ## GPT2 visual_cotext_aware_decoder\n",
        "        self.VCAdecoder = GPT2Model.from_pretrained('gpt2')\n",
        "\n",
        "        ## intermediate_layers\n",
        "        self.intermediate_layer = nn.Linear(768, 512)  #(512+768)\n",
        "        self.se_layer = nn.Sequential(\n",
        "            nn.Linear(512, 512),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.LayerNorm = nn.BatchNorm1d(512)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "        ## classifier\n",
        "        self.classifier = nn.Linear(512, num_class)\n",
        "\n",
        "    def forward(self, input, img):\n",
        "\n",
        "        ## image encoder features\n",
        "        img['pixel_values'] = img['pixel_values'].to(device)\n",
        "        img_feature = self.img_feature_extractor(**img)\n",
        "\n",
        "        ## visual Embedding : id type 1, pos: zero / incremental\n",
        "        visual_embeds = img_feature[0]\n",
        "        visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float)\n",
        "        visual_attention_mask = visual_attention_mask.to(device)\n",
        "        visual_id_type = torch.ones(*visual_embeds.size()[:-1], dtype=torch.long, device=device)\n",
        "        visual_position_id = torch.zeros(*visual_embeds.size()[:-1], dtype=torch.long, device=device)\n",
        "\n",
        "        ## question embedding: id type 0, pose incremental\n",
        "        input['input_ids'] = input['input_ids'].to(device)\n",
        "        input['attention_mask'] = input['attention_mask'].to(device)\n",
        "\n",
        "        question_embeds = self.question_embedder(input['input_ids'])\n",
        "        question_attention_mask = input['attention_mask']\n",
        "        question_id_type = torch.zeros(*question_embeds.size()[:-1], dtype=torch.long, device=device)\n",
        "        question_position_id = torch.arange(0,question_embeds.size()[1])\n",
        "        question_position_id = torch.unsqueeze(question_position_id,0)\n",
        "        question_position_id = question_position_id.repeat(question_embeds.size()[0], 1)\n",
        "        question_position_id = question_position_id.to(device)\n",
        "\n",
        "        ## question first\n",
        "        inputs_embeds = torch.cat((question_embeds, visual_embeds), dim=1)\n",
        "        attention_mask = torch.cat((question_attention_mask, visual_attention_mask), dim=1)\n",
        "        token_type_ids = torch.cat((question_id_type, visual_id_type), dim=1)\n",
        "        position_ids = torch.cat((question_position_id, visual_position_id), dim=1)\n",
        "\n",
        "        ## VCA_GPT2 decoder\n",
        "        decoder_output = self.VCAdecoder(inputs_embeds=inputs_embeds, attention_mask=attention_mask, position_ids = position_ids, token_type_ids = token_type_ids)\n",
        "\n",
        "        decoder_output = decoder_output.last_hidden_state.swapaxes(1,2)\n",
        "        decoder_output = F.adaptive_avg_pool1d(decoder_output,1)\n",
        "        decoder_output = decoder_output.swapaxes(1,2).squeeze(1)#[40, 768]\n",
        "\n",
        "        ## intermediate layers\n",
        "        out =self.intermediate_layer(decoder_output)#40, 512]\n",
        "        out = torch.mul(out, self.se_layer(out))\n",
        "        out = self.LayerNorm(out)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        ## classifier\n",
        "        out = self.classifier(out)\n",
        "        # print(out.size())\n",
        "        return out\n",
        "\n",
        "def train(args, train_dataloader, model, criterion, optimizer, epoch, tokenizer, device):\n",
        "\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    label_true = None\n",
        "    label_pred = None\n",
        "    label_score = None\n",
        "\n",
        "    for i, (_, v_f, q, labels) in enumerate(train_dataloader,0):\n",
        "        questions = []\n",
        "        for question in q: questions.append(question)\n",
        "        inputs = tokenizer(questions, padding=\"max_length\",max_length= args.question_len, return_tensors=\"pt\")\n",
        "\n",
        "        # Visual features\n",
        "        if args.model_ver == \"efvlegpt2Swin\" or args.model_ver == 'efvlegpt2ViT':\n",
        "            visual_features = v_f\n",
        "            visual_features['pixel_values'] = torch.squeeze(visual_features['pixel_values'],1)\n",
        "        else:\n",
        "            visual_features = v_f.to(device)\n",
        "\n",
        "        # labels\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(inputs, visual_features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        scores, predicted = torch.max(F.softmax(outputs, dim=1).data, 1)\n",
        "        label_true = labels.data.cpu() if label_true == None else torch.cat((label_true, labels.data.cpu()), 0)\n",
        "        label_pred = predicted.data.cpu() if label_pred == None else torch.cat((label_pred, predicted.data.cpu()), 0)\n",
        "        label_score = scores.data.cpu() if label_score == None else torch.cat((label_score, scores.data.cpu()), 0)\n",
        "        # if i==1: break\n",
        "    # loss and acc\n",
        "    acc, c_acc = calc_acc(label_true, label_pred), calc_classwise_acc(label_true, label_pred)\n",
        "    precision, recall, fscore = calc_precision_recall_fscore(label_true, label_pred)\n",
        "    print('Train: epoch: %d loss: %.6f | Acc: %.6f | Precision: %.6f | Recall: %.6f | FScore: %.6f' %(epoch, total_loss, acc, precision, recall, fscore))\n",
        "    return acc\n",
        "\n",
        "\n",
        "def validate(args, val_loader, model, criterion, epoch, tokenizer, device, save_output = False):\n",
        "\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    label_true = None\n",
        "    label_pred = None\n",
        "    label_score = None\n",
        "    file_names = list()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (file_name, v_f, q, labels) in enumerate(val_loader,0):\n",
        "\n",
        "            # prepare questions\n",
        "            questions = []\n",
        "            for question in q: questions.append(question)\n",
        "            inputs = tokenizer(questions, padding=\"max_length\",max_length=args.question_len, return_tensors=\"pt\")\n",
        "\n",
        "            # Visual features\n",
        "            if args.model_ver == \"efvlegpt2Swin\" or args.model_ver == 'efvlegpt2ViT':\n",
        "                visual_features = v_f\n",
        "                visual_features['pixel_values'] = torch.squeeze(visual_features['pixel_values'],1)\n",
        "            else:\n",
        "                visual_features = v_f.to(device)\n",
        "\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(inputs, visual_features)\n",
        "\n",
        "            # loss\n",
        "            loss = criterion(outputs,labels)\n",
        "            total_loss += loss.item()\n",
        "            scores, predicted = torch.max(F.softmax(outputs, dim=1).data, 1)\n",
        "            label_true = labels.data.cpu() if label_true == None else torch.cat((label_true, labels.data.cpu()), 0)\n",
        "            label_pred = predicted.data.cpu() if label_pred == None else torch.cat((label_pred, predicted.data.cpu()), 0)\n",
        "            label_score = scores.data.cpu() if label_score == None else torch.cat((label_score, scores.data.cpu()), 0)\n",
        "            for f in file_name: file_names.append(f)\n",
        "            # if i==1: break\n",
        "\n",
        "    acc = calc_acc(label_true, label_pred)\n",
        "    c_acc = 0.0\n",
        "    precision, recall, fscore = calc_precision_recall_fscore(label_true, label_pred)\n",
        "    print('Test: epoch: %d loss: %.6f | Acc: %.6f | Precision: %.6f | Recall: %.6f | FScore: %.6f' %(epoch, total_loss, acc, precision, recall, fscore))\n",
        "\n",
        "    return (acc, c_acc, precision, recall, fscore)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    args = get_arg()\n",
        "    os.makedirs('checkpoints/efvlegpt2Swin', exist_ok=True)\n",
        "    seed_everything()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    start_epoch = 1\n",
        "    best_epoch = [0]\n",
        "    best_results = [0.0]\n",
        "    epochs_since_improvement = 0\n",
        "    final_args = {\"emb_dim\": args.emb_dim, \"n_heads\": args.n_heads, \"dropout\": args.dropout, \"encoder_layers\": args.encoder_layers}\n",
        "\n",
        "\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # data location\n",
        "    train_seq = [2, 3, 4, 6, 7, 9, 10, 11, 12, 14, 15]\n",
        "    val_seq = [1, 5, 16]\n",
        "\n",
        "    folder_head = 'dataset/EndoVis-18-VQA/seq_'\n",
        "    folder_tail = '/vqa/Classification/*.txt'\n",
        "\n",
        "    train_dataset = EndoVis18VQAGPTClassification(train_seq, folder_head, folder_tail, model_ver=args.model_ver)\n",
        "    train_dataloader = DataLoader(dataset=train_dataset, batch_size= args.batch_size, shuffle=True, num_workers=8)\n",
        "    val_dataset = EndoVis18VQAGPTClassification(val_seq, folder_head, folder_tail, model_ver=args.model_ver)\n",
        "    val_dataloader = DataLoader(dataset=val_dataset, batch_size= args.batch_size, shuffle=False, num_workers=8)\n",
        "\n",
        "    # num_classes\n",
        "    args.num_class = 18\n",
        "\n",
        "    model = EFVLEGPT2SwinClassification(num_class = args.num_class, model_subver = args.model_subver, vis_pos_emb = args.vis_pos_emb)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
        "\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "    for epoch in range(start_epoch, args.epochs):\n",
        "\n",
        "        if epochs_since_improvement > 0 and epochs_since_improvement % 5 == 0:\n",
        "            adjust_learning_rate(optimizer, 0.8)\n",
        "\n",
        "        # train\n",
        "        train_acc = train(args, train_dataloader=train_dataloader, model = model, criterion=criterion, optimizer=optimizer, epoch=epoch, tokenizer = tokenizer, device = device)\n",
        "\n",
        "        # validation\n",
        "        test_acc, test_c_acc, test_precision, test_recall, test_fscore = validate(args, val_loader=val_dataloader, model = model, criterion=criterion, epoch=epoch, tokenizer = tokenizer, device = device)\n",
        "\n",
        "        if test_acc >= best_results[0]:\n",
        "            print('Best Epoch:', epoch)\n",
        "            epochs_since_improvement = 0\n",
        "            best_results[0] = test_acc\n",
        "            best_epoch[0] = epoch\n",
        "            save_clf_checkpoint(args.checkpoint_dir, epoch, epochs_since_improvement, model, optimizer, best_results[0], final_args)\n",
        "        else:\n",
        "            epochs_since_improvement += 1\n",
        "            print(\"\\nEpochs since last improvement: %d\\n\" % (epochs_since_improvement,))\n"
      ],
      "metadata": {
        "id": "Wj1Az0gsWW6E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b486e35-d479-4f25-9623-b7716b5d0df9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/SurgicalGPT\n",
            "Total files: 1560 | Total question: 9014\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total files: 447 | Total question: 2769\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pElbqJsyYVHz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}