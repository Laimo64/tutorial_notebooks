{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/tutorial_notebooks/blob/main/SAMed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Customized Segment Anything Model for Medical Image Segmentation\n",
        "### [[Paper](https://arxiv.org/pdf/2304.13785.pdf)] [[Github](https://github.com/hitachinsk/SAMed)]\n",
        "---\n",
        "[Kaidong Zhang](https://hitachinsk.github.io/), and [Dong Liu](https://faculty.ustc.edu.cn/dongeliu/), technical report, 2023\n",
        "\n",
        "All rights reserved by the authors of SAMed\n",
        "\n",
        "We provide the entire inference pipeline in this page."
      ],
      "metadata": {
        "id": "1P2M4gZbKZWt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup environment"
      ],
      "metadata": {
        "id": "Id3D1PuuLQMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q einops==0.6.1 icecream==2.1.3 MedPy==0.4.0 monai==1.1.0 opencv_python==4.5.4.58 SimpleITK==2.2.1 tensorboardX==2.6 ml-collections==0.1.1 onnx==1.13.1 onnxruntime==1.14.1 tensorboardX torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmmYvx7FLUif",
        "outputId": "10c2e46d-21e8-4ad4-f354-2a05eb140b9b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/151.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.8/151.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'opencv-python' candidate (version 4.5.4.58 at https://files.pythonhosted.org/packages/ea/8c/e01428f31e473f765355c65c24f2dbd62a6a093a3248a9fa97bc65eeeb22/opencv_python-4.5.4.58-cp310-cp310-manylinux2014_x86_64.whl (from https://pypi.org/simple/opencv-python/) (requires-python:>=3.6))\n",
            "Reason for being yanked: deprecated, use  4.5.4.60\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.3/60.3 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.7/52.7 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m869.5/869.5 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for MedPy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ml-collections (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download codes, pretrained weights and test data"
      ],
      "metadata": {
        "id": "m-tSMFkgPhyc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare codes\n",
        "import os\n",
        "CODE_DIR = 'samed_codes'\n",
        "os.makedirs(f'./{CODE_DIR}')\n",
        "!git clone https://github.com/hitachinsk/SAMed.git $CODE_DIR\n",
        "os.chdir(f'./{CODE_DIR}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyB2eYACPtEX",
        "outputId": "e6b52c77-e08c-44e1-b6b1-96931d33ddfb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'samed_codes'...\n",
            "remote: Enumerating objects: 225, done.\u001b[K\n",
            "remote: Counting objects: 100% (102/102), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 225 (delta 86), reused 72 (delta 72), pack-reused 123 (from 1)\u001b[K\n",
            "Receiving objects: 100% (225/225), 635.01 KiB | 22.68 MiB/s, done.\n",
            "Resolving deltas: 100% (105/105), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import os\n",
        "\n",
        "download_with_pydrive = True\n",
        "\n",
        "class Downloader(object):\n",
        "  def __init__(self, use_pydrive):\n",
        "    self.use_pydrive = use_pydrive\n",
        "    current_directory = os.getcwd()\n",
        "    self.save_dir = '.'\n",
        "    if self.use_pydrive:\n",
        "      self.authenticate()\n",
        "\n",
        "  def authenticate(self):\n",
        "    auth.authenticate_user()\n",
        "    gauth = GoogleAuth()\n",
        "    gauth.credentials = GoogleCredentials.get_application_default()\n",
        "    self.drive = GoogleDrive(gauth)\n",
        "\n",
        "  def download_file(self, file_id, file_name):\n",
        "    file_dst = f'{self.save_dir}/{file_name}'\n",
        "    if os.path.exists(file_dst):\n",
        "      print(f'{file_name} already exists')\n",
        "      return\n",
        "    downloaded = self.drive.CreateFile({'id': file_id})\n",
        "    downloaded.FetchMetadata(fetch_all=True)\n",
        "    downloaded.GetContentFile(file_dst)\n",
        "\n",
        "downloader = Downloader(download_with_pydrive)\n",
        "samed_model = {'id': '1P0Bm-05l-rfeghbrT1B62v5eN-3A-uOr', 'name': 'epoch_159.pth'}\n",
        "sam_model = {'id': '1_oCdoEEu3mNhRfFxeWyRerOKt8OEUvcg', 'name': 'sam_vit_b_01ec64.pth'}\n",
        "test_data = {'id': '1RczbNSB37OzPseKJZ1tDxa5OO1IIICzK', 'name': 'test_vol_h5.zip'}\n",
        "downloader.download_file(file_id=samed_model['id'], file_name=samed_model['name'])\n",
        "downloader.download_file(file_id=sam_model['id'], file_name=sam_model['name'])\n",
        "downloader.download_file(file_id=test_data['id'], file_name=test_data['name'])"
      ],
      "metadata": {
        "id": "NI9jWQnsPty2",
        "outputId": "46a9564e-9030-4ff6-d6b7-feb1dae83ac2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:pydrive is deprecated and no longer maintained. We recommend that you migrate your projects to pydrive2, the maintained fork of pydrive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch_159.pth already exists\n",
            "sam_vit_b_01ec64.pth already exists\n",
            "test_vol_h5.zip already exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip test_vol_h5.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ng42nD8DSCp0",
        "outputId": "57c15289-9dec-474c-c99a-248a3290bbce"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  test_vol_h5.zip\n",
            "   creating: test_vol_h5/\n",
            "  inflating: test_vol_h5/case0038.npy.h5  \n",
            "  inflating: test_vol_h5/case0036.npy.h5  \n",
            "  inflating: test_vol_h5/case0035.npy.h5  \n",
            "  inflating: test_vol_h5/case0032.npy.h5  \n",
            "  inflating: test_vol_h5/case0029.npy.h5  \n",
            "  inflating: test_vol_h5/case0025.npy.h5  \n",
            "  inflating: test_vol_h5/case0022.npy.h5  \n",
            "  inflating: test_vol_h5/case0008.npy.h5  \n",
            "  inflating: test_vol_h5/case0004.npy.h5  \n",
            "  inflating: test_vol_h5/case0003.npy.h5  \n",
            "  inflating: test_vol_h5/case0001.npy.h5  \n",
            "  inflating: test_vol_h5/case0002.npy.h5  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Execute SAMed"
      ],
      "metadata": {
        "id": "ModmC4zTSSQj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install numpy==1.24.4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0X9mTXZwz8i",
        "outputId": "e0c73fa9-d3cc-4356-d614-a0f4010ed8b3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/17.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/17.3 MB\u001b[0m \u001b[31m118.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/17.3 MB\u001b[0m \u001b[31m110.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m12.4/17.3 MB\u001b[0m \u001b[31m115.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m136.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m136.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m136.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_, x, y = slice.shape\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPO7i2MlxEs-",
        "outputId": "0c6ae5ce-019d-4af4-add9-9f56d8fd83fc"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "512"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image = np.zeros((148, 512, 512))\n",
        "prediction = []\n",
        "num_slice = image.shape[0]\n",
        "for ind in range(0, num_slice, 10):\n",
        "    slice = image[ind:ind + 10, :, :]\n",
        "    print(slice.shape)\n",
        "    x, y = slice.shape[0], slice.shape[1]\n",
        "    inputs = torch.from_numpy(slice).unsqueeze(1).float().cuda()\n",
        "    inputs = repeat(inputs, 'b c h w -> b (repeat c) h w', repeat=3)\n",
        "    print(inputs.shape)\n",
        "    prediction.extend(inputs.cpu().detach().numpy())\n",
        "\n",
        "print(np.array(prediction).shape)\n",
        ""
      ],
      "metadata": {
        "id": "KGTX1N2Q36vd",
        "outputId": "32621705-ec14-4e41-a847-11cbc86595b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 512, 512)\n",
            "torch.Size([10, 3, 512, 512])\n",
            "(10, 512, 512)\n",
            "torch.Size([10, 3, 512, 512])\n",
            "(10, 512, 512)\n",
            "torch.Size([10, 3, 512, 512])\n",
            "(10, 512, 512)\n",
            "torch.Size([10, 3, 512, 512])\n",
            "(10, 512, 512)\n",
            "torch.Size([10, 3, 512, 512])\n",
            "(10, 512, 512)\n",
            "torch.Size([10, 3, 512, 512])\n",
            "(10, 512, 512)\n",
            "torch.Size([10, 3, 512, 512])\n",
            "(10, 512, 512)\n",
            "torch.Size([10, 3, 512, 512])\n",
            "(10, 512, 512)\n",
            "torch.Size([10, 3, 512, 512])\n",
            "(10, 512, 512)\n",
            "torch.Size([10, 3, 512, 512])\n",
            "(10, 512, 512)\n",
            "torch.Size([10, 3, 512, 512])\n",
            "(10, 512, 512)\n",
            "torch.Size([10, 3, 512, 512])\n",
            "(10, 512, 512)\n",
            "torch.Size([10, 3, 512, 512])\n",
            "(10, 512, 512)\n",
            "torch.Size([10, 3, 512, 512])\n",
            "(8, 512, 512)\n",
            "torch.Size([8, 3, 512, 512])\n",
            "(148, 3, 512, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchmetrics\n",
        "from torchmetrics.classification import Dice\n",
        "\n",
        "# Define Dice metric for binary segmentation (num_classes=1)\n",
        "dice = Dice(num_classes=1).to('cuda')  # Move Dice metric to GPU\n",
        "\n",
        "# Example 3D prediction and ground truth segmentation masks\n",
        "# Shape: (batch_size, num_classes, depth, height, width)\n",
        "pred = torch.rand((4, 1, 64, 128, 128), device='cuda')  # Simulated 3D prediction mask on GPU\n",
        "target = torch.randint(0, 2, (4, 1, 64, 128, 128), device='cuda')  # Simulated 3D ground truth mask on GPU\n",
        "\n",
        "# Apply thresholding to convert probabilities into binary masks (0 or 1)\n",
        "pred = (pred > 0.5).float()\n",
        "\n",
        "# Compute Dice coefficient for 3D volumes\n",
        "dice_score = dice(pred, target)\n",
        "\n",
        "print(f\"Dice coefficient (3D): {dice_score.item()}\")"
      ],
      "metadata": {
        "id": "DMVtR4ICI9BV",
        "outputId": "b4f58647-86d9-4268-97e5-69cfe1ad9c65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "The implied number of classes (from shape of inputs) does not match num_classes.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-9f4b2c30ec6f>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Compute Dice coefficient for 3D volumes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mdice_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Dice coefficient (3D): {dice_score.item()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchmetrics/metric.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_full_state_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_reduce_state_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchmetrics/metric.py\u001b[0m in \u001b[0;36m_forward_reduce_state_update\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;31m# calculate batch state and compute batch value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0mbatch_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchmetrics/metric.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    481\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m                     \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;34m\"Expected all tensors to be on\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchmetrics/classification/dice.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, preds, target)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;34m\"\"\"Update state with predictions and targets.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         tp, fp, tn, fn = _stat_scores_update(\n\u001b[0m\u001b[1;32m    211\u001b[0m             \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchmetrics/functional/classification/stat_scores.py\u001b[0m in \u001b[0;36m_stat_scores_update\u001b[0;34m(preds, target, reduce, mdmc_reduce, num_classes, top_k, threshold, multiclass, ignore_index, mode)\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0m_negative_index_dropped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m     preds, target, _ = _input_format_classification(\n\u001b[0m\u001b[1;32m    961\u001b[0m         \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m         \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchmetrics/utilities/checks.py\u001b[0m in \u001b[0;36m_input_format_classification\u001b[0;34m(preds, target, threshold, top_k, num_classes, multiclass, ignore_index)\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m     case = _check_classification_inputs(\n\u001b[0m\u001b[1;32m    413\u001b[0m         \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchmetrics/utilities/checks.py\u001b[0m in \u001b[0;36m_check_classification_inputs\u001b[0;34m(preds, target, threshold, num_classes, multiclass, top_k, ignore_index)\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0m_check_num_classes_mc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulticlass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimplied_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMULTILABEL\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m             \u001b[0m_check_num_classes_ml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulticlass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimplied_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;31m# Check that top_k is consistent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchmetrics/utilities/checks.py\u001b[0m in \u001b[0;36m_check_num_classes_ml\u001b[0;34m(num_classes, multiclass, implied_classes)\u001b[0m\n\u001b[1;32m    182\u001b[0m         )\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmulticlass\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnum_classes\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mimplied_classes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The implied number of classes (from shape of inputs) does not match num_classes.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The implied number of classes (from shape of inputs) does not match num_classes."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import tensor\n",
        "from torchmetrics.classification import Dice\n",
        "pred  = tensor([2, 0, 2, 1])\n",
        "target = tensor([1, 1, 2, 0])\n",
        "\n",
        "pred = torch.rand((4, 1, 64, 128, 128), device='cuda')  # Simulated 3D prediction mask on GPU\n",
        "target = torch.randint(0, 2, (4, 1, 64, 128, 128), device='cuda')  # Simulated 3D ground truth mask on GPU\n",
        "\n",
        "# Apply thresholding to convert probabilities into binary masks (0 or 1)\n",
        "pred = (pred > 0.5).float()\n",
        "dice = Dice(average='micro').to('cuda')\n",
        "print(pred.shape, target.shape)\n",
        "dice(pred, target)\n",
        "\n",
        "hd95_score = hd_metric(pred, target)\n"
      ],
      "metadata": {
        "id": "8FdG7PyaJJp6",
        "outputId": "6c23da79-6678-4904-e95a-9857f89c50f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 1, 64, 128, 128]) torch.Size([4, 1, 64, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from monai.metrics import HausdorffDistanceMetric\n",
        "\n",
        "# Initialize the Hausdorff Distance metric (3D), optionally for HD95\n",
        "hd_metric = HausdorffDistanceMetric(percentile=95, include_background=False, reduction=\"mean\", get_not_nans=False)\n",
        "\n",
        "# Example 3D prediction and ground truth segmentation masks\n",
        "# Shape: (batch_size, num_classes, depth, height, width)\n",
        "pred = torch.randint(0, 2, (4, 1, 64, 128, 128), dtype=torch.float32)  # Simulated 3D prediction mask\n",
        "target = torch.randint(0, 2, (4, 1, 64, 128, 128), dtype=torch.float32)  # Simulated 3D ground truth mask\n",
        "\n",
        "# Move tensors to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "pred = pred.to(device)\n",
        "target = target.to(device)\n",
        "# hd_metric = hd_metric.to(device)\n",
        "\n",
        "# Calculate HD95\n",
        "hd95_score = hd_metric(pred, target)\n",
        "\n",
        "print(f\"HD95 score: {hd95_score.item()}\")\n"
      ],
      "metadata": {
        "id": "YnurVaSuNZbT",
        "outputId": "797629b7-4540-419f-883f-5283bee9caaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "a Tensor with 4 elements cannot be converted to Scalar",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-2b6ec9c925a4>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mhd95_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhd_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"HD95 score: {hd95_score.item()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: a Tensor with 4 elements cannot be converted to Scalar"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from monai.metrics import HausdorffDistanceMetric\n",
        "hd95_metric = HausdorffDistanceMetric(percentile=95, include_background=False, reduction=\"mean\", get_not_nans=False)\n"
      ],
      "metadata": {
        "id": "H6OjIb-MLegn"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing:\n",
        "Fix numpy bool issue here: /usr/local/lib/python3.10/dist-packages/medpy/metric/binary.py<br>\n",
        "Old: result = numpy.atleast_1d(result.astype(numpy.bool)) <br>\n",
        "New: result = numpy.atleast_1d(result.astype(bool))"
      ],
      "metadata": {
        "id": "Kssy1bII-mhU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/samed_codes\n",
        "import os\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "import numpy as np\n",
        "import argparse\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.backends.cudnn as cudnn\n",
        "# from utils import test_single_volume\n",
        "from importlib import import_module\n",
        "from segment_anything import sam_model_registry\n",
        "from datasets.dataset_synapse import Synapse_dataset\n",
        "from icecream import ic\n",
        "from medpy import metric\n",
        "from scipy.ndimage import zoom\n",
        "import torch.nn as nn\n",
        "import SimpleITK as sitk\n",
        "import torch.nn.functional as F\n",
        "import imageio\n",
        "from einops import repeat\n",
        "from torchmetrics.classification import Dice\n",
        "from monai.metrics import HausdorffDistanceMetric\n",
        "# from torchmetrics.classification import HausdorffDistance\n",
        "\n",
        "dice_metric = Dice(average='micro').to('cuda')\n",
        "hd95_metric = HausdorffDistanceMetric(percentile=95, include_background=False, reduction=\"mean\", get_not_nans=False)\n",
        "# hausdorff_metric = HausdorffDistance(reduction='none').to('cuda')\n",
        "\n",
        "\n",
        "\n",
        "class_to_name = {1: 'spleen', 2: 'right kidney', 3: 'left kidney', 4: 'gallbladder', 5: 'liver', 6: 'stomach', 7: 'aorta', 8: 'pancreas'}\n",
        "\n",
        "def calculate_metric_percase(pred, gt):\n",
        "    pred[pred > 0] = 1\n",
        "    gt[gt > 0] = 1\n",
        "    if pred.sum() > 0 and gt.sum() > 0:\n",
        "        # print('\\ncalculate_metric_percase_pred', np.unique(pred), pred.shape)\n",
        "        # print('\\n calculate_metric_percase_gt', np.unique(gt), gt.shape)\n",
        "        dice = metric.binary.dc(pred, gt)\n",
        "        hd95 = 0# ßmetric.binary.hd95(pred, gt)\n",
        "\n",
        "        # pred = torch.tensor(pred).cuda().float()\n",
        "        # gt = torch.tensor(gt).cuda().int()\n",
        "        # dice = dice_metric (pred, gt)\n",
        "        # dice = dice.cpu().detach().numpy()\n",
        "\n",
        "        # hd95 = hd95_metric(pred, gt)\n",
        "        # hd95 = hd95.numpy()\n",
        "\n",
        "        return dice, hd95\n",
        "    elif pred.sum() > 0 and gt.sum() == 0:\n",
        "        return 1, 0\n",
        "    else:\n",
        "        return 0, 0\n",
        "\n",
        "def test_single_volume(image, label, net, classes, multimask_output, image_size=[512, 512],\n",
        "                       test_save_path=None, case=None):\n",
        "    image, label = image.squeeze(0).cpu().detach().numpy(), label.squeeze(0).cpu().detach().numpy()\n",
        "    # print('image, label:', image.shape, label.shape )\n",
        "    # prediction = np.zeros_like(label)\n",
        "    # for ind in range(image.shape[0]):\n",
        "    #     slice = image[ind, :, :]\n",
        "\n",
        "    local_batch_size = 50\n",
        "    prediction = []\n",
        "    num_slice = image.shape[0]\n",
        "    for ind in range(0, num_slice, local_batch_size):\n",
        "        slice = image[ind:ind + local_batch_size, :, :]\n",
        "        _, x, y = slice.shape\n",
        "        inputs = torch.from_numpy(slice).unsqueeze(1).float().cuda()\n",
        "        inputs = repeat(inputs, 'b c h w -> b (repeat c) h w', repeat=3)\n",
        "        net.eval()\n",
        "        with torch.no_grad():\n",
        "            print('model input:', inputs.shape)\n",
        "            outputs = net(inputs, multimask_output, image_size[0])\n",
        "            output_masks = outputs['masks']\n",
        "            out = torch.argmax(torch.softmax(output_masks, dim=1), dim=1).squeeze(0)\n",
        "            out = out.cpu().detach().numpy()\n",
        "            _, out_h, out_w = out.shape\n",
        "            if x != out_h or y != out_w:\n",
        "                pred = zoom(out, (x / out_h, y / out_w), order=0)\n",
        "            else:\n",
        "                pred = out\n",
        "            # prediction[ind] = pred\n",
        "            prediction.extend(pred)\n",
        "\n",
        "    print('test_single_volume prediction:', np.array(prediction).shape)\n",
        "    prediction = np.array(prediction)\n",
        "    metric_list = []\n",
        "    for i in range(1, classes + 1):\n",
        "        metric_list.append(calculate_metric_percase(prediction == i, label == i))\n",
        "\n",
        "    return metric_list\n",
        "\n",
        "def inference(args, multimask_output, db_config, model, test_save_path=None):\n",
        "    db_test = db_config['Dataset'](base_dir=args.volume_path, list_dir=args.list_dir, split='test_vol')\n",
        "    print('sample size:', len(db_test))\n",
        "    testloader = DataLoader(db_test, batch_size=1, shuffle=False, num_workers=1)\n",
        "    logging.info(f'{len(testloader)} test iterations per epoch')\n",
        "    model.eval()\n",
        "    metric_list = 0.0\n",
        "    for i_batch, sampled_batch in tqdm(enumerate(testloader)):\n",
        "        h, w = sampled_batch['image'].shape[2:]\n",
        "        image, label, case_name = sampled_batch['image'], sampled_batch['label'], sampled_batch['case_name'][0]\n",
        "        print(image.shape, label.shape)\n",
        "        metric_i = test_single_volume(image, label, model, classes=args.num_classes, multimask_output=multimask_output,\n",
        "                                      image_size=[args.img_size, args.img_size],\n",
        "                                      test_save_path=test_save_path, case=case_name)\n",
        "        metric_list += np.array(metric_i)\n",
        "        print('idx %d case %s mean_dice %f mean_hd95 %f' % (\n",
        "            i_batch, case_name, np.mean(metric_i, axis=0)[0], np.mean(metric_i, axis=0)[1]))\n",
        "        logging.info('idx %d case %s mean_dice %f mean_hd95 %f' % (\n",
        "            i_batch, case_name, np.mean(metric_i, axis=0)[0], np.mean(metric_i, axis=0)[1]))\n",
        "        if i_batch == 0:\n",
        "            break\n",
        "    metric_list = metric_list / len(db_test)\n",
        "    for i in range(1, args.num_classes + 1):\n",
        "        try:\n",
        "            logging.info('Mean class %d name %s mean_dice %f mean_hd95 %f' % (i, class_to_name[i], metric_list[i - 1][0], metric_list[i - 1][1]))\n",
        "        except:\n",
        "            logging.info('Mean class %d mean_dice %f mean_hd95 %f' % (i, metric_list[i - 1][0], metric_list[i - 1][1]))\n",
        "    performance = np.mean(metric_list, axis=0)[0]\n",
        "    mean_hd95 = np.mean(metric_list, axis=0)[1]\n",
        "    logging.info('Testing performance in best val model: mean_dice : %f mean_hd95 : %f' % (performance, mean_hd95))\n",
        "    logging.info(\"Testing Finished!\")\n",
        "    return 1\n",
        "\n",
        "\n",
        "def config_to_dict(config):\n",
        "    items_dict = {}\n",
        "    with open(config, 'r') as f:\n",
        "        items = f.readlines()\n",
        "    for i in range(len(items)):\n",
        "        key, value = items[i].strip().split(': ')\n",
        "        items_dict[key] = value\n",
        "    return items_dict\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--config', type=str, default=None, help='The config file provided by the trained model')\n",
        "    parser.add_argument('--volume_path', type=str, default='testset/test_vol_h5/')\n",
        "    parser.add_argument('--dataset', type=str, default='Synapse', help='Experiment name')\n",
        "    parser.add_argument('--num_classes', type=int, default=8)\n",
        "    parser.add_argument('--list_dir', type=str, default='./lists/lists_Synapse/', help='list_dir')\n",
        "    parser.add_argument('--output_dir', type=str, default='/output')\n",
        "    parser.add_argument('--img_size', type=int, default=512, help='Input image size of the network')\n",
        "    parser.add_argument('--input_size', type=int, default=224, help='The input size for training SAM model')\n",
        "    parser.add_argument('--seed', type=int,\n",
        "                        default=1234, help='random seed')\n",
        "    parser.add_argument('--is_savenii', action='store_true', help='Whether to save results during inference')\n",
        "    parser.add_argument('--deterministic', type=int, default=1, help='whether use deterministic training')\n",
        "    parser.add_argument('--ckpt', type=str, default='checkpoints/sam_vit_b_01ec64.pth',\n",
        "                        help='Pretrained checkpoint')\n",
        "    parser.add_argument('--lora_ckpt', type=str, default='checkpoints/epoch_159.pth', help='The checkpoint from LoRA')\n",
        "    parser.add_argument('--vit_name', type=str, default='vit_b', help='Select one vit model')\n",
        "    parser.add_argument('--rank', type=int, default=4, help='Rank for LoRA adaptation')\n",
        "    parser.add_argument('--module', type=str, default='sam_lora_image_encoder')\n",
        "\n",
        "    args = parser.parse_args([])\n",
        "    args.volume_path = 'test_vol_h5'\n",
        "    args.output_dir = 'results'\n",
        "    args.ckpt = 'sam_vit_b_01ec64.pth'\n",
        "    args.lora_ckpt = 'epoch_159.pth'\n",
        "\n",
        "    if args.config is not None:\n",
        "        # overwtite default configurations with config file\\\n",
        "        config_dict = config_to_dict(args.config)\n",
        "        for key in config_dict:\n",
        "            setattr(args, key, config_dict[key])\n",
        "\n",
        "    if not args.deterministic:\n",
        "        cudnn.benchmark = True\n",
        "        cudnn.deterministic = False\n",
        "    else:\n",
        "        cudnn.benchmark = False\n",
        "        cudnn.deterministic = True\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    torch.cuda.manual_seed(args.seed)\n",
        "    dataset_name = args.dataset\n",
        "    dataset_config = {\n",
        "        'Synapse': {\n",
        "            'Dataset': Synapse_dataset,\n",
        "            'volume_path': args.volume_path,\n",
        "            'list_dir': args.list_dir,\n",
        "            'num_classes': args.num_classes,\n",
        "            'z_spacing': 1\n",
        "        }\n",
        "    }\n",
        "    if not os.path.exists(args.output_dir):\n",
        "        os.makedirs(args.output_dir)\n",
        "\n",
        "    # register model\n",
        "    sam, img_embedding_size = sam_model_registry[args.vit_name](image_size=args.img_size,\n",
        "                                                                    num_classes=args.num_classes,\n",
        "                                                                    checkpoint=args.ckpt, pixel_mean=[0, 0, 0],\n",
        "                                                                    pixel_std=[1, 1, 1])\n",
        "\n",
        "    pkg = import_module(args.module)\n",
        "    net = pkg.LoRA_Sam(sam, args.rank).cuda()\n",
        "\n",
        "    assert args.lora_ckpt is not None\n",
        "    net.load_lora_parameters(args.lora_ckpt)\n",
        "\n",
        "    if args.num_classes > 1:\n",
        "        multimask_output = True\n",
        "    else:\n",
        "        multimask_output = False\n",
        "\n",
        "    # initialize log\n",
        "    log_folder = os.path.join(args.output_dir, 'test_log')\n",
        "    os.makedirs(log_folder, exist_ok=True)\n",
        "    logging.basicConfig(filename=log_folder + '/' + 'log.txt', level=logging.INFO,\n",
        "                        format='[%(asctime)s.%(msecs)03d] %(message)s', datefmt='%H:%M:%S')\n",
        "    logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))\n",
        "    logging.info(str(args))\n",
        "\n",
        "    if args.is_savenii:\n",
        "        test_save_path = os.path.join(args.output_dir, 'predictions')\n",
        "        os.makedirs(test_save_path, exist_ok=True)\n",
        "    else:\n",
        "        test_save_path = None\n",
        "    inference(args, multimask_output, dataset_config[dataset_name], net, test_save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-y2mu4ndPfOZ",
        "outputId": "b28a6057-4a19-4c1e-888b-ce7db5bbb2bf"
      },
      "execution_count": 16,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/samed_codes\n",
            "sample size: 12\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 148, 512, 512]) torch.Size([1, 148, 512, 512])\n",
            "model input: torch.Size([50, 3, 512, 512])\n",
            "model input: torch.Size([50, 3, 512, 512])\n",
            "model input: torch.Size([48, 3, 512, 512])\n",
            "test_single_volume prediction: (148, 512, 512)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r0it [02:29, ?it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "idx 0 case case0008 mean_dice 0.681535 mean_hd95 16.097952\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "0it [02:24, ?it/s]idx 0 case case0008 mean_dice 0.681535 mean_hd95 16.097952"
      ],
      "metadata": {
        "id": "NVGIYOjPKEbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing with command:"
      ],
      "metadata": {
        "id": "dPc3iQTZ-pTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "python test.py --volume_path test_vol_h5 --output_dir results --ckpt sam_vit_b_01ec64.pth --lora_ckpt epoch_159.pth"
      ],
      "metadata": {
        "id": "TsKn0gVsSUYz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ede4aa0-3f6e-4d13-e98c-53fba3c2d6dd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Process is terminated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UeuJxMJbRspH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}