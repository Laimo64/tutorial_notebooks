{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/tutorial_notebooks/blob/main/SAMed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Customized Segment Anything Model for Medical Image Segmentation\n",
        "### [[Paper](https://arxiv.org/pdf/2304.13785.pdf)] [[Github](https://github.com/hitachinsk/SAMed)]\n",
        "---\n",
        "[Kaidong Zhang](https://hitachinsk.github.io/), and [Dong Liu](https://faculty.ustc.edu.cn/dongeliu/), technical report, 2023\n",
        "\n",
        "All rights reserved by the authors of SAMed\n",
        "\n",
        "We provide the entire inference pipeline in this page."
      ],
      "metadata": {
        "id": "1P2M4gZbKZWt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup environment"
      ],
      "metadata": {
        "id": "Id3D1PuuLQMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q einops==0.6.1 icecream==2.1.3 MedPy==0.4.0 monai==1.1.0 opencv_python==4.5.4.58 SimpleITK==2.2.1 tensorboardX==2.6 ml-collections==0.1.1 onnx==1.13.1 onnxruntime==1.14.1 tensorboardX torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmmYvx7FLUif",
        "outputId": "c3ef6828-a2b6-4d8c-9161-053027d619a0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/151.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.8/151.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'opencv-python' candidate (version 4.5.4.58 at https://files.pythonhosted.org/packages/ea/8c/e01428f31e473f765355c65c24f2dbd62a6a093a3248a9fa97bc65eeeb22/opencv_python-4.5.4.58-cp310-cp310-manylinux2014_x86_64.whl (from https://pypi.org/simple/opencv-python/) (requires-python:>=3.6))\n",
            "Reason for being yanked: deprecated, use  4.5.4.60\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.3/60.3 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.7/52.7 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m869.5/869.5 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for MedPy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ml-collections (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download codes, pretrained weights and test data"
      ],
      "metadata": {
        "id": "m-tSMFkgPhyc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare codes\n",
        "import os\n",
        "CODE_DIR = 'samed_codes'\n",
        "os.makedirs(f'./{CODE_DIR}')\n",
        "!git clone https://github.com/hitachinsk/SAMed.git $CODE_DIR\n",
        "os.chdir(f'./{CODE_DIR}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyB2eYACPtEX",
        "outputId": "e1a86112-d377-40a5-e7eb-c2a775677641"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'samed_codes'...\n",
            "remote: Enumerating objects: 225, done.\u001b[K\n",
            "remote: Counting objects: 100% (102/102), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 225 (delta 86), reused 72 (delta 72), pack-reused 123 (from 1)\u001b[K\n",
            "Receiving objects: 100% (225/225), 635.01 KiB | 2.04 MiB/s, done.\n",
            "Resolving deltas: 100% (105/105), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1RczbNSB37OzPseKJZ1tDxa5OO1IIICzK\n",
        "!gdown 1P0Bm-05l-rfeghbrT1B62v5eN-3A-uOr\n",
        "!gdown 1_oCdoEEu3mNhRfFxeWyRerOKt8OEUvcg"
      ],
      "metadata": {
        "id": "Evam59NSYTFQ",
        "outputId": "0d27f63f-89cd-4fd7-8c9c-a584936ea4e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=157Q_e4oR4b-Wo4lywUIvQSHnNb7LFgR5\n",
            "From (redirected): https://drive.google.com/uc?id=157Q_e4oR4b-Wo4lywUIvQSHnNb7LFgR5&confirm=t&uuid=5bb03065-3717-42c0-883f-fb0193292be1\n",
            "To: /content/samed_codes/Endonasal_Slices_All.zip\n",
            "100% 914M/914M [00:22<00:00, 40.0MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1P0Bm-05l-rfeghbrT1B62v5eN-3A-uOr\n",
            "To: /content/samed_codes/epoch_159.pth\n",
            "100% 19.7M/19.7M [00:01<00:00, 15.8MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1_oCdoEEu3mNhRfFxeWyRerOKt8OEUvcg\n",
            "From (redirected): https://drive.google.com/uc?id=1_oCdoEEu3mNhRfFxeWyRerOKt8OEUvcg&confirm=t&uuid=f9f83579-9c56-4b7f-addf-ae40f5c81e31\n",
            "To: /content/samed_codes/sam_vit_b_01ec64.pth\n",
            "100% 375M/375M [00:07<00:00, 49.7MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip test_vol_h5.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ng42nD8DSCp0",
        "outputId": "911ebd3f-6414-4303-c4bf-d20014740954"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  test_vol_h5.zip\n",
            "   creating: test_vol_h5/\n",
            "  inflating: test_vol_h5/case0038.npy.h5  \n",
            "  inflating: test_vol_h5/case0036.npy.h5  \n",
            "  inflating: test_vol_h5/case0035.npy.h5  \n",
            "  inflating: test_vol_h5/case0032.npy.h5  \n",
            "  inflating: test_vol_h5/case0029.npy.h5  \n",
            "  inflating: test_vol_h5/case0025.npy.h5  \n",
            "  inflating: test_vol_h5/case0022.npy.h5  \n",
            "  inflating: test_vol_h5/case0008.npy.h5  \n",
            "  inflating: test_vol_h5/case0004.npy.h5  \n",
            "  inflating: test_vol_h5/case0003.npy.h5  \n",
            "  inflating: test_vol_h5/case0001.npy.h5  \n",
            "  inflating: test_vol_h5/case0002.npy.h5  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Execute SAMed"
      ],
      "metadata": {
        "id": "ModmC4zTSSQj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install numpy==1.24.4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0X9mTXZwz8i",
        "outputId": "e0c73fa9-d3cc-4356-d614-a0f4010ed8b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/17.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/17.3 MB\u001b[0m \u001b[31m118.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/17.3 MB\u001b[0m \u001b[31m110.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m12.4/17.3 MB\u001b[0m \u001b[31m115.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m136.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m136.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m136.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing:\n",
        "Fix numpy bool issue here: /usr/local/lib/python3.10/dist-packages/medpy/metric/binary.py<br>\n",
        "Old: result = numpy.atleast_1d(result.astype(numpy.bool)) <br>\n",
        "New: result = numpy.atleast_1d(result.astype(bool))"
      ],
      "metadata": {
        "id": "Kssy1bII-mhU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/samed_codes\n",
        "import os\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "import numpy as np\n",
        "import argparse\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.backends.cudnn as cudnn\n",
        "# from utils import test_single_volume\n",
        "from importlib import import_module\n",
        "from segment_anything import sam_model_registry\n",
        "from datasets.dataset_synapse import Synapse_dataset\n",
        "from icecream import ic\n",
        "from medpy import metric\n",
        "from scipy.ndimage import zoom\n",
        "import torch.nn as nn\n",
        "import SimpleITK as sitk\n",
        "import torch.nn.functional as F\n",
        "import imageio\n",
        "from einops import repeat\n",
        "from torchmetrics.classification import Dice\n",
        "from monai.metrics import HausdorffDistanceMetric\n",
        "# from torchmetrics.classification import HausdorffDistance\n",
        "\n",
        "dice_metric = Dice(average='micro').to('cuda')\n",
        "hd95_metric = HausdorffDistanceMetric(percentile=95, include_background=False, reduction=\"mean\", get_not_nans=False)\n",
        "# hausdorff_metric = HausdorffDistance(reduction='none').to('cuda')\n",
        "\n",
        "\n",
        "\n",
        "class_to_name = {1: 'spleen', 2: 'right kidney', 3: 'left kidney', 4: 'gallbladder', 5: 'liver', 6: 'stomach', 7: 'aorta', 8: 'pancreas'}\n",
        "\n",
        "def calculate_metric_percase(pred, gt):\n",
        "    pred[pred > 0] = 1\n",
        "    gt[gt > 0] = 1\n",
        "    if pred.sum() > 0 and gt.sum() > 0:\n",
        "        # print('\\ncalculate_metric_percase_pred', np.unique(pred), pred.shape)\n",
        "        # print('\\n calculate_metric_percase_gt', np.unique(gt), gt.shape)\n",
        "        # dice = metric.binary.dc(pred, gt)\n",
        "        hd95 = 0# ßmetric.binary.hd95(pred, gt)\n",
        "\n",
        "        pred = torch.tensor(pred).cuda().float()\n",
        "        gt = torch.tensor(gt).cuda().int()\n",
        "        dice = dice_metric (pred, gt).cpu().detach().numpy()\n",
        "        # dice = dice.cpu().detach().numpy()\n",
        "\n",
        "        # hd95 = hd95_metric(pred, gt)\n",
        "        # hd95 = hd95.numpy()\n",
        "\n",
        "        return dice, hd95\n",
        "    elif pred.sum() > 0 and gt.sum() == 0:\n",
        "        return 1, 0\n",
        "    else:\n",
        "        return 0, 0\n",
        "\n",
        "def test_single_volume(image, label, net, classes, multimask_output, image_size=[512, 512],\n",
        "                       test_save_path=None, case=None):\n",
        "    image, label = image.squeeze(0).cpu().detach().numpy(), label.squeeze(0).cpu().detach().numpy()\n",
        "    # print('image, label:', image.shape, label.shape )\n",
        "    # prediction = np.zeros_like(label)\n",
        "    # for ind in range(image.shape[0]):\n",
        "    #     slice = image[ind, :, :]\n",
        "\n",
        "    local_batch_size = 50\n",
        "    prediction = []\n",
        "    num_slice = image.shape[0]\n",
        "    for ind in range(0, num_slice, local_batch_size):\n",
        "        slice = image[ind:ind + local_batch_size, :, :]\n",
        "        _, x, y = slice.shape\n",
        "        inputs = torch.from_numpy(slice).unsqueeze(1).float().cuda()\n",
        "        inputs = repeat(inputs, 'b c h w -> b (repeat c) h w', repeat=3)\n",
        "        net.eval()\n",
        "        with torch.no_grad():\n",
        "            print('model input:', inputs.shape)\n",
        "            outputs = net(inputs, multimask_output, image_size[0])\n",
        "            output_masks = outputs['masks']\n",
        "            out = torch.argmax(torch.softmax(output_masks, dim=1), dim=1).squeeze(0)\n",
        "            out = out.cpu().detach().numpy()\n",
        "            _, out_h, out_w = out.shape\n",
        "            if x != out_h or y != out_w:\n",
        "                pred = zoom(out, (x / out_h, y / out_w), order=0)\n",
        "            else:\n",
        "                pred = out\n",
        "            # prediction[ind] = pred\n",
        "            prediction.extend(pred)\n",
        "\n",
        "    print('test_single_volume prediction:', np.array(prediction).shape)\n",
        "    prediction = np.array(prediction)\n",
        "    metric_list = []\n",
        "    for i in range(1, classes + 1):\n",
        "        metric_list.append(calculate_metric_percase(prediction == i, label == i))\n",
        "\n",
        "    return metric_list\n",
        "\n",
        "def inference(args, multimask_output, db_config, model, test_save_path=None):\n",
        "    db_test = db_config['Dataset'](base_dir=args.volume_path, list_dir=args.list_dir, split='test_vol')\n",
        "    print('sample size:', len(db_test))\n",
        "    testloader = DataLoader(db_test, batch_size=1, shuffle=False, num_workers=1)\n",
        "    logging.info(f'{len(testloader)} test iterations per epoch')\n",
        "    model.eval()\n",
        "    metric_list = 0.0\n",
        "    for i_batch, sampled_batch in tqdm(enumerate(testloader)):\n",
        "        h, w = sampled_batch['image'].shape[2:]\n",
        "        image, label, case_name = sampled_batch['image'], sampled_batch['label'], sampled_batch['case_name'][0]\n",
        "        print(image.shape, label.shape)\n",
        "        metric_i = test_single_volume(image, label, model, classes=args.num_classes, multimask_output=multimask_output,\n",
        "                                      image_size=[args.img_size, args.img_size],\n",
        "                                      test_save_path=test_save_path, case=case_name)\n",
        "        metric_list += np.array(metric_i)\n",
        "        print('idx %d case %s mean_dice %f mean_hd95 %f' % (\n",
        "            i_batch, case_name, np.mean(metric_i, axis=0)[0], np.mean(metric_i, axis=0)[1]))\n",
        "        logging.info('idx %d case %s mean_dice %f mean_hd95 %f' % (\n",
        "            i_batch, case_name, np.mean(metric_i, axis=0)[0], np.mean(metric_i, axis=0)[1]))\n",
        "        if i_batch == 0:\n",
        "            break\n",
        "    metric_list = metric_list / len(db_test)\n",
        "    for i in range(1, args.num_classes + 1):\n",
        "        try:\n",
        "            logging.info('Mean class %d name %s mean_dice %f mean_hd95 %f' % (i, class_to_name[i], metric_list[i - 1][0], metric_list[i - 1][1]))\n",
        "        except:\n",
        "            logging.info('Mean class %d mean_dice %f mean_hd95 %f' % (i, metric_list[i - 1][0], metric_list[i - 1][1]))\n",
        "    performance = np.mean(metric_list, axis=0)[0]\n",
        "    mean_hd95 = np.mean(metric_list, axis=0)[1]\n",
        "    logging.info('Testing performance in best val model: mean_dice : %f mean_hd95 : %f' % (performance, mean_hd95))\n",
        "    logging.info(\"Testing Finished!\")\n",
        "    return 1\n",
        "\n",
        "\n",
        "def config_to_dict(config):\n",
        "    items_dict = {}\n",
        "    with open(config, 'r') as f:\n",
        "        items = f.readlines()\n",
        "    for i in range(len(items)):\n",
        "        key, value = items[i].strip().split(': ')\n",
        "        items_dict[key] = value\n",
        "    return items_dict\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--config', type=str, default=None, help='The config file provided by the trained model')\n",
        "    parser.add_argument('--volume_path', type=str, default='testset/test_vol_h5/')\n",
        "    parser.add_argument('--dataset', type=str, default='Synapse', help='Experiment name')\n",
        "    parser.add_argument('--num_classes', type=int, default=8)\n",
        "    parser.add_argument('--list_dir', type=str, default='./lists/lists_Synapse/', help='list_dir')\n",
        "    parser.add_argument('--output_dir', type=str, default='/output')\n",
        "    parser.add_argument('--img_size', type=int, default=512, help='Input image size of the network')\n",
        "    parser.add_argument('--input_size', type=int, default=224, help='The input size for training SAM model')\n",
        "    parser.add_argument('--seed', type=int,\n",
        "                        default=1234, help='random seed')\n",
        "    parser.add_argument('--is_savenii', action='store_true', help='Whether to save results during inference')\n",
        "    parser.add_argument('--deterministic', type=int, default=1, help='whether use deterministic training')\n",
        "    parser.add_argument('--ckpt', type=str, default='checkpoints/sam_vit_b_01ec64.pth',\n",
        "                        help='Pretrained checkpoint')\n",
        "    parser.add_argument('--lora_ckpt', type=str, default='checkpoints/epoch_159.pth', help='The checkpoint from LoRA')\n",
        "    parser.add_argument('--vit_name', type=str, default='vit_b', help='Select one vit model')\n",
        "    parser.add_argument('--rank', type=int, default=4, help='Rank for LoRA adaptation')\n",
        "    parser.add_argument('--module', type=str, default='sam_lora_image_encoder')\n",
        "\n",
        "    args = parser.parse_args([])\n",
        "    args.volume_path = 'test_vol_h5'\n",
        "    args.output_dir = 'results'\n",
        "    args.ckpt = 'sam_vit_b_01ec64.pth'\n",
        "    args.lora_ckpt = 'epoch_159.pth'\n",
        "\n",
        "    if args.config is not None:\n",
        "        # overwtite default configurations with config file\\\n",
        "        config_dict = config_to_dict(args.config)\n",
        "        for key in config_dict:\n",
        "            setattr(args, key, config_dict[key])\n",
        "\n",
        "    if not args.deterministic:\n",
        "        cudnn.benchmark = True\n",
        "        cudnn.deterministic = False\n",
        "    else:\n",
        "        cudnn.benchmark = False\n",
        "        cudnn.deterministic = True\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    torch.cuda.manual_seed(args.seed)\n",
        "    dataset_name = args.dataset\n",
        "    dataset_config = {\n",
        "        'Synapse': {\n",
        "            'Dataset': Synapse_dataset,\n",
        "            'volume_path': args.volume_path,\n",
        "            'list_dir': args.list_dir,\n",
        "            'num_classes': args.num_classes,\n",
        "            'z_spacing': 1\n",
        "        }\n",
        "    }\n",
        "    if not os.path.exists(args.output_dir):\n",
        "        os.makedirs(args.output_dir)\n",
        "\n",
        "    # register model\n",
        "    sam, img_embedding_size = sam_model_registry[args.vit_name](image_size=args.img_size,\n",
        "                                                                    num_classes=args.num_classes,\n",
        "                                                                    checkpoint=args.ckpt, pixel_mean=[0, 0, 0],\n",
        "                                                                    pixel_std=[1, 1, 1])\n",
        "\n",
        "    pkg = import_module(args.module)\n",
        "    net = pkg.LoRA_Sam(sam, args.rank).cuda()\n",
        "\n",
        "    assert args.lora_ckpt is not None\n",
        "    net.load_lora_parameters(args.lora_ckpt)\n",
        "\n",
        "    if args.num_classes > 1:\n",
        "        multimask_output = True\n",
        "    else:\n",
        "        multimask_output = False\n",
        "\n",
        "    # initialize log\n",
        "    log_folder = os.path.join(args.output_dir, 'test_log')\n",
        "    os.makedirs(log_folder, exist_ok=True)\n",
        "    logging.basicConfig(filename=log_folder + '/' + 'log.txt', level=logging.INFO,\n",
        "                        format='[%(asctime)s.%(msecs)03d] %(message)s', datefmt='%H:%M:%S')\n",
        "    logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))\n",
        "    logging.info(str(args))\n",
        "\n",
        "    if args.is_savenii:\n",
        "        test_save_path = os.path.join(args.output_dir, 'predictions')\n",
        "        os.makedirs(test_save_path, exist_ok=True)\n",
        "    else:\n",
        "        test_save_path = None\n",
        "    inference(args, multimask_output, dataset_config[dataset_name], net, test_save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-y2mu4ndPfOZ",
        "outputId": "0135150b-4c48-4a3a-f9fb-503a58b03af1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "sample size: 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 148, 512, 512]) torch.Size([1, 148, 512, 512])\n",
            "model input: torch.Size([50, 3, 512, 512])\n",
            "model input: torch.Size([50, 3, 512, 512])\n",
            "model input: torch.Size([48, 3, 512, 512])\n",
            "test_single_volume prediction: (148, 512, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r0it [00:15, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "idx 0 case case0008 mean_dice 0.681535 mean_hd95 0.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "0it [02:24, ?it/s]idx 0 case case0008 mean_dice 0.681535 mean_hd95 16.097952"
      ],
      "metadata": {
        "id": "NVGIYOjPKEbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing with command:"
      ],
      "metadata": {
        "id": "dPc3iQTZ-pTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "python test.py --volume_path test_vol_h5 --output_dir results --ckpt sam_vit_b_01ec64.pth --lora_ckpt epoch_159.pth"
      ],
      "metadata": {
        "id": "TsKn0gVsSUYz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ede4aa0-3f6e-4d13-e98c-53fba3c2d6dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Process is terminated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UeuJxMJbRspH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}