{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/tutorial_notebooks/blob/main/SAMed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Customized Segment Anything Model for Medical Image Segmentation\n",
        "### [[Paper](https://arxiv.org/pdf/2304.13785.pdf)] [[Github](https://github.com/hitachinsk/SAMed)]\n",
        "---\n",
        "[Kaidong Zhang](https://hitachinsk.github.io/), and [Dong Liu](https://faculty.ustc.edu.cn/dongeliu/), technical report, 2023\n",
        "\n",
        "All rights reserved by the authors of SAMed\n",
        "\n",
        "We provide the entire inference pipeline in this page."
      ],
      "metadata": {
        "id": "1P2M4gZbKZWt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup environment"
      ],
      "metadata": {
        "id": "Id3D1PuuLQMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q einops==0.6.1 icecream==2.1.3 MedPy==0.4.0 monai==1.1.0 opencv_python==4.5.4.58 SimpleITK==2.2.1 tensorboardX==2.6 ml-collections==0.1.1 onnx==1.13.1 onnxruntime==1.14.1 tensorboardX torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmmYvx7FLUif",
        "outputId": "fb32b006-6455-48ed-858f-45ef8e4898c8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/151.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.8/151.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'opencv-python' candidate (version 4.5.4.58 at https://files.pythonhosted.org/packages/ea/8c/e01428f31e473f765355c65c24f2dbd62a6a093a3248a9fa97bc65eeeb22/opencv_python-4.5.4.58-cp310-cp310-manylinux2014_x86_64.whl (from https://pypi.org/simple/opencv-python/) (requires-python:>=3.6))\n",
            "Reason for being yanked: deprecated, use  4.5.4.60\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.3/60.3 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.7/52.7 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m866.2/866.2 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for MedPy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ml-collections (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download codes, pretrained weights and test data"
      ],
      "metadata": {
        "id": "m-tSMFkgPhyc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare codes\n",
        "import os\n",
        "CODE_DIR = 'samed_codes'\n",
        "os.makedirs(f'./{CODE_DIR}')\n",
        "!git clone https://github.com/hitachinsk/SAMed.git $CODE_DIR\n",
        "os.chdir(f'./{CODE_DIR}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyB2eYACPtEX",
        "outputId": "a843114d-8e7f-4e53-d563-d12e789af0f1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'samed_codes'...\n",
            "remote: Enumerating objects: 225, done.\u001b[K\n",
            "remote: Counting objects: 100% (102/102), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 225 (delta 86), reused 72 (delta 72), pack-reused 123 (from 1)\u001b[K\n",
            "Receiving objects: 100% (225/225), 635.01 KiB | 1.87 MiB/s, done.\n",
            "Resolving deltas: 100% (105/105), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import os\n",
        "\n",
        "download_with_pydrive = True\n",
        "\n",
        "class Downloader(object):\n",
        "  def __init__(self, use_pydrive):\n",
        "    self.use_pydrive = use_pydrive\n",
        "    current_directory = os.getcwd()\n",
        "    self.save_dir = '.'\n",
        "    if self.use_pydrive:\n",
        "      self.authenticate()\n",
        "\n",
        "  def authenticate(self):\n",
        "    auth.authenticate_user()\n",
        "    gauth = GoogleAuth()\n",
        "    gauth.credentials = GoogleCredentials.get_application_default()\n",
        "    self.drive = GoogleDrive(gauth)\n",
        "\n",
        "  def download_file(self, file_id, file_name):\n",
        "    file_dst = f'{self.save_dir}/{file_name}'\n",
        "    if os.path.exists(file_dst):\n",
        "      print(f'{file_name} already exists')\n",
        "      return\n",
        "    downloaded = self.drive.CreateFile({'id': file_id})\n",
        "    downloaded.FetchMetadata(fetch_all=True)\n",
        "    downloaded.GetContentFile(file_dst)\n",
        "\n",
        "downloader = Downloader(download_with_pydrive)\n",
        "samed_model = {'id': '1P0Bm-05l-rfeghbrT1B62v5eN-3A-uOr', 'name': 'epoch_159.pth'}\n",
        "sam_model = {'id': '1_oCdoEEu3mNhRfFxeWyRerOKt8OEUvcg', 'name': 'sam_vit_b_01ec64.pth'}\n",
        "test_data = {'id': '1RczbNSB37OzPseKJZ1tDxa5OO1IIICzK', 'name': 'test_vol_h5.zip'}\n",
        "downloader.download_file(file_id=samed_model['id'], file_name=samed_model['name'])\n",
        "downloader.download_file(file_id=sam_model['id'], file_name=sam_model['name'])\n",
        "downloader.download_file(file_id=test_data['id'], file_name=test_data['name'])"
      ],
      "metadata": {
        "id": "NI9jWQnsPty2",
        "outputId": "c1ca72c3-9752-4431-8a5c-4c31cdc8fb9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:pydrive is deprecated and no longer maintained. We recommend that you migrate your projects to pydrive2, the maintained fork of pydrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip test_vol_h5.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ng42nD8DSCp0",
        "outputId": "1cab9e46-8434-408d-f970-4ef4c8ae52a0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  test_vol_h5.zip\n",
            "   creating: test_vol_h5/\n",
            "  inflating: test_vol_h5/case0038.npy.h5  \n",
            "  inflating: test_vol_h5/case0036.npy.h5  \n",
            "  inflating: test_vol_h5/case0035.npy.h5  \n",
            "  inflating: test_vol_h5/case0032.npy.h5  \n",
            "  inflating: test_vol_h5/case0029.npy.h5  \n",
            "  inflating: test_vol_h5/case0025.npy.h5  \n",
            "  inflating: test_vol_h5/case0022.npy.h5  \n",
            "  inflating: test_vol_h5/case0008.npy.h5  \n",
            "  inflating: test_vol_h5/case0004.npy.h5  \n",
            "  inflating: test_vol_h5/case0003.npy.h5  \n",
            "  inflating: test_vol_h5/case0001.npy.h5  \n",
            "  inflating: test_vol_h5/case0002.npy.h5  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Execute SAMed"
      ],
      "metadata": {
        "id": "ModmC4zTSSQj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing:"
      ],
      "metadata": {
        "id": "Kssy1bII-mhU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/samed_codes\n",
        "import os\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "import numpy as np\n",
        "import argparse\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.backends.cudnn as cudnn\n",
        "# from utils import test_single_volume\n",
        "from importlib import import_module\n",
        "from segment_anything import sam_model_registry\n",
        "from datasets.dataset_synapse import Synapse_dataset\n",
        "from icecream import ic\n",
        "from medpy import metric\n",
        "from scipy.ndimage import zoom\n",
        "import torch.nn as nn\n",
        "import SimpleITK as sitk\n",
        "import torch.nn.functional as F\n",
        "import imageio\n",
        "from einops import repeat\n",
        "\n",
        "\n",
        "\n",
        "class_to_name = {1: 'spleen', 2: 'right kidney', 3: 'left kidney', 4: 'gallbladder', 5: 'liver', 6: 'stomach', 7: 'aorta', 8: 'pancreas'}\n",
        "\n",
        "def calculate_metric_percase(pred, gt):\n",
        "    pred[pred > 0] = 1\n",
        "    gt[gt > 0] = 1\n",
        "    if pred.sum() > 0 and gt.sum() > 0:\n",
        "        dice = metric.binary.dc(pred, gt)\n",
        "        hd95 = metric.binary.hd95(pred, gt)\n",
        "        return dice, hd95\n",
        "    elif pred.sum() > 0 and gt.sum() == 0:\n",
        "        return 1, 0\n",
        "    else:\n",
        "        return 0, 0\n",
        "\n",
        "def test_single_volume(image, label, net, classes, multimask_output, image_size=[512, 512],\n",
        "                       test_save_path=None, case=None):\n",
        "    image, label = image.squeeze(0).cpu().detach().numpy(), label.squeeze(0).cpu().detach().numpy()\n",
        "    print('image, label:', image.shape, label.shape )\n",
        "\n",
        "    prediction = np.zeros_like(label)\n",
        "    for ind in range(image.shape[0]):\n",
        "        slice = image[ind, :, :]\n",
        "        x, y = slice.shape[0], slice.shape[1]\n",
        "        inputs = torch.from_numpy(slice).unsqueeze(0).unsqueeze(0).float().cuda()\n",
        "        inputs = repeat(inputs, 'b c h w -> b (repeat c) h w', repeat=3)\n",
        "        net.eval()\n",
        "        with torch.no_grad():\n",
        "            # print('model input:', inputs.shape)\n",
        "            outputs = net(inputs, multimask_output, image_size[0])\n",
        "            output_masks = outputs['masks']\n",
        "            out = torch.argmax(torch.softmax(output_masks, dim=1), dim=1).squeeze(0)\n",
        "            out = out.cpu().detach().numpy()\n",
        "            out_h, out_w = out.shape\n",
        "            if x != out_h or y != out_w:\n",
        "                pred = zoom(out, (x / out_h, y / out_w), order=0)\n",
        "            else:\n",
        "                pred = out\n",
        "            prediction[ind] = pred\n",
        "\n",
        "    metric_list = []\n",
        "    for i in range(1, classes + 1):\n",
        "        metric_list.append(calculate_metric_percase(prediction == i, label == i))\n",
        "\n",
        "    return metric_list\n",
        "\n",
        "def inference(args, multimask_output, db_config, model, test_save_path=None):\n",
        "    db_test = db_config['Dataset'](base_dir=args.volume_path, list_dir=args.list_dir, split='test_vol')\n",
        "    print('sample size:', len(db_test))\n",
        "    testloader = DataLoader(db_test, batch_size=1, shuffle=False, num_workers=1)\n",
        "    logging.info(f'{len(testloader)} test iterations per epoch')\n",
        "    model.eval()\n",
        "    metric_list = 0.0\n",
        "    for i_batch, sampled_batch in tqdm(enumerate(testloader)):\n",
        "        h, w = sampled_batch['image'].shape[2:]\n",
        "        image, label, case_name = sampled_batch['image'], sampled_batch['label'], sampled_batch['case_name'][0]\n",
        "        print(image.shape, label.shape)\n",
        "        metric_i = test_single_volume(image, label, model, classes=args.num_classes, multimask_output=multimask_output,\n",
        "                                      image_size=[args.img_size, args.img_size],\n",
        "                                      test_save_path=test_save_path, case=case_name)\n",
        "        metric_list += np.array(metric_i)\n",
        "        print('idx %d case %s mean_dice %f mean_hd95 %f' % (\n",
        "            i_batch, case_name, np.mean(metric_i, axis=0)[0], np.mean(metric_i, axis=0)[1]))\n",
        "        logging.info('idx %d case %s mean_dice %f mean_hd95 %f' % (\n",
        "            i_batch, case_name, np.mean(metric_i, axis=0)[0], np.mean(metric_i, axis=0)[1]))\n",
        "        if i_batch == 0:\n",
        "            break\n",
        "    metric_list = metric_list / len(db_test)\n",
        "    for i in range(1, args.num_classes + 1):\n",
        "        try:\n",
        "            logging.info('Mean class %d name %s mean_dice %f mean_hd95 %f' % (i, class_to_name[i], metric_list[i - 1][0], metric_list[i - 1][1]))\n",
        "        except:\n",
        "            logging.info('Mean class %d mean_dice %f mean_hd95 %f' % (i, metric_list[i - 1][0], metric_list[i - 1][1]))\n",
        "    performance = np.mean(metric_list, axis=0)[0]\n",
        "    mean_hd95 = np.mean(metric_list, axis=0)[1]\n",
        "    logging.info('Testing performance in best val model: mean_dice : %f mean_hd95 : %f' % (performance, mean_hd95))\n",
        "    logging.info(\"Testing Finished!\")\n",
        "    return 1\n",
        "\n",
        "\n",
        "def config_to_dict(config):\n",
        "    items_dict = {}\n",
        "    with open(config, 'r') as f:\n",
        "        items = f.readlines()\n",
        "    for i in range(len(items)):\n",
        "        key, value = items[i].strip().split(': ')\n",
        "        items_dict[key] = value\n",
        "    return items_dict\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--config', type=str, default=None, help='The config file provided by the trained model')\n",
        "    parser.add_argument('--volume_path', type=str, default='testset/test_vol_h5/')\n",
        "    parser.add_argument('--dataset', type=str, default='Synapse', help='Experiment name')\n",
        "    parser.add_argument('--num_classes', type=int, default=8)\n",
        "    parser.add_argument('--list_dir', type=str, default='./lists/lists_Synapse/', help='list_dir')\n",
        "    parser.add_argument('--output_dir', type=str, default='/output')\n",
        "    parser.add_argument('--img_size', type=int, default=512, help='Input image size of the network')\n",
        "    parser.add_argument('--input_size', type=int, default=224, help='The input size for training SAM model')\n",
        "    parser.add_argument('--seed', type=int,\n",
        "                        default=1234, help='random seed')\n",
        "    parser.add_argument('--is_savenii', action='store_true', help='Whether to save results during inference')\n",
        "    parser.add_argument('--deterministic', type=int, default=1, help='whether use deterministic training')\n",
        "    parser.add_argument('--ckpt', type=str, default='checkpoints/sam_vit_b_01ec64.pth',\n",
        "                        help='Pretrained checkpoint')\n",
        "    parser.add_argument('--lora_ckpt', type=str, default='checkpoints/epoch_159.pth', help='The checkpoint from LoRA')\n",
        "    parser.add_argument('--vit_name', type=str, default='vit_b', help='Select one vit model')\n",
        "    parser.add_argument('--rank', type=int, default=4, help='Rank for LoRA adaptation')\n",
        "    parser.add_argument('--module', type=str, default='sam_lora_image_encoder')\n",
        "\n",
        "    args = parser.parse_args([])\n",
        "    args.volume_path = 'test_vol_h5'\n",
        "    args.output_dir = 'results'\n",
        "    args.ckpt = 'sam_vit_b_01ec64.pth'\n",
        "    args.lora_ckpt = 'epoch_159.pth'\n",
        "\n",
        "    if args.config is not None:\n",
        "        # overwtite default configurations with config file\\\n",
        "        config_dict = config_to_dict(args.config)\n",
        "        for key in config_dict:\n",
        "            setattr(args, key, config_dict[key])\n",
        "\n",
        "    if not args.deterministic:\n",
        "        cudnn.benchmark = True\n",
        "        cudnn.deterministic = False\n",
        "    else:\n",
        "        cudnn.benchmark = False\n",
        "        cudnn.deterministic = True\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    torch.cuda.manual_seed(args.seed)\n",
        "    dataset_name = args.dataset\n",
        "    dataset_config = {\n",
        "        'Synapse': {\n",
        "            'Dataset': Synapse_dataset,\n",
        "            'volume_path': args.volume_path,\n",
        "            'list_dir': args.list_dir,\n",
        "            'num_classes': args.num_classes,\n",
        "            'z_spacing': 1\n",
        "        }\n",
        "    }\n",
        "    if not os.path.exists(args.output_dir):\n",
        "        os.makedirs(args.output_dir)\n",
        "\n",
        "    # register model\n",
        "    sam, img_embedding_size = sam_model_registry[args.vit_name](image_size=args.img_size,\n",
        "                                                                    num_classes=args.num_classes,\n",
        "                                                                    checkpoint=args.ckpt, pixel_mean=[0, 0, 0],\n",
        "                                                                    pixel_std=[1, 1, 1])\n",
        "\n",
        "    pkg = import_module(args.module)\n",
        "    net = pkg.LoRA_Sam(sam, args.rank).cuda()\n",
        "\n",
        "    assert args.lora_ckpt is not None\n",
        "    net.load_lora_parameters(args.lora_ckpt)\n",
        "\n",
        "    if args.num_classes > 1:\n",
        "        multimask_output = True\n",
        "    else:\n",
        "        multimask_output = False\n",
        "\n",
        "    # initialize log\n",
        "    log_folder = os.path.join(args.output_dir, 'test_log')\n",
        "    os.makedirs(log_folder, exist_ok=True)\n",
        "    logging.basicConfig(filename=log_folder + '/' + 'log.txt', level=logging.INFO,\n",
        "                        format='[%(asctime)s.%(msecs)03d] %(message)s', datefmt='%H:%M:%S')\n",
        "    logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))\n",
        "    logging.info(str(args))\n",
        "\n",
        "    if args.is_savenii:\n",
        "        test_save_path = os.path.join(args.output_dir, 'predictions')\n",
        "        os.makedirs(test_save_path, exist_ok=True)\n",
        "    else:\n",
        "        test_save_path = None\n",
        "    inference(args, multimask_output, dataset_config[dataset_name], net, test_save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602
        },
        "id": "-y2mu4ndPfOZ",
        "outputId": "8482e13e-9b59-4c58-f8f3-9ec7c0b5c846"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/samed_codes/segment_anything/build_sam.py:131: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(f)\n",
            "/content/samed_codes/sam_lora_image_encoder.py:152: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(filename)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample size: 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 148, 512, 512]) torch.Size([1, 148, 512, 512])\n",
            "image, label: (148, 512, 512) (148, 512, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/medpy/metric/binary.py:68: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
            "  result = numpy.atleast_1d(result.astype(numpy.bool))\n",
            "\r0it [00:16, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'numpy' has no attribute 'bool'.\n`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-4aba44d23d43>\u001b[0m in \u001b[0;36m<cell line: 116>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0mtest_save_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m     \u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultimask_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_save_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-4aba44d23d43>\u001b[0m in \u001b[0;36minference\u001b[0;34m(args, multimask_output, db_config, model, test_save_path)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcase_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msampled_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampled_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampled_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'case_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         metric_i = test_single_volume(image, label, model, classes=args.num_classes, multimask_output=multimask_output,\n\u001b[0m\u001b[1;32m     85\u001b[0m                                       \u001b[0mimage_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                                       test_save_path=test_save_path, case=case_name)\n",
            "\u001b[0;32m<ipython-input-5-4aba44d23d43>\u001b[0m in \u001b[0;36mtest_single_volume\u001b[0;34m(image, label, net, classes, multimask_output, image_size, test_save_path, case)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mmetric_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mmetric_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalculate_metric_percase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmetric_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-4aba44d23d43>\u001b[0m in \u001b[0;36mcalculate_metric_percase\u001b[0;34m(pred, gt)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mgt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgt\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mdice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mhd95\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhd95\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhd95\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/medpy/metric/binary.py\u001b[0m in \u001b[0;36mdc\u001b[0;34m(result, reference)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mThis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mreal\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mbinary\u001b[0m \u001b[0mimages\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mtherefore\u001b[0m \u001b[0mbe\u001b[0m \u001b[0msupplied\u001b[0m \u001b[0;32min\u001b[0m \u001b[0many\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \"\"\"\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mreference\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreference\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m__former_attrs__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__former_attrs__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'testing'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'bool'.\n`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing with command:"
      ],
      "metadata": {
        "id": "dPc3iQTZ-pTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "python test.py --volume_path test_vol_h5 --output_dir results --ckpt sam_vit_b_01ec64.pth --lora_ckpt epoch_159.pth"
      ],
      "metadata": {
        "id": "TsKn0gVsSUYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/VIBE-Lab/cosas-algorithm-submition.git"
      ],
      "metadata": {
        "id": "tEWvz8iy2N-3",
        "outputId": "5db400d0-d2a7-4b35-a8bf-38bda0d47020",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'cosas-algorithm-submition'...\n",
            "remote: Enumerating objects: 69, done.\u001b[K\n",
            "remote: Counting objects: 100% (61/61), done.\u001b[K\n",
            "remote: Compressing objects: 100% (49/49), done.\u001b[K\n",
            "remote: Total 69 (delta 28), reused 42 (delta 12), pack-reused 8 (from 1)\u001b[K\n",
            "Receiving objects: 100% (69/69), 69.96 MiB | 22.19 MiB/s, done.\n",
            "Resolving deltas: 100% (28/28), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "import warnings\n",
        "\n",
        "import SimpleITK\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "def read(path):\n",
        "    image = SimpleITK.ReadImage(path)\n",
        "    return SimpleITK.GetArrayFromImage(image)\n",
        "\n",
        "\n",
        "def write(path, array):\n",
        "    image = SimpleITK.GetImageFromArray(array)\n",
        "    SimpleITK.WriteImage(image, path, useCompression=False)\n",
        "\n",
        "transform_img = transforms.Compose([\n",
        "        transforms.Resize((256, 256)),  #Lets try 128*128\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225]\n",
        "            )\n",
        "        ])\n",
        "\n",
        "\n",
        "input_path = 'cosas-algorithm-submition/data/1a7385773597.mha'\n",
        "image = read(input_path)\n",
        "print(image.shape)\n",
        "image = Image.fromarray(image)\n",
        "image = transform_img(image)\n",
        "print(image.shape)\n",
        "inputs = image.unsqueeze(0).float().cuda()\n",
        "print(inputs.shape)"
      ],
      "metadata": {
        "id": "6qulcig0RUEU",
        "outputId": "9038d7ad-ca79-4636-9008-58d2bc00ca89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1487, 1445, 3)\n",
            "torch.Size([3, 256, 256])\n",
            "torch.Size([1, 3, 256, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls cosas-algorithm-submition/data"
      ],
      "metadata": {
        "id": "cMNTupU6RjbK",
        "outputId": "fe317632-b8ea-4cae-aec2-fec033068dfd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1a7385773597.mha  1ffea5df80b4.mha  2cefbd83c986.mha\n",
            "1a7385773597.png  1ffea5df80b4.png  2cefbd83c986.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UeuJxMJbRspH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}