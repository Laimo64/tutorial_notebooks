{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/tutorial_notebooks/blob/main/CIFAR_10H_VIT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vL2RMWddkaQ"
      },
      "source": [
        "# Cloning repository of CIFAR-10H Annotation\n",
        "Paper: Human uncertainty makes classification more robust (https://arxiv.org/pdf/1908.07086.pdf)\n",
        "\n",
        "Label:  CIFAR10 [0: airplane, 1: automobile, 2: bird, 3: cat, 4: deer, 5: dog, 6: frog, 7: horse, 8: ship, 9: truck] <br>\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1010/1*r8S5tF_6naagKOnlIcGXoQ.png\" alt=\"alternatetext\">\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KrQIqW6_NYl",
        "outputId": "02f4c2e2-b889-44b6-f4e1-41ce67d5ae74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'cifar-10h'...\n",
            "remote: Enumerating objects: 49, done.\u001b[K\n",
            "remote: Counting objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 49 (delta 0), reused 0 (delta 0), pack-reused 48\u001b[K\n",
            "Unpacking objects: 100% (49/49), done.\n",
            "/content/cifar-10h/cifar-10h\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/jcpeterson/cifar-10h\n",
        "%cd cifar-10h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3cVy5LVFfrZ"
      },
      "source": [
        "## Installing huggingface transformer "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIzJpadXFmUC",
        "outputId": "ef7c5bd5-dcf4-40c2-9c17-2c9aa4e19141"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 3.8 MB 4.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 67 kB 4.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 43.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 38.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 895 kB 50.5 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "! pip -q install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNN4TybGd2Q_"
      },
      "source": [
        "# main script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NN371ewSCWX7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import models\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "import argparse\n",
        "import copy\n",
        "import random\n",
        "import numpy as np\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "def seed_everything(seed=12):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "parser = argparse.ArgumentParser(description='CIFAR-10H Training')\n",
        "parser.add_argument('--lr', default=3e-2, type=float, help='learning rate')\n",
        "parser.add_argument('--lr_schedule', default=0, type=int, help='lr scheduler')\n",
        "parser.add_argument('--batch_size', default=32, type=int, help='batch size')\n",
        "parser.add_argument('--test_batch_size', default=64, type=int, help='batch size')\n",
        "parser.add_argument('--num_epoch', default=10, type=int, help='epoch number')\n",
        "parser.add_argument('--num_classes', type=int, default=10, help='number classes')\n",
        "args = parser.parse_args(args=[])\n",
        "\n",
        "def train(model, trainloader, criterion, optimizer):\n",
        "    model.train()\n",
        "    for batch_idx, (inputs, targets, ad) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs).logits\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "def test(model, testloader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs).logits\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "    return correct / total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHKUA3yBd87G"
      },
      "source": [
        "# CIFAR-10H dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4uiWMiyCkjk"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torchvision\n",
        "\n",
        "from transformers import ViTFeatureExtractor, ViTForImageClassification, BatchFeature\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.transforms import ToTensor, Normalize, Resize, Compose\n",
        "\n",
        "import torch\n",
        "\n",
        "class ViTFeatureExtractorTransforms:\n",
        "    def __init__(self, model_name_or_path):\n",
        "        feature_extractor = ViTFeatureExtractor.from_pretrained(model_name_or_path)\n",
        "        transform = []\n",
        "\n",
        "        if feature_extractor.do_resize:\n",
        "            transform.append(Resize(feature_extractor.size))\n",
        "\n",
        "        transform.append(ToTensor())\n",
        "\n",
        "        if feature_extractor.do_normalize:\n",
        "            transform.append(Normalize(feature_extractor.image_mean, feature_extractor.image_std))\n",
        "\n",
        "        self.transform = Compose(transform)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.transform(x)\n",
        "\n",
        "class CIFAR10H(torchvision.datasets.CIFAR10):\n",
        "\n",
        "    def __init__(self, root,  rand_number=0, train=False, transform=None, target_transform=None,\n",
        "                 download=False):\n",
        "        super(CIFAR10H, self).__init__(root, train, transform, target_transform, download) \n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        self.ad = np.load(os.path.join(root,'cifar10h-probs.npy'))\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        img, target = self.data[index], self.targets[index]\n",
        "        img = Image.fromarray(img)\n",
        "        ad = self.ad[index]\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        "        return img, target, ad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRQEQIdaerKa"
      },
      "source": [
        "# Run script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rpOrSuEFY2N",
        "outputId": "57e62a8b-72ea-401a-ef22-6a8e34b6feaf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "train samples: 10000 test samples: 50000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTForImageClassification: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 0  acc: 0.8934  best epoch: 0  best acc: 0.8934\n",
            "epoch: 1  acc: 0.9123  best epoch: 1  best acc: 0.9123\n",
            "epoch: 2  acc: 0.9076  best epoch: 1  best acc: 0.9123\n",
            "epoch: 3  acc: 0.9108  best epoch: 1  best acc: 0.9123\n",
            "epoch: 4  acc: 0.9098  best epoch: 1  best acc: 0.9123\n",
            "epoch: 5  acc: 0.9280  best epoch: 5  best acc: 0.9280\n"
          ]
        }
      ],
      "source": [
        "seed_everything()\n",
        "#2e-5,\n",
        "model_name_or_path = 'google/vit-base-patch16-224-in21k'\n",
        "\n",
        "\n",
        "train_dataset = CIFAR10H(root='./data', train=False, download=True, transform=ViTFeatureExtractorTransforms(model_name_or_path))\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=ViTFeatureExtractorTransforms(model_name_or_path))\n",
        "print('train samples:',len(train_dataset), 'test samples:',len(test_dataset))\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.test_batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "model = ViTForImageClassification.from_pretrained(model_name_or_path, num_labels=args.num_classes)\n",
        "\n",
        "# model = models.resnet34(pretrained=True).to(device)\n",
        "# model.fc = nn.Linear(model.fc.in_features, args.num_classes)\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=0.9, nesterov=False, weight_decay=0.0001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "best_epoch, best_acc = 0.0, 0\n",
        "for epoch in range(args.num_epoch):\n",
        "    train(model, train_loader, criterion, optimizer)\n",
        "    accuracy = test(model, test_loader)\n",
        "    if accuracy > best_acc:\n",
        "        patience = 0\n",
        "        best_acc = accuracy\n",
        "        best_epoch = epoch\n",
        "        best_model = copy.deepcopy(model)\n",
        "        torch.save(best_model.state_dict(), 'best_model_cifar10h_vit.pth.tar')\n",
        "    print('epoch: {}  acc: {:.4f}  best epoch: {}  best acc: {:.4f}'.format(\n",
        "            epoch, accuracy, best_epoch, best_acc, optimizer.param_groups[0]['lr']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krBAqR41UU7p"
      },
      "outputs": [],
      "source": [
        "epoch: 0  acc: 0.8934  best epoch: 0  best acc: 0.8934\n",
        "epoch: 1  acc: 0.9123  best epoch: 1  best acc: 0.9123\n",
        "epoch: 2  acc: 0.9076  best epoch: 1  best acc: 0.9123\n",
        "epoch: 3  acc: 0.9108  best epoch: 1  best acc: 0.9123\n",
        "epoch: 4  acc: 0.9098  best epoch: 1  best acc: 0.9123\n",
        "epoch: 5  acc: 0.9280  best epoch: 5  best acc: 0.9280"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "CIFAR-10H_VIT.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}