{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/tutorial_notebooks/blob/main/Surgical_LLaMA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pU4iPZR0ucs",
        "outputId": "5136ee24-3511-41ab-f455-4854225c3b71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SurgicalGPT'...\n",
            "remote: Enumerating objects: 89, done.\u001b[K\n",
            "remote: Counting objects: 100% (89/89), done.\u001b[K\n",
            "remote: Compressing objects: 100% (52/52), done.\u001b[K\n",
            "remote: Total 89 (delta 42), reused 82 (delta 35), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (89/89), 749.26 KiB | 1.15 MiB/s, done.\n",
            "Resolving deltas: 100% (42/42), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/lalithjets/SurgicalGPT.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SWZYckbo1Wuv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aea2250e-fc02-4f6c-8fd2-4a30daebd28a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m105.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Installing required dependencies\n",
        "!pip -q install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "google drive: https://drive.google.com/file/d/1K5YnSPMPvn2x1gtRAw2ZfxIqoIo2DX3I"
      ],
      "metadata": {
        "id": "hvfERkLBGAM8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7wl6PSm0xF9",
        "outputId": "2de2b992-6f3e-4046-c146-e60cce5e4b80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1WGdztykX3nW6pi_BKp4rO8nA7ESNRfVN\n",
            "To: /content/EndoVis-18-VQA.zip\n",
            "100% 2.70G/2.70G [00:41<00:00, 65.2MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Downloading the VQA EndoVis18 Dataset https://drive.google.com/file/d/1WGdztykX3nW6pi_BKp4rO8nA7ESNRfVN/view?usp=sharing\n",
        "!gdown --id 1WGdztykX3nW6pi_BKp4rO8nA7ESNRfVN\n",
        "\n",
        "# Unzipping the VQA EndoVis18 Dataset\\\n",
        "!unzip -q EndoVis-18-VQA.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset path correction as repo\n",
        "!mv -f EndoVis-18-VQA /content/SurgicalGPT/dataset"
      ],
      "metadata": {
        "id": "NtzbQyOoNypt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcMJ5Prm0_dL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a02a820-833f-4873-eb75-a0bcff318dfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/SurgicalGPT\n",
            "2023-10-20 21:38:59.073664: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "efvlegpt2Swin v1 zeroes m18 cat1 1e-05 checkpoints/efvlegpt2Swin/m18_v1_z_qf_\n",
            "device = cuda\n",
            "Downloading (…)olve/main/vocab.json: 100% 1.04M/1.04M [00:00<00:00, 3.94MB/s]\n",
            "Downloading (…)olve/main/merges.txt: 100% 456k/456k [00:00<00:00, 3.45MB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 4.08MB/s]\n",
            "Downloading (…)lve/main/config.json: 100% 665/665 [00:00<00:00, 4.05MB/s]\n",
            "Downloading (…)rocessor_config.json: 100% 255/255 [00:00<00:00, 1.33MB/s]\n",
            "Total files: 1560 | Total question: 9014\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Total files: 447 | Total question: 2769\n",
            "Downloading (…)lve/main/config.json: 100% 71.8k/71.8k [00:00<00:00, 215MB/s]\n",
            "Downloading model.safetensors: 100% 113M/113M [00:01<00:00, 87.2MB/s]\n",
            "Downloading model.safetensors: 100% 548M/548M [00:07<00:00, 77.2MB/s]\n",
            "model params:  190960524\n",
            "Train: epoch: 1 loss: 349.300427 | Acc: 0.532172 | Precision: 0.257939 | Recall: 0.215611 | FScore: 0.216388\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Test: epoch: 1 loss: 70.736336 | Acc: 0.596244 | Precision: 0.747001 | Recall: 0.310399 | FScore: 0.276300\n"
          ]
        }
      ],
      "source": [
        "# Running the training loop with all arguments\n",
        "%cd /content/SurgicalGPT\n",
        "import os\n",
        "os.makedirs('checkpoints/efvlegpt2Swin', exist_ok=True)\n",
        "!python train.py \\\n",
        "--lr=0.00001 \\\n",
        "--checkpoint_dir='checkpoints/efvlegpt2Swin/m18_v1_z_qf_' \\\n",
        "--dataset_type='m18' \\\n",
        "--tokenizer_ver='gpt2v1' \\\n",
        "--model_ver='efvlegpt2Swin' \\\n",
        "--model_subver='v1' \\\n",
        "--vis_pos_emb='zeroes'\\\n",
        "--batch_size=40 \\\n",
        "--epochs=2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jTfnDewsRHrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/SurgicalGPT\n",
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.utils.data\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data  import DataLoader\n",
        "from transformers import BertTokenizer, GPT2Tokenizer\n",
        "from train import seed_everything\n",
        "from dataloaders.dataloaderGPT2Classification import EndoVis18VQAGPTClassification\n",
        "# from models.EFGPT2Classification import EFVLEGPT2RS18Classification, EFVLEGPT2SwinClassification\n",
        "from utils import save_clf_checkpoint, adjust_learning_rate, calc_acc, calc_precision_recall_fscore, calc_classwise_acc\n",
        "\n",
        "from transformers import  VisualBertConfig, GPT2Config\n",
        "from transformers import VisualBertModel, GPT2Model, ViTModel, SwinModel\n",
        "from transformers import AutoTokenizer, LlamaForCausalLM, LlamaForSequenceClassification, LlamaTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "def get_arg():\n",
        "    parser = argparse.ArgumentParser(description='VisualQuestionAnswerClassification')\n",
        "\n",
        "    # VB Model parameters\n",
        "    parser.add_argument('--emb_dim',        type=int,   default=300,                                help='dimension of word embeddings.')\n",
        "    parser.add_argument('--n_heads',        type=int,   default=8,                                  help='Multi-head attention.')\n",
        "    parser.add_argument('--dropout',        type=float, default=0.1,                                help='dropout')\n",
        "    parser.add_argument('--encoder_layers', type=int,   default=6,                                  help='the number of layers of encoder in Transformer.')\n",
        "\n",
        "    # Training parameters\n",
        "    parser.add_argument('--epochs',         type=int,   default=2,                                 help='number of epochs to train for (if early stopping is not triggered).') #80, 26\n",
        "    parser.add_argument('--batch_size',     type=int,   default=40,                                 help='batch_size')\n",
        "    parser.add_argument('--workers',        type=int,   default=1,                                  help='for data-loading; right now, only 1 works with h5pys.')\n",
        "    parser.add_argument('--print_freq',     type=int,   default=100,                                help='print training/validation stats every __ batches.')\n",
        "\n",
        "    # existing checkpoint\n",
        "    parser.add_argument('--checkpoint',     default=None,                                           help='path to checkpoint, None if none.')\n",
        "\n",
        "    parser.add_argument('--lr',             type=float, default=0.00001,                           help='0.000005, 0.00001, 0.000005')\n",
        "    parser.add_argument('--checkpoint_dir', default= 'checkpoints/efvlegpt2Swin/m18_v1_z_qf_',            help='med_vqa_c$version$/m18/c80/m18_vid$temporal_size$/c80_vid$temporal_size$') #clf_v1_2_1x1/med_vqa_c3\n",
        "    parser.add_argument('--dataset_type',   default= 'm18',                                          help='med_vqa/m18/c80/m18_vid/c80_vid')\n",
        "    parser.add_argument('--dataset_cat',    default= 'cat1',                                        help='cat1/cat2/cat3')\n",
        "    parser.add_argument('--tokenizer_ver',  default= 'gpt2v1',                                      help='btv2/btv3/gpt2v1')\n",
        "    parser.add_argument('--question_len',   default= 25,                                            help='25')\n",
        "    parser.add_argument('--model_ver',      default= 'efvlegpt2Swin',                                          help='vb/vbrm/efvlegpt2rs18/efvlegpt2Swin/\"')  #vrvb/gpt2rs18/gpt2ViT/gpt2Swin/biogpt2rs18/vilgpt2vqa/efgpt2rs18gr/efvlegpt2Swingr\n",
        "    parser.add_argument('--model_subver',   default= 'v1',                                          help='V0,v1/v2/v3/v4')\n",
        "    parser.add_argument('--vis_pos_emb',   default= 'zeroes',                                           help='None, zeroes, pos')\n",
        "    parser.add_argument('--patch_size',     default= 5,                                             help='1/2/3/4/5')\n",
        "\n",
        "    parser.add_argument('--num_class',      default= 2,                                             help='25')\n",
        "    # parser.add_argument('--temporal_size',  default= 1,                                             help='1/2/3/4/5')\n",
        "    parser.add_argument('--validate',       default=False,                                          help='When only validation required False/True')\n",
        "\n",
        "    if 'ipykernel' in sys.modules:\n",
        "        args = parser.parse_args([])\n",
        "    else:\n",
        "        args = parser.parse_args()\n",
        "    return args\n",
        "\n",
        "class EFVLEGPT2SwinClassification(nn.Module):\n",
        "    def __init__(self, num_class = 12, model_subver = 'v0', vis_pos_emb = None):\n",
        "        super(EFVLEGPT2SwinClassification, self).__init__()\n",
        "        '''\n",
        "        v0: visual embedding : Default patch1 + embedding form VB + GPT2 decoder\n",
        "        v1: visual embedding : Default patch1 + GPT2 decoder\n",
        "        '''\n",
        "\n",
        "        self.sub_ver = model_subver\n",
        "        self.vis_pos_emb = vis_pos_emb\n",
        "\n",
        "        ## image processing\n",
        "        self.img_feature_extractor = SwinModel.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\")\n",
        "\n",
        "        ## Visual_embedding\n",
        "        # visual bert embedding\n",
        "        if self.sub_ver == \"v0\":\n",
        "            VB_config = VisualBertConfig.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n",
        "            VB_config.visual_embedding_dim = 768\n",
        "            visualbert = VisualBertModel(config=VB_config)\n",
        "            self.visual_embedder = visualbert.embeddings.visual_projection\n",
        "\n",
        "        ## Question_embedding\n",
        "        # self.VCAdecoder = GPT2Model.from_pretrained('gpt2')\n",
        "        # self.question_embedder = self.VCAdecoder.wte\n",
        "\n",
        "        self.VCAdecoder = LlamaForSequenceClassification.from_pretrained(\"meta-llama/Llama-2-7b-hf\", num_labels=num_class)\n",
        "        self.question_embedder = self.VCAdecoder.embed_tokens\n",
        "\n",
        "        ## GPT2 visual_cotext_aware_decoder\n",
        "        # self.VCAdecoder = GPT2Model.from_pretrained('gpt2')\n",
        "\n",
        "        ## intermediate_layers\n",
        "        self.intermediate_layer = nn.Linear(768, 512)  #(512+768)\n",
        "        self.LayerNorm = nn.BatchNorm1d(512)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "        ## classifier\n",
        "        self.classifier = nn.Linear(512, num_class)\n",
        "\n",
        "    def forward(self, input, img):\n",
        "\n",
        "        ## image encoder features\n",
        "        img['pixel_values'] = img['pixel_values'].to(device)\n",
        "        img_feature = self.img_feature_extractor(**img)\n",
        "\n",
        "        ## visual Embedding : id type 1, pos: zero / incremental\n",
        "        if self.sub_ver == 'v0':\n",
        "            visual_embeds = self.visual_embedder(img_feature[0])\n",
        "        if self.sub_ver == 'v1':\n",
        "            visual_embeds = img_feature[0]\n",
        "\n",
        "        visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float)\n",
        "        visual_attention_mask = visual_attention_mask.to(device)\n",
        "\n",
        "        if self.vis_pos_emb == 'zeroes':\n",
        "            visual_id_type = torch.ones(*visual_embeds.size()[:-1], dtype=torch.long, device=device)\n",
        "            visual_position_id = torch.zeros(*visual_embeds.size()[:-1], dtype=torch.long, device=device)\n",
        "        elif self.vis_pos_emb == 'pos':\n",
        "            visual_id_type = torch.ones(*visual_embeds.size()[:-1], dtype=torch.long, device=device)\n",
        "            visual_position_id = torch.arange(0,visual_embeds.size()[1])\n",
        "            visual_position_id = torch.unsqueeze(visual_position_id,0)\n",
        "            visual_position_id = visual_position_id.repeat(visual_embeds.size()[0], 1)\n",
        "            visual_position_id = visual_position_id.to(device)\n",
        "\n",
        "        ## question embedding: id type 0, pose incremental\n",
        "        input['input_ids'] = input['input_ids'].to(device)\n",
        "        input['attention_mask'] = input['attention_mask'].to(device)\n",
        "\n",
        "        question_embeds = self.question_embedder(input['input_ids'])\n",
        "        question_attention_mask = input['attention_mask']\n",
        "\n",
        "        if self.vis_pos_emb == 'zeroes' or self.vis_pos_emb == 'pos':\n",
        "            question_id_type = torch.zeros(*question_embeds.size()[:-1], dtype=torch.long, device=device)\n",
        "            question_position_id = torch.arange(0,question_embeds.size()[1])\n",
        "            question_position_id = torch.unsqueeze(question_position_id,0)\n",
        "            question_position_id = question_position_id.repeat(question_embeds.size()[0], 1)\n",
        "            question_position_id = question_position_id.to(device)\n",
        "\n",
        "        ## question first\n",
        "        inputs_embeds = torch.cat((question_embeds, visual_embeds), dim=1)\n",
        "        attention_mask = torch.cat((question_attention_mask, visual_attention_mask), dim=1)\n",
        "\n",
        "        if self.vis_pos_emb == 'zeroes' or self.vis_pos_emb == 'pos':\n",
        "            token_type_ids = torch.cat((question_id_type, visual_id_type), dim=1)\n",
        "            position_ids = torch.cat((question_position_id, visual_position_id), dim=1)\n",
        "\n",
        "        ## VCA_GPT2 decoder\n",
        "        if self.vis_pos_emb == 'zeroes' or self.vis_pos_emb == 'pos':\n",
        "            decoder_output = self.VCAdecoder(inputs_embeds=inputs_embeds, attention_mask=attention_mask, position_ids = position_ids, token_type_ids = token_type_ids)\n",
        "        else:\n",
        "            decoder_output = self.VCAdecoder(inputs_embeds=inputs_embeds, attention_mask=attention_mask)\n",
        "        decoder_output = decoder_output.last_hidden_state.swapaxes(1,2)\n",
        "        decoder_output = F.adaptive_avg_pool1d(decoder_output,1)\n",
        "        decoder_output = decoder_output.swapaxes(1,2).squeeze(1)\n",
        "\n",
        "        ## intermediate layers\n",
        "        out =self.intermediate_layer(decoder_output)\n",
        "        out = self.LayerNorm(out)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        ## classifier\n",
        "        out = self.classifier(out)\n",
        "        # print(out.size())\n",
        "        return out\n",
        "\n",
        "def train(args, train_dataloader, model, criterion, optimizer, epoch, tokenizer, device):\n",
        "\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    label_true = None\n",
        "    label_pred = None\n",
        "    label_score = None\n",
        "\n",
        "    for i, (_, v_f, q, labels) in enumerate(train_dataloader,0):\n",
        "        questions = []\n",
        "        for question in q: questions.append(question)\n",
        "\n",
        "        if args.model_ver == 'efvlegpt2rs18' or args.model_ver == \"efvlegpt2Swin\" or args.model_ver == 'efvlegpt2ViT':\n",
        "            inputs = tokenizer(questions, padding=\"max_length\",max_length= args.question_len, return_tensors=\"pt\")\n",
        "\n",
        "        # Visual features\n",
        "        if args.model_ver == \"efvlegpt2Swin\" or args.model_ver == 'efvlegpt2ViT':\n",
        "            visual_features = v_f\n",
        "            visual_features['pixel_values'] = torch.squeeze(visual_features['pixel_values'],1)\n",
        "        else:\n",
        "            visual_features = v_f.to(device)\n",
        "\n",
        "        # labels\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(inputs, visual_features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        scores, predicted = torch.max(F.softmax(outputs, dim=1).data, 1)\n",
        "        label_true = labels.data.cpu() if label_true == None else torch.cat((label_true, labels.data.cpu()), 0)\n",
        "        label_pred = predicted.data.cpu() if label_pred == None else torch.cat((label_pred, predicted.data.cpu()), 0)\n",
        "        label_score = scores.data.cpu() if label_score == None else torch.cat((label_score, scores.data.cpu()), 0)\n",
        "\n",
        "    # loss and acc\n",
        "    acc, c_acc = calc_acc(label_true, label_pred), calc_classwise_acc(label_true, label_pred)\n",
        "    precision, recall, fscore = calc_precision_recall_fscore(label_true, label_pred)\n",
        "    print('Train: epoch: %d loss: %.6f | Acc: %.6f | Precision: %.6f | Recall: %.6f | FScore: %.6f' %(epoch, total_loss, acc, precision, recall, fscore))\n",
        "    return acc\n",
        "\n",
        "\n",
        "def validate(args, val_loader, model, criterion, epoch, tokenizer, device, save_output = False):\n",
        "\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    label_true = None\n",
        "    label_pred = None\n",
        "    label_score = None\n",
        "    file_names = list()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (file_name, v_f, q, labels) in enumerate(val_loader,0):\n",
        "\n",
        "            # prepare questions\n",
        "            questions = []\n",
        "            for question in q: questions.append(question)\n",
        "\n",
        "            if args.model_ver == 'efvlegpt2rs18' or args.model_ver == \"efvlegpt2Swin\" or args.model_ver == 'efvlegpt2ViT':\n",
        "                inputs = tokenizer(questions, padding=\"max_length\",max_length=args.question_len, return_tensors=\"pt\")\n",
        "\n",
        "            # Visual features\n",
        "            if args.model_ver == \"efvlegpt2Swin\" or args.model_ver == 'efvlegpt2ViT':\n",
        "                visual_features = v_f\n",
        "                visual_features['pixel_values'] = torch.squeeze(visual_features['pixel_values'],1)\n",
        "            else:\n",
        "                visual_features = v_f.to(device)\n",
        "\n",
        "            # label\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # model forward pass\n",
        "            outputs = model(inputs, visual_features)\n",
        "\n",
        "            # loss\n",
        "            loss = criterion(outputs,labels)\n",
        "            total_loss += loss.item()\n",
        "            scores, predicted = torch.max(F.softmax(outputs, dim=1).data, 1)\n",
        "            label_true = labels.data.cpu() if label_true == None else torch.cat((label_true, labels.data.cpu()), 0)\n",
        "            label_pred = predicted.data.cpu() if label_pred == None else torch.cat((label_pred, predicted.data.cpu()), 0)\n",
        "            label_score = scores.data.cpu() if label_score == None else torch.cat((label_score, scores.data.cpu()), 0)\n",
        "            for f in file_name: file_names.append(f)\n",
        "\n",
        "    acc = calc_acc(label_true, label_pred)\n",
        "    c_acc = 0.0\n",
        "    precision, recall, fscore = calc_precision_recall_fscore(label_true, label_pred)\n",
        "    print('Test: epoch: %d loss: %.6f | Acc: %.6f | Precision: %.6f | Recall: %.6f | FScore: %.6f' %(epoch, total_loss, acc, precision, recall, fscore))\n",
        "\n",
        "    return (acc, c_acc, precision, recall, fscore)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    args = get_arg()\n",
        "    os.makedirs('checkpoints/efvlegpt2Swin', exist_ok=True)\n",
        "    seed_everything()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    start_epoch = 1\n",
        "    best_epoch = [0]\n",
        "    best_results = [0.0]\n",
        "    epochs_since_improvement = 0\n",
        "    final_args = {\"emb_dim\": args.emb_dim, \"n_heads\": args.n_heads, \"dropout\": args.dropout, \"encoder_layers\": args.encoder_layers}\n",
        "\n",
        "    if args.dataset_type == 'm18':\n",
        "        # tokenizer\n",
        "        if args.tokenizer_ver == 'gpt2v1':\n",
        "            # tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "            tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "        # data location\n",
        "        train_seq = [2, 3, 4, 6, 7, 9, 10, 11, 12, 14, 15]\n",
        "        val_seq = [1, 5, 16]\n",
        "\n",
        "        folder_head = 'dataset/EndoVis-18-VQA/seq_'\n",
        "        folder_tail = '/vqa/Classification/*.txt'\n",
        "\n",
        "        # dataloader\n",
        "        if args.model_ver == 'efvlegpt2rs18' or args.model_ver == \"efvlegpt2Swin\" or args.model_ver == 'efvlegpt2ViT':\n",
        "            train_dataset = EndoVis18VQAGPTClassification(train_seq, folder_head, folder_tail, model_ver=args.model_ver)\n",
        "            train_dataloader = DataLoader(dataset=train_dataset, batch_size= args.batch_size, shuffle=True, num_workers=8)\n",
        "            val_dataset = EndoVis18VQAGPTClassification(val_seq, folder_head, folder_tail, model_ver=args.model_ver)\n",
        "            val_dataloader = DataLoader(dataset=val_dataset, batch_size= args.batch_size, shuffle=False, num_workers=8)\n",
        "\n",
        "        # num_classes\n",
        "        args.num_class = 18\n",
        "\n",
        "        if args.model_ver == 'efvlegpt2Swin':\n",
        "            model = EFVLEGPT2SwinClassification(num_class = args.num_class, model_subver = args.model_subver, vis_pos_emb = args.vis_pos_emb)\n",
        "            optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
        "\n",
        "        model = model.to(device)\n",
        "        criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "        for epoch in range(start_epoch, args.epochs):\n",
        "\n",
        "            if epochs_since_improvement > 0 and epochs_since_improvement % 5 == 0:\n",
        "                adjust_learning_rate(optimizer, 0.8)\n",
        "\n",
        "            # train\n",
        "            train_acc = train(args, train_dataloader=train_dataloader, model = model, criterion=criterion, optimizer=optimizer, epoch=epoch, tokenizer = tokenizer, device = device)\n",
        "\n",
        "            # validation\n",
        "            test_acc, test_c_acc, test_precision, test_recall, test_fscore = validate(args, val_loader=val_dataloader, model = model, criterion=criterion, epoch=epoch, tokenizer = tokenizer, device = device)\n",
        "\n",
        "            if test_acc >= best_results[0]:\n",
        "                print('Best Epoch:', epoch)\n",
        "                epochs_since_improvement = 0\n",
        "                best_results[0] = test_acc\n",
        "                best_epoch[0] = epoch\n",
        "                save_clf_checkpoint(args.checkpoint_dir, epoch, epochs_since_improvement, model, optimizer, best_results[0], final_args)\n",
        "\n"
      ],
      "metadata": {
        "id": "Wj1Az0gsWW6E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cbaeabc-e499-4960-d413-0c6ae16674ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/SurgicalGPT\n",
            "Total files: 1560 | Total question: 9014\n",
            "Total files: 447 | Total question: 2769\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: epoch: 1 loss: 349.368337 | Acc: 0.533393 | Precision: 0.258904 | Recall: 0.216620 | FScore: 0.217461\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test: epoch: 1 loss: 70.788700 | Acc: 0.598050 | Precision: 0.746089 | Recall: 0.312809 | FScore: 0.277164\n",
            "Best Epoch: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "/content/SurgicalGPT\n",
        "Total files: 1560 | Total question: 9014\n",
        "Total files: 447 | Total question: 2769\n",
        "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
        "  warnings.warn(_create_warning_msg(\n",
        "Train: epoch: 1 loss: 349.368337 | Acc: 0.533393 | Precision: 0.258904 | Recall: 0.216620 | FScore: 0.217461\n",
        "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
        "  warnings.warn(_create_warning_msg(\n",
        "Test: epoch: 1 loss: 70.788700 | Acc: 0.598050 | Precision: 0.746089 | Recall: 0.312809 | FScore: 0.277164\n",
        "Best Epoch: 1"
      ],
      "metadata": {
        "id": "xfstGr_kIkFJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}